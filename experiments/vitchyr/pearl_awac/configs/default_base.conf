{
    qf_kwargs: {
        hidden_sizes: [300, 300, 300],
    },
    policy_kwargs: {
        hidden_sizes: [300, 300, 300],
    },
    trainer_kwargs: {
        soft_target_tau: 0.005,  // for SAC target network update
        target_update_period: 1,
        policy_lr: 3E-4,
        qf_lr: 3E-4,
        context_lr: 3e-4,
        kl_lambda: .1,  // weight on KL divergence term in encoder loss
        beta: 1.0,
        alpha: 0.0,
        rl_weight: 1.0,
        use_awr_update: true,
        use_reparam_update: false,
        use_automatic_entropy_tuning: false,
        use_information_bottleneck: true, // false makes latent context deterministic
        use_next_obs_in_context: false, // use next obs if it is useful in distinguishing tasks
        sparse_rewards: false, // whether to sparsify rewards as determined in env
        recurrent: false, // recurrent or permutation-invariant encoder
        discount: 0.99, // RL discount factor
        reward_scale: 5.0, // scale rewards before constructing Bellman update, effectively controls weight on the entropy of the policy
        reparam_weight: 0.0,
        awr_weight: 1.0,
        bc_weight: 0.0,
        compute_bc: false,
        awr_use_mle_for_vf: false,
        awr_sample_actions: false,
        awr_min_q: true,
    },
    algo_kwargs: {
        meta_batch: 16, // number of tasks to average the gradient across
        num_iterations: 500, // number of data sampling / training iterates
        num_initial_steps: 2000, // number of transitions collected per task before training
        num_tasks_sample: 5, // number of randomly sampled tasks to collect data for each iteration
        num_steps_prior: 400, // number of transitions to collect per task with z ~ prior
        num_steps_posterior: 0, // number of transitions to collect per task with z ~ posterior
        num_extra_rl_steps_posterior: 400, // number of additional transitions to collect per task with z ~ posterior that are only used to train the policy and NOT the encoder
        num_train_steps_per_itr: 2000, // number of meta-gradient steps taken per iteration
        num_evals: 2, // number of independent evals
        num_steps_per_eval: 600,  // nuumber of transitions to eval on
        batch_size: 256, // number of transitions in the RL batch
        embedding_batch_size: 64, // number of transitions in the context batch
        embedding_mini_batch_size: 64, // number of context transitions to backprop through (should equal the arg above except in the recurrent encoder case)
        max_path_length: 200, // max path length for this environment
        update_post_train: 1, // how often to resample the context when collecting data during training (in trajectories)
        num_exp_traj_eval: 1, // how many exploration trajs to collect before beginning posterior sampling at test time
        dump_eval_paths: false, // whether to save evaluation trajectories
        num_iterations_with_reward_supervision: null,
        save_extra_manual_epoch_list: [0, 1, 49, 100, 200, 300, 400, 500],
        save_replay_buffer: true,
        save_algorithm: true,
    },
    latent_dim: 5,
    logger_config: {
        snapshot_mode: 'gap_and_last',
        snapshot_gap: 25,
    }
}
