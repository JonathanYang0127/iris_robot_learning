{
    trainer_kwargs: {
        soft_target_tau: 0.005, // for SAC target network update
        policy_lr: 3E-4,
        qf_lr: 3E-4,
        context_lr: 3e-4,
        kl_lambda: 1, // weight on KL divergence term in encoder loss
        use_information_bottleneck: true, // False makes latent context deterministic
        use_next_obs_in_context: false, // use next obs if it is useful in distinguishing tasks
        sparse_rewards: false, // whether to sparsify rewards as determined in env
        recurrent: false, // recurrent or permutation-invariant encoder
        discount: .99, // RL discount factor
        reward_scale: 5.0,  // scale rewards before constructing Bellman update, effectively controls weight on the entropy of the policy
        kl_lambda: 0.1,
    }
    replay_buffer_kwargs: {
        max_replay_buffer_size: 1000000,
        use_next_obs_in_context: false,
        sparse_rewards: false,
    }
    pearl_buffer_kwargs: {
        meta_batch_size: 16,
        embedding_batch_size: 64,
    }
    qf_kwargs: {
        hidden_sizes: [200, 200, 200],
    }
    vf_kwargs: {
        hidden_sizes: [200, 200, 200],
    }
    policy_kwargs: {
        hidden_sizes: [200, 200, 200],
    }
    context_encoder_kwargs: {
        hidden_sizes: [200, 200, 200],
    },
    replay_buffer_kwargs: {
        max_replay_buffer_size: 1000000,
        use_next_obs_in_context: false,
        sparse_rewards: false,
    }
    name_to_expl_path_collector_kwargs: {
        prior: {  // just sample data with the prior
            sample_initial_context: false,
            accum_context_across_rollouts: false,
            resample_latent_period: 0,
            update_posterior_period: 0,
            use_predicted_reward: false,
        },
        posterior: {
            sample_initial_context: false,
            accum_context_across_rollouts: true,
            resample_latent_period: 0,
            update_posterior_period: 0,
            use_predicted_reward: false,
        },
    }
    name_to_eval_path_collector_kwargs: {
        init_from_buffer: {
            sample_initial_context: true,
            accum_context_across_rollouts: false,
            resample_latent_period: 0,
            update_posterior_period: 0,
            use_predicted_reward: false,
        },
        init_from_buffer_live_update: {
            sample_initial_context: true,
            accum_context_across_rollouts: false,
            resample_latent_period: 0,
            update_posterior_period: 1,
            use_predicted_reward: false,
        },
        posterior: {
            sample_initial_context: false,
            accum_context_across_rollouts: true,
            resample_latent_period: 0,
            update_posterior_period: 0,
            use_predicted_reward: false,
        },
        posterior_live_update: {
            sample_initial_context: false,
            accum_context_across_rollouts: true,
            resample_latent_period: 0,
            update_posterior_period: 1,
            use_predicted_reward: false,
        },
    },
    algo_kwargs: {
        batch_size: 256,
        num_train_loops_per_epoch: 1,
        save_algorithm: true,
        save_extra_manual_epoch_list: [
            0, 50,
            100, 150,
            200, 250,
            300, 350,
            400, 450,
            500, 550,
            600, 650,
            700, 750,
            800, 850,
            900, 950,
            999, 1000,
        ],
    }
    save_video: true
    save_video_kwargs: {
        save_video_period: 25,
        video_img_size: 128,
    }
}
