{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PEARL TRAINING CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from rlkit.torch.torch_rl_algorithm import TorchTrainer\n",
    "from collections import OrderedDict\n",
    "from itertools import chain\n",
    "from torch.distributions import kl_divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _product_of_gaussians(mus, sigmas_squared):\n",
    "    '''\n",
    "    compute mu, sigma of product of gaussians\n",
    "    '''\n",
    "    sigmas_squared = torch.clamp(sigmas_squared, min=1e-7)\n",
    "    sigma_squared = 1. / torch.sum(torch.reciprocal(sigmas_squared), dim=0)\n",
    "    mu = sigma_squared * torch.sum(mus / sigmas_squared, dim=0)\n",
    "    return mu, sigma_squared\n",
    "\n",
    "\n",
    "class PEARLAgent:\n",
    "    def __init__(self,\n",
    "                 latent_dim,\n",
    "                 context_encoder,\n",
    "                 reward_predictor,\n",
    "                 obs_keys=None,\n",
    "                 use_next_obs_in_context=False,\n",
    "                 ):\n",
    "        self.latent_dim = latent_dim\n",
    "        self.context_encoder = context_encoder\n",
    "        self.context_encoder_rp = copy.deepcopy(self.context_encoder)\n",
    "        self.context_encoder_rp.to(ptu.device)\n",
    "        self.reward_predictor = reward_predictor\n",
    "        self.obs_keys = obs_keys\n",
    "        self.use_next_obs_in_context = use_next_obs_in_context\n",
    "        \n",
    "        self.latent_prior = torch.distributions.Normal(\n",
    "            ptu.zeros(self.latent_dim),\n",
    "            ptu.ones(self.latent_dim)\n",
    "        )\n",
    "\n",
    "        \n",
    "    def latent_posterior(self, context, squeeze=False, use_encoder_copy=False):\n",
    "        if isinstance(context, np.ndarray):\n",
    "            context = ptu.from_numpy(context)\n",
    "        if use_encoder_copy:\n",
    "            context_encoder = self.context_encoder_rp\n",
    "        else:\n",
    "            context_encoder = self.context_encoder\n",
    "\n",
    "        t, b = context.size(0), context.size(1)\n",
    "        context_flat = context.view(t*b, -1)\n",
    "        params = context_encoder(context_flat)\n",
    "        params = params.view(context.size(0), -1, context_encoder.output_size)\n",
    "        mu = params[..., :self.latent_dim]\n",
    "        sigma_squared = F.softplus(params[..., self.latent_dim:])\n",
    "        z_params = [_product_of_gaussians(m, s) for m, s in zip(torch.unbind(mu), torch.unbind(sigma_squared))]\n",
    "        z_means = torch.stack([p[0] for p in z_params])\n",
    "        z_vars = torch.stack([p[1] for p in z_params])\n",
    "        if squeeze:\n",
    "            z_means = z_means.squeeze(dim=0)\n",
    "            z_vars = z_vars.squeeze(dim=0)\n",
    "            \n",
    "        return torch.distributions.Normal(z_means, torch.sqrt(z_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PearlTrainer(TorchTrainer):\n",
    "    def __init__(\n",
    "        self,\n",
    "        agent: PEARLAgent,\n",
    "        env,\n",
    "        latent_dim,\n",
    "        context_encoder,\n",
    "        reward_predictor,\n",
    "        context_decoder,\n",
    "\n",
    "        train_context_decoder=True,\n",
    "        context_lr=1e-3,\n",
    "        kl_lambda=1.0,\n",
    "        optimizer_class=optim.Adam,\n",
    "\n",
    "        discount=0.99,\n",
    "        reward_scale=1.0,\n",
    "        \n",
    "        kl_annealing=False,\n",
    "        kl_annealing_x0=50000,\n",
    "        kl_annealing_k=0.00005,\n",
    "        kl_annealing_start=20000\n",
    "    ):\n",
    "\n",
    "        self.train_context_decoder = train_context_decoder\n",
    "        self.train_encoder_decoder = True\n",
    "        self.reward_scale = reward_scale\n",
    "        self.discount = discount\n",
    "\n",
    "        self.agent = agent\n",
    "        self.context_encoder = context_encoder\n",
    "        self.context_decoder = context_decoder\n",
    "        self.reward_predictor = reward_predictor\n",
    "        \n",
    "        self.kl_annealing = kl_annealing\n",
    "        self.kl_annealing_x0 = kl_annealing_x0\n",
    "        self.kl_annealing_k = kl_annealing_k\n",
    "        self.kl_annealing_start = kl_annealing_start\n",
    "\n",
    "        self.env = env\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        self.kl_lambda = kl_lambda\n",
    "        if train_context_decoder:\n",
    "            self.context_optimizer = optimizer_class(\n",
    "                chain(\n",
    "                    self.context_encoder.parameters(),\n",
    "                    self.context_decoder.parameters(),\n",
    "                ),\n",
    "                lr=context_lr,\n",
    "            )\n",
    "        else:\n",
    "            self.context_optimizer = optimizer_class(\n",
    "                self.context_encoder.parameters(),\n",
    "                lr=context_lr,\n",
    "            )\n",
    "\n",
    "        self.discount = discount\n",
    "        self.reward_scale = reward_scale\n",
    "        self.eval_statistics = OrderedDict()\n",
    "        self._n_train_steps_total = 0\n",
    "        self._need_to_update_eval_statistics = True\n",
    "        \n",
    "    \n",
    "    def train_from_torch(self, batch, learn_task_z=True):\n",
    "        rewards = batch['rewards']\n",
    "        terminals = batch['terminals']\n",
    "        obs = batch['observations']\n",
    "        actions = batch['actions']\n",
    "        next_obs = batch['next_observations']\n",
    "        context = batch['context']\n",
    "\n",
    "\n",
    "        # flattens out the task dimension\n",
    "        t, b, _ = obs.size()\n",
    "        obs = obs.view(t * b, -1)\n",
    "        actions = actions.view(t * b, -1)\n",
    "        next_obs = next_obs.view(t * b, -1)\n",
    "        unscaled_rewards_flat = rewards.view(t * b, 1)\n",
    "        rewards_flat = unscaled_rewards_flat * self.reward_scale\n",
    "        terms_flat = terminals.view(t * b, 1)\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "        Policy and Alpha Loss\n",
    "        \"\"\"\n",
    "        p_z = self.agent.latent_posterior(context, use_encoder_copy = not learn_task_z)\n",
    "        task_z_with_grad = p_z.rsample()\n",
    "        task_z_with_grad = [z.repeat(b, 1) for z in task_z_with_grad]\n",
    "        task_z_with_grad = torch.cat(task_z_with_grad, dim=0)\n",
    "        task_z_detached = task_z_with_grad.detach()\n",
    "\n",
    "\n",
    "        \"\"\"\n",
    "        Context Encoder Loss\n",
    "        \"\"\"\n",
    "        kl_div = kl_divergence(p_z, self.agent.latent_prior).mean(dim=0).sum()\n",
    "        if self.kl_annealing:\n",
    "            if self._n_train_steps_total < self.kl_annealing_start:\n",
    "                kl_lambda = 0\n",
    "            else:\n",
    "                kl_lambda = self.kl_lambda * float(1 / (1 + np.exp(-self.kl_annealing_k * \n",
    "                    (self._n_train_steps_total - self.kl_annealing_start - self.kl_annealing_x0))))\n",
    "        else:\n",
    "            kl_lambda = self.kl_lambda\n",
    "        kl_loss = kl_lambda * kl_div\n",
    "\n",
    "        if self.train_context_decoder:\n",
    "            # TODO: change to use a distribution\n",
    "            if learn_task_z:\n",
    "                task_z = task_z_with_grad\n",
    "            else:\n",
    "                task_z = task_z_detached\n",
    "            reward_pred = self.context_decoder(obs, actions, task_z)\n",
    "            reward_prediction_loss = ((reward_pred - unscaled_rewards_flat)**2).mean()\n",
    "            context_loss = kl_loss + reward_prediction_loss\n",
    "        else:\n",
    "            context_loss = kl_loss\n",
    "            reward_prediction_loss = ptu.zeros(1)\n",
    "\n",
    "        \"\"\"\n",
    "        Update networks\n",
    "        \"\"\"\n",
    "        self.context_optimizer.zero_grad()\n",
    "        context_loss.backward()\n",
    "        self.context_optimizer.step()\n",
    "\n",
    "        \"\"\"\n",
    "        Save some statistics for eval\n",
    "        \"\"\"\n",
    "        if self._need_to_update_eval_statistics:\n",
    "            self._need_to_update_eval_statistics = False\n",
    "            \"\"\"\n",
    "            Eval should set this to None.\n",
    "            This way, these statistics are only computed for one batch.\n",
    "            \"\"\"\n",
    "            if self.kl_annealing:\n",
    "                self.eval_statistics['task_embedding/kl_lambda'] = (\n",
    "                    kl_lambda\n",
    "                )\n",
    "            self.eval_statistics['task_embedding/kl_divergence'] = (\n",
    "                ptu.get_numpy(kl_div)\n",
    "            )\n",
    "            self.eval_statistics['task_embedding/kl_loss'] = (\n",
    "                ptu.get_numpy(kl_loss)\n",
    "            )\n",
    "            self.eval_statistics['task_embedding/reward_prediction_loss'] = (\n",
    "                ptu.get_numpy(reward_prediction_loss)\n",
    "            )\n",
    "            self.eval_statistics['task_embedding/context_loss'] = (\n",
    "                ptu.get_numpy(context_loss)\n",
    "            )\n",
    "\n",
    "        self._n_train_steps_total += 1\n",
    "        \n",
    "    @property\n",
    "    def networks(self):\n",
    "        return [\n",
    "            self.context_encoder,\n",
    "            self.context_decoder,\n",
    "            self.reward_predictor,\n",
    "        ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN PEARL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "import rlkit.torch.pytorch_util as ptu\n",
    "from rlkit.data_management.multitask_replay_buffer import ObsDictMultiTaskReplayBuffer\n",
    "from rlkit.misc.roboverse_utils import add_data_to_buffer_multitask_v2, get_buffer_size_multitask\n",
    "from rlkit.torch.sac.policies import GaussianCNNPolicy\n",
    "from rlkit.torch.networks.cnn import CNN, ConcatCNN\n",
    "from rlkit.torch.core import np_to_pytorch_batch\n",
    "import roboverse\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "gpu_id = 3\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(gpu_id)\n",
    "ptu.set_gpu_mode(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file paths\n",
    "BUFFER = ('/nfs/kun1/users/avi/scripted_sim_datasets/'\n",
    "    'may26_Widow250PickPlaceMetaTrainMultiObjectMultiContainer-v0_16K_save_all_noise_0.1_2021-05-26T16-12-04/'\n",
    "    'may26_Widow250PickPlaceMetaTrainMultiObjectMultiContainer-v0_16K_save_all_noise_0.1_2021-05-26T16-12-04_16000.npy')\n",
    "VALIDATION_BUFFER = ('/nfs/kun1/users/jonathan/minibullet_data/'\n",
    "    'jun_14_validation_Widow250PickPlaceMetaTrainMultiObjectMultiContainer-v0_1K_save_all_noise_0.1_2021-06-14T11-27-40/'\n",
    "    'jun_14_validation_Widow250PickPlaceMetaTrainMultiObjectMultiContainer-v0_1K_save_all_noise_0.1_2021-06-14T11-27-40_1024.npy')\n",
    "ENV = 'Widow250PickPlaceMetaTestMultiObjectMultiContainer-v0'\n",
    "\n",
    "#agent kwargs\n",
    "LATENT_DIM = 5\n",
    "USE_NEXT_OBS_IN_CONTEXT = False\n",
    "_DEBUG_DO_NOT_SQRT = False\n",
    "\n",
    "#context kwargs\n",
    "META_BATCH_SIZE = 4\n",
    "TASK_EMBEDDING_BATCH_SIZE = 64\n",
    "\n",
    "#trainer kwargs\n",
    "NUM_BATCHES = 1000 * 200\n",
    "TRAIN_TASKS = np.arange(32)\n",
    "BATCH_SIZE = 128\n",
    "LOGGING_PERIOD = 1000\n",
    "KL_ANNEALING_X0 = 1000 * 20\n",
    "KL_ANNEALING_K = 0.05 / 1000\n",
    "KL_ANNEALING_START = 1000 * 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ConcatCNN(\n",
       "  (hidden_activation): ReLU()\n",
       "  (conv_layers): ModuleList(\n",
       "    (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (2): Conv2d(16, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  )\n",
       "  (conv_norm_layers): ModuleList()\n",
       "  (pool_layers): ModuleList(\n",
       "    (0): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (fc_layers): ModuleList(\n",
       "    (0): Linear(in_features=2327, out_features=256, bias=True)\n",
       "    (1): Linear(in_features=256, out_features=256, bias=True)\n",
       "  )\n",
       "  (fc_norm_layers): ModuleList()\n",
       "  (last_fc): Linear(in_features=256, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expl_env = roboverse.make(ENV, transpose_image=True)\n",
    "state_observation_dim = expl_env.observation_space.spaces['state'].low.size\n",
    "action_dim = expl_env.action_space.low.size\n",
    "reward_dim = 1\n",
    "cnn_params = dict(\n",
    "        input_width=48,\n",
    "        input_height=48,\n",
    "        input_channels=3,\n",
    "        kernel_sizes=[3, 3, 3],\n",
    "        n_channels=[16, 16, 16],\n",
    "        strides=[1, 1, 1],\n",
    "        hidden_sizes=[1024, 512, 256],\n",
    "        paddings=[1, 1, 1],\n",
    "        pool_type='max2d',\n",
    "        pool_sizes=[2, 2, 1],  # the one at the end means no pool\n",
    "        pool_strides=[2, 2, 1],\n",
    "        pool_paddings=[0, 0, 0],\n",
    "        image_augmentation=True,\n",
    "        image_augmentation_padding=4,\n",
    "    )\n",
    "context_encoder_output_dim = LATENT_DIM * 2\n",
    "cnn_params.update(\n",
    "    added_fc_input_size=state_observation_dim + action_dim + reward_dim,\n",
    "    output_size=context_encoder_output_dim,\n",
    "    hidden_sizes=[256, 256],\n",
    ")\n",
    "context_encoder = ConcatCNN(**cnn_params)\n",
    "context_encoder.to(ptu.device)\n",
    "cnn_params.update(\n",
    "    added_fc_input_size=state_observation_dim + action_dim + LATENT_DIM,\n",
    "    output_size=1,\n",
    "    hidden_sizes=[256, 256],\n",
    "    image_augmentation=False,\n",
    ")\n",
    "context_decoder = ConcatCNN(**cnn_params)\n",
    "context_decoder.to(ptu.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_task_indices = list(range(32))\n",
    "observation_keys = ['image', 'state']\n",
    "with open(BUFFER, 'rb') as fl:\n",
    "    data = np.load(fl, allow_pickle=True)\n",
    "num_transitions = get_buffer_size_multitask(data)\n",
    "max_replay_buffer_size = num_transitions + 10\n",
    "\n",
    "replay_buffer = ObsDictMultiTaskReplayBuffer(\n",
    "    max_replay_buffer_size,\n",
    "    expl_env,\n",
    "    train_task_indices,\n",
    "    use_next_obs_in_context=False,\n",
    "    sparse_rewards=False,\n",
    "    observation_keys=observation_keys\n",
    ")\n",
    "add_data_to_buffer_multitask_v2(data, replay_buffer, observation_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_task_indices = list(range(32))\n",
    "observation_keys = ['image', 'state']\n",
    "with open(VALIDATION_BUFFER, 'rb') as fl:\n",
    "    data = np.load(fl, allow_pickle=True)\n",
    "num_transitions = get_buffer_size_multitask(data)\n",
    "max_replay_buffer_size = num_transitions + 10\n",
    "\n",
    "validation_buffer = ObsDictMultiTaskReplayBuffer(\n",
    "    max_replay_buffer_size,\n",
    "    expl_env,\n",
    "    train_task_indices,\n",
    "    use_next_obs_in_context=False,\n",
    "    sparse_rewards=False,\n",
    "    observation_keys=observation_keys\n",
    ")\n",
    "add_data_to_buffer_multitask_v2(data, validation_buffer, observation_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = PEARLAgent(\n",
    "    LATENT_DIM,\n",
    "    context_encoder,\n",
    "    None,\n",
    "    obs_keys=observation_keys,\n",
    ")\n",
    "\n",
    "trainer = PearlTrainer(\n",
    "    agent=agent,\n",
    "    env=expl_env,\n",
    "    latent_dim=LATENT_DIM,\n",
    "    reward_predictor=context_decoder,\n",
    "    context_encoder=context_encoder,\n",
    "    context_decoder=context_decoder,\n",
    "    kl_annealing=True,\n",
    "    kl_annealing_x0=KL_ANNEALING_X0,\n",
    "    kl_annealing_k=KL_ANNEALING_K,\n",
    "    kl_annealing_start=KL_ANNEALING_START\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('task_embedding/kl_lambda', 0.2689414213699951), ('task_embedding/kl_divergence', array(7.4598436, dtype=float32)), ('task_embedding/kl_loss', array(2.006261, dtype=float32)), ('task_embedding/reward_prediction_loss', array(0.00091146, dtype=float32)), ('task_embedding/context_loss', array(2.0071726, dtype=float32))])\n",
      "OrderedDict([('task_embedding/kl_lambda', 0.2788848219771369), ('task_embedding/kl_divergence', array(6.292349, dtype=float32)), ('task_embedding/kl_loss', array(1.7548406, dtype=float32)), ('task_embedding/reward_prediction_loss', array(0.00094373, dtype=float32)), ('task_embedding/context_loss', array(1.7557844, dtype=float32))])\n",
      "OrderedDict([('task_embedding/kl_lambda', 0.289050497374996), ('task_embedding/kl_divergence', array(5.4834385, dtype=float32)), ('task_embedding/kl_loss', array(1.5849906, dtype=float32)), ('task_embedding/reward_prediction_loss', array(0.00047288, dtype=float32)), ('task_embedding/context_loss', array(1.5854635, dtype=float32))])\n",
      "OrderedDict([('task_embedding/kl_lambda', 0.29943285752602705), ('task_embedding/kl_divergence', array(4.878623, dtype=float32)), ('task_embedding/kl_loss', array(1.46082, dtype=float32)), ('task_embedding/reward_prediction_loss', array(0.00226673, dtype=float32)), ('task_embedding/context_loss', array(1.4630867, dtype=float32))])\n",
      "OrderedDict([('task_embedding/kl_lambda', 0.31002551887238755), ('task_embedding/kl_divergence', array(4.39669, dtype=float32)), ('task_embedding/kl_loss', array(1.363086, dtype=float32)), ('task_embedding/reward_prediction_loss', array(0.00033616, dtype=float32)), ('task_embedding/context_loss', array(1.3634222, dtype=float32))])\n",
      "OrderedDict([('task_embedding/kl_lambda', 0.320821300824607), ('task_embedding/kl_divergence', array(3.995378, dtype=float32)), ('task_embedding/kl_loss', array(1.2818024, dtype=float32)), ('task_embedding/reward_prediction_loss', array(0.00286691, dtype=float32)), ('task_embedding/context_loss', array(1.2846693, dtype=float32))])\n",
      "OrderedDict([('task_embedding/kl_lambda', 0.3318122278318339), ('task_embedding/kl_divergence', array(3.6514595, dtype=float32)), ('task_embedding/kl_loss', array(1.2115989, dtype=float32)), ('task_embedding/reward_prediction_loss', array(0.00019907, dtype=float32)), ('task_embedding/context_loss', array(1.211798, dtype=float32))])\n",
      "OrderedDict([('task_embedding/kl_lambda', 0.34298953732650117), ('task_embedding/kl_divergence', array(3.351165, dtype=float32)), ('task_embedding/kl_loss', array(1.1494145, dtype=float32)), ('task_embedding/reward_prediction_loss', array(0.00217962, dtype=float32)), ('task_embedding/context_loss', array(1.1515942, dtype=float32))])\n",
      "OrderedDict([('task_embedding/kl_lambda', 0.35434369377420455), ('task_embedding/kl_divergence', array(3.0856228, dtype=float32)), ('task_embedding/kl_loss', array(1.0933709, dtype=float32)), ('task_embedding/reward_prediction_loss', array(0.00426034, dtype=float32)), ('task_embedding/context_loss', array(1.0976312, dtype=float32))])\n",
      "OrderedDict([('task_embedding/kl_lambda', 0.36586440898919936), ('task_embedding/kl_divergence', array(2.848652, dtype=float32)), ('task_embedding/kl_loss', array(1.0422204, dtype=float32)), ('task_embedding/reward_prediction_loss', array(0.00248289, dtype=float32)), ('task_embedding/context_loss', array(1.0447032, dtype=float32))])\n",
      "OrderedDict([('task_embedding/kl_lambda', 0.3775406687981454), ('task_embedding/kl_divergence', array(2.635656, dtype=float32)), ('task_embedding/kl_loss', array(0.9950674, dtype=float32)), ('task_embedding/reward_prediction_loss', array(0.00047762, dtype=float32)), ('task_embedding/context_loss', array(0.995545, dtype=float32))])\n",
      "OrderedDict([('task_embedding/kl_lambda', 0.389360766050778), ('task_embedding/kl_divergence', array(2.4430842, dtype=float32)), ('task_embedding/kl_loss', array(0.95124114, dtype=float32)), ('task_embedding/reward_prediction_loss', array(0.00024902, dtype=float32)), ('task_embedding/context_loss', array(0.95149016, dtype=float32))])\n",
      "OrderedDict([('task_embedding/kl_lambda', 0.401312339887548), ('task_embedding/kl_divergence', array(2.2680962, dtype=float32)), ('task_embedding/kl_loss', array(0.910215, dtype=float32)), ('task_embedding/reward_prediction_loss', array(0.00292699, dtype=float32)), ('task_embedding/context_loss', array(0.913142, dtype=float32))])\n",
      "OrderedDict([('task_embedding/kl_lambda', 0.41338242108267), ('task_embedding/kl_divergence', array(2.1083798, dtype=float32)), ('task_embedding/kl_loss', array(0.87156713, dtype=float32)), ('task_embedding/reward_prediction_loss', array(0.00109873, dtype=float32)), ('task_embedding/context_loss', array(0.8726659, dtype=float32))])\n",
      "OrderedDict([('task_embedding/kl_lambda', 0.425557483188341), ('task_embedding/kl_divergence', array(1.9620332, dtype=float32)), ('task_embedding/kl_loss', array(0.8349579, dtype=float32)), ('task_embedding/reward_prediction_loss', array(0.00035587, dtype=float32)), ('task_embedding/context_loss', array(0.83531374, dtype=float32))])\n",
      "OrderedDict([('task_embedding/kl_lambda', 0.43782349911420193), ('task_embedding/kl_divergence', array(1.8274672, dtype=float32)), ('task_embedding/kl_loss', array(0.8001081, dtype=float32)), ('task_embedding/reward_prediction_loss', array(0.00063312, dtype=float32)), ('task_embedding/context_loss', array(0.8007412, dtype=float32))])\n",
      "OrderedDict([('task_embedding/kl_lambda', 0.45016600268752216), ('task_embedding/kl_divergence', array(1.7033522, dtype=float32)), ('task_embedding/kl_loss', array(0.7667913, dtype=float32)), ('task_embedding/reward_prediction_loss', array(0.00211368, dtype=float32)), ('task_embedding/context_loss', array(0.768905, dtype=float32))])\n",
      "OrderedDict([('task_embedding/kl_lambda', 0.46257015465625045), ('task_embedding/kl_divergence', array(1.5885607, dtype=float32)), ('task_embedding/kl_loss', array(0.7348208, dtype=float32)), ('task_embedding/reward_prediction_loss', array(0.00167933, dtype=float32)), ('task_embedding/context_loss', array(0.73650014, dtype=float32))])\n",
      "OrderedDict([('task_embedding/kl_lambda', 0.47502081252106), ('task_embedding/kl_divergence', array(1.4821237, dtype=float32)), ('task_embedding/kl_loss', array(0.70403963, dtype=float32)), ('task_embedding/reward_prediction_loss', array(0.00140532, dtype=float32)), ('task_embedding/context_loss', array(0.70544493, dtype=float32))])\n",
      "OrderedDict([('task_embedding/kl_lambda', 0.4875026035157896), ('task_embedding/kl_divergence', array(1.3832166, dtype=float32)), ('task_embedding/kl_loss', array(0.6743217, dtype=float32)), ('task_embedding/reward_prediction_loss', array(0.00026899, dtype=float32)), ('task_embedding/context_loss', array(0.6745907, dtype=float32))])\n",
      "OrderedDict([('task_embedding/kl_lambda', 0.5), ('task_embedding/kl_divergence', array(1.2911282, dtype=float32)), ('task_embedding/kl_loss', array(0.6455641, dtype=float32)), ('task_embedding/reward_prediction_loss', array(0.00165227, dtype=float32)), ('task_embedding/context_loss', array(0.6472164, dtype=float32))])\n",
      "OrderedDict([('task_embedding/kl_lambda', 0.5124973964842103), ('task_embedding/kl_divergence', array(1.2052438, dtype=float32)), ('task_embedding/kl_loss', array(0.61768436, dtype=float32)), ('task_embedding/reward_prediction_loss', array(0.00042414, dtype=float32)), ('task_embedding/context_loss', array(0.6181085, dtype=float32))])\n",
      "OrderedDict([('task_embedding/kl_lambda', 0.52497918747894), ('task_embedding/kl_divergence', array(1.1250033, dtype=float32)), ('task_embedding/kl_loss', array(0.59060335, dtype=float32)), ('task_embedding/reward_prediction_loss', array(0.00140489, dtype=float32)), ('task_embedding/context_loss', array(0.59200823, dtype=float32))])\n",
      "OrderedDict([('task_embedding/kl_lambda', 0.5374298453437496), ('task_embedding/kl_divergence', array(1.0499555, dtype=float32)), ('task_embedding/kl_loss', array(0.5642774, dtype=float32)), ('task_embedding/reward_prediction_loss', array(0.00238724, dtype=float32)), ('task_embedding/context_loss', array(0.56666464, dtype=float32))])\n",
      "OrderedDict([('task_embedding/kl_lambda', 0.549833997312478), ('task_embedding/kl_divergence', array(0.97965604, dtype=float32)), ('task_embedding/kl_loss', array(0.5386482, dtype=float32)), ('task_embedding/reward_prediction_loss', array(0.00226924, dtype=float32)), ('task_embedding/context_loss', array(0.54091746, dtype=float32))])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('task_embedding/kl_lambda', 0.5621765008857981), ('task_embedding/kl_divergence', array(0.9137697, dtype=float32)), ('task_embedding/kl_loss', array(0.5136999, dtype=float32)), ('task_embedding/reward_prediction_loss', array(0.00072458, dtype=float32)), ('task_embedding/context_loss', array(0.51442444, dtype=float32))])\n",
      "OrderedDict([('task_embedding/kl_lambda', 0.574442516811659), ('task_embedding/kl_divergence', array(0.85190517, dtype=float32)), ('task_embedding/kl_loss', array(0.48937052, dtype=float32)), ('task_embedding/reward_prediction_loss', array(0.00145546, dtype=float32)), ('task_embedding/context_loss', array(0.49082598, dtype=float32))])\n",
      "OrderedDict([('task_embedding/kl_lambda', 0.5866175789173301), ('task_embedding/kl_divergence', array(0.793808, dtype=float32)), ('task_embedding/kl_loss', array(0.46566173, dtype=float32)), ('task_embedding/reward_prediction_loss', array(0.00280774, dtype=float32)), ('task_embedding/context_loss', array(0.46846947, dtype=float32))])\n",
      "OrderedDict([('task_embedding/kl_lambda', 0.598687660112452), ('task_embedding/kl_divergence', array(0.73922515, dtype=float32)), ('task_embedding/kl_loss', array(0.44256496, dtype=float32)), ('task_embedding/reward_prediction_loss', array(0.00223092, dtype=float32)), ('task_embedding/context_loss', array(0.44479588, dtype=float32))])\n",
      "OrderedDict([('task_embedding/kl_lambda', 0.610639233949222), ('task_embedding/kl_divergence', array(0.68790734, dtype=float32)), ('task_embedding/kl_loss', array(0.4200632, dtype=float32)), ('task_embedding/reward_prediction_loss', array(0.00162751, dtype=float32)), ('task_embedding/context_loss', array(0.4216907, dtype=float32))])\n",
      "OrderedDict([('task_embedding/kl_lambda', 0.6224593312018546), ('task_embedding/kl_divergence', array(0.63963044, dtype=float32)), ('task_embedding/kl_loss', array(0.39814395, dtype=float32)), ('task_embedding/reward_prediction_loss', array(0.00093153, dtype=float32)), ('task_embedding/context_loss', array(0.39907548, dtype=float32))])\n",
      "OrderedDict([('task_embedding/kl_lambda', 0.6341355910108007), ('task_embedding/kl_divergence', array(0.5941931, dtype=float32)), ('task_embedding/kl_loss', array(0.376799, dtype=float32)), ('task_embedding/reward_prediction_loss', array(0.00034685, dtype=float32)), ('task_embedding/context_loss', array(0.37714583, dtype=float32))])\n",
      "OrderedDict([('task_embedding/kl_lambda', 0.6456563062257954), ('task_embedding/kl_divergence', array(0.55141294, dtype=float32)), ('task_embedding/kl_loss', array(0.35602322, dtype=float32)), ('task_embedding/reward_prediction_loss', array(0.0019384, dtype=float32)), ('task_embedding/context_loss', array(0.35796162, dtype=float32))])\n",
      "OrderedDict([('task_embedding/kl_lambda', 0.6570104626734988), ('task_embedding/kl_divergence', array(0.51112497, dtype=float32)), ('task_embedding/kl_loss', array(0.33581445, dtype=float32)), ('task_embedding/reward_prediction_loss', array(0.00030923, dtype=float32)), ('task_embedding/context_loss', array(0.33612368, dtype=float32))])\n",
      "OrderedDict([('task_embedding/kl_lambda', 0.6681877721681662), ('task_embedding/kl_divergence', array(0.47317868, dtype=float32)), ('task_embedding/kl_loss', array(0.3161722, dtype=float32)), ('task_embedding/reward_prediction_loss', array(0.00073314, dtype=float32)), ('task_embedding/context_loss', array(0.31690535, dtype=float32))])\n",
      "OrderedDict([('task_embedding/kl_lambda', 0.679178699175393), ('task_embedding/kl_divergence', array(0.43743724, dtype=float32)), ('task_embedding/kl_loss', array(0.29709807, dtype=float32)), ('task_embedding/reward_prediction_loss', array(0.00028647, dtype=float32)), ('task_embedding/context_loss', array(0.29738453, dtype=float32))])\n",
      "OrderedDict([('task_embedding/kl_lambda', 0.6899744811276125), ('task_embedding/kl_divergence', array(0.40378517, dtype=float32)), ('task_embedding/kl_loss', array(0.27860147, dtype=float32)), ('task_embedding/reward_prediction_loss', array(0.00064557, dtype=float32)), ('task_embedding/context_loss', array(0.27924705, dtype=float32))])\n",
      "OrderedDict([('task_embedding/kl_lambda', 0.700567142473973), ('task_embedding/kl_divergence', array(0.3722085, dtype=float32)), ('task_embedding/kl_loss', array(0.26075703, dtype=float32)), ('task_embedding/reward_prediction_loss', array(0.00202894, dtype=float32)), ('task_embedding/context_loss', array(0.26278597, dtype=float32))])\n",
      "OrderedDict([('task_embedding/kl_lambda', 0.7109495026250039), ('task_embedding/kl_divergence', array(0.34247726, dtype=float32)), ('task_embedding/kl_loss', array(0.24348404, dtype=float32)), ('task_embedding/reward_prediction_loss', array(0.00150357, dtype=float32)), ('task_embedding/context_loss', array(0.2449876, dtype=float32))])\n",
      "OrderedDict([('task_embedding/kl_lambda', 0.7211151780228631), ('task_embedding/kl_divergence', array(0.31449378, dtype=float32)), ('task_embedding/kl_loss', array(0.22678623, dtype=float32)), ('task_embedding/reward_prediction_loss', array(0.00124307, dtype=float32)), ('task_embedding/context_loss', array(0.2280293, dtype=float32))])\n",
      "OrderedDict([('task_embedding/kl_lambda', 0.7310585786300049), ('task_embedding/kl_divergence', array(0.28816974, dtype=float32)), ('task_embedding/kl_loss', array(0.21066897, dtype=float32)), ('task_embedding/reward_prediction_loss', array(0.00022242, dtype=float32)), ('task_embedding/context_loss', array(0.21089138, dtype=float32))])\n",
      "OrderedDict([('task_embedding/kl_lambda', 0.740774899182154), ('task_embedding/kl_divergence', array(0.26348516, dtype=float32)), ('task_embedding/kl_loss', array(0.19518319, dtype=float32)), ('task_embedding/reward_prediction_loss', array(0.003612, dtype=float32)), ('task_embedding/context_loss', array(0.1987952, dtype=float32))])\n",
      "OrderedDict([('task_embedding/kl_lambda', 0.7502601055951177), ('task_embedding/kl_divergence', array(0.24032477, dtype=float32)), ('task_embedding/kl_loss', array(0.18030609, dtype=float32)), ('task_embedding/reward_prediction_loss', array(0.00141692, dtype=float32)), ('task_embedding/context_loss', array(0.18172301, dtype=float32))])\n",
      "OrderedDict([('task_embedding/kl_lambda', 0.759510916949111), ('task_embedding/kl_divergence', array(0.21858299, dtype=float32)), ('task_embedding/kl_loss', array(0.16601618, dtype=float32)), ('task_embedding/reward_prediction_loss', array(0.00244234, dtype=float32)), ('task_embedding/context_loss', array(0.1684585, dtype=float32))])\n",
      "OrderedDict([('task_embedding/kl_lambda', 0.7685247834990175), ('task_embedding/kl_divergence', array(0.19824569, dtype=float32)), ('task_embedding/kl_loss', array(0.15235673, dtype=float32)), ('task_embedding/reward_prediction_loss', array(0.00148977, dtype=float32)), ('task_embedding/context_loss', array(0.15384649, dtype=float32))])\n",
      "OrderedDict([('task_embedding/kl_lambda', 0.7772998611746911), ('task_embedding/kl_divergence', array(0.17922053, dtype=float32)), ('task_embedding/kl_loss', array(0.1393081, dtype=float32)), ('task_embedding/reward_prediction_loss', array(0.00076172, dtype=float32)), ('task_embedding/context_loss', array(0.14006981, dtype=float32))])\n",
      "OrderedDict([('task_embedding/kl_lambda', 0.7858349830425586), ('task_embedding/kl_divergence', array(0.16144297, dtype=float32)), ('task_embedding/kl_loss', array(0.12686753, dtype=float32)), ('task_embedding/reward_prediction_loss', array(0.0011144, dtype=float32)), ('task_embedding/context_loss', array(0.12798193, dtype=float32))])\n",
      "OrderedDict([('task_embedding/kl_lambda', 0.7941296281990528), ('task_embedding/kl_divergence', array(0.14488088, dtype=float32)), ('task_embedding/kl_loss', array(0.11505419, dtype=float32)), ('task_embedding/reward_prediction_loss', array(0.00205588, dtype=float32)), ('task_embedding/context_loss', array(0.11711007, dtype=float32))])\n",
      "OrderedDict([('task_embedding/kl_lambda', 0.8021838885585818), ('task_embedding/kl_divergence', array(0.12945817, dtype=float32)), ('task_embedding/kl_loss', array(0.10384926, dtype=float32)), ('task_embedding/reward_prediction_loss', array(0.00074399, dtype=float32)), ('task_embedding/context_loss', array(0.10459325, dtype=float32))])\n",
      "OrderedDict([('task_embedding/kl_lambda', 0.8099984339846871), ('task_embedding/kl_divergence', array(0.11513597, dtype=float32)), ('task_embedding/kl_loss', array(0.09325995, dtype=float32)), ('task_embedding/reward_prediction_loss', array(0.00068862, dtype=float32)), ('task_embedding/context_loss', array(0.09394857, dtype=float32))])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('task_embedding/kl_lambda', 0.8175744761936437), ('task_embedding/kl_divergence', array(0.10187259, dtype=float32)), ('task_embedding/kl_loss', array(0.08328843, dtype=float32)), ('task_embedding/reward_prediction_loss', array(0.00015292, dtype=float32)), ('task_embedding/context_loss', array(0.08344135, dtype=float32))])\n",
      "OrderedDict([('task_embedding/kl_lambda', 0.8249137318359602), ('task_embedding/kl_divergence', array(0.08961202, dtype=float32)), ('task_embedding/kl_loss', array(0.07392219, dtype=float32)), ('task_embedding/reward_prediction_loss', array(0.0035808, dtype=float32)), ('task_embedding/context_loss', array(0.07750299, dtype=float32))])\n",
      "OrderedDict([('task_embedding/kl_lambda', 0.8320183851339245), ('task_embedding/kl_divergence', array(0.07831882, dtype=float32)), ('task_embedding/kl_loss', array(0.0651627, dtype=float32)), ('task_embedding/reward_prediction_loss', array(0.00047984, dtype=float32)), ('task_embedding/context_loss', array(0.06564254, dtype=float32))])\n",
      "OrderedDict([('task_embedding/kl_lambda', 0.8388910504234147), ('task_embedding/kl_divergence', array(0.06795398, dtype=float32)), ('task_embedding/kl_loss', array(0.05700599, dtype=float32)), ('task_embedding/reward_prediction_loss', array(0.00031642, dtype=float32)), ('task_embedding/context_loss', array(0.05732241, dtype=float32))])\n",
      "OrderedDict([('task_embedding/kl_lambda', 0.8455347349164652), ('task_embedding/kl_divergence', array(0.05847834, dtype=float32)), ('task_embedding/kl_loss', array(0.04944547, dtype=float32)), ('task_embedding/reward_prediction_loss', array(0.00042062, dtype=float32)), ('task_embedding/context_loss', array(0.04986609, dtype=float32))])\n",
      "OrderedDict([('task_embedding/kl_lambda', 0.8519528019683106), ('task_embedding/kl_divergence', array(0.04985593, dtype=float32)), ('task_embedding/kl_loss', array(0.0424749, dtype=float32)), ('task_embedding/reward_prediction_loss', array(0.00088982, dtype=float32)), ('task_embedding/context_loss', array(0.04336471, dtype=float32))])\n",
      "OrderedDict([('task_embedding/kl_lambda', 0.8581489350995123), ('task_embedding/kl_divergence', array(0.04205161, dtype=float32)), ('task_embedding/kl_loss', array(0.03608654, dtype=float32)), ('task_embedding/reward_prediction_loss', array(0.0028269, dtype=float32)), ('task_embedding/context_loss', array(0.03891344, dtype=float32))])\n",
      "OrderedDict([('task_embedding/kl_lambda', 0.8641271029909058), ('task_embedding/kl_divergence', array(0.0350325, dtype=float32)), ('task_embedding/kl_loss', array(0.03027253, dtype=float32)), ('task_embedding/reward_prediction_loss', array(0.00307794, dtype=float32)), ('task_embedding/context_loss', array(0.03335046, dtype=float32))])\n",
      "OrderedDict([('task_embedding/kl_lambda', 0.8698915256370021), ('task_embedding/kl_divergence', array(0.02876666, dtype=float32)), ('task_embedding/kl_loss', array(0.02502388, dtype=float32)), ('task_embedding/reward_prediction_loss', array(0.00020485, dtype=float32)), ('task_embedding/context_loss', array(0.02522872, dtype=float32))])\n",
      "OrderedDict([('task_embedding/kl_lambda', 0.8754466418125836), ('task_embedding/kl_divergence', array(0.02322188, dtype=float32)), ('task_embedding/kl_loss', array(0.02032952, dtype=float32)), ('task_embedding/reward_prediction_loss', array(0.00036455, dtype=float32)), ('task_embedding/context_loss', array(0.02069407, dtype=float32))])\n",
      "OrderedDict([('task_embedding/kl_lambda', 0.8807970779778823), ('task_embedding/kl_divergence', array(0.01836828, dtype=float32)), ('task_embedding/kl_loss', array(0.01617873, dtype=float32)), ('task_embedding/reward_prediction_loss', array(0.0004362, dtype=float32)), ('task_embedding/context_loss', array(0.01661493, dtype=float32))])\n",
      "OrderedDict([('task_embedding/kl_lambda', 0.8859476187202091), ('task_embedding/kl_divergence', array(0.01417432, dtype=float32)), ('task_embedding/kl_loss', array(0.0125577, dtype=float32)), ('task_embedding/reward_prediction_loss', array(0.00029887, dtype=float32)), ('task_embedding/context_loss', array(0.01285657, dtype=float32))])\n",
      "OrderedDict([('task_embedding/kl_lambda', 0.8909031788043871), ('task_embedding/kl_divergence', array(0.01060978, dtype=float32)), ('task_embedding/kl_loss', array(0.00945228, dtype=float32)), ('task_embedding/reward_prediction_loss', array(0.00042299, dtype=float32)), ('task_embedding/context_loss', array(0.00987527, dtype=float32))])\n",
      "OrderedDict([('task_embedding/kl_lambda', 0.8956687768809987), ('task_embedding/kl_divergence', array(0.00764192, dtype=float32)), ('task_embedding/kl_loss', array(0.00684463, dtype=float32)), ('task_embedding/reward_prediction_loss', array(0.00233363, dtype=float32)), ('task_embedding/context_loss', array(0.00917826, dtype=float32))])\n",
      "OrderedDict([('task_embedding/kl_lambda', 0.9002495108803148), ('task_embedding/kl_divergence', array(0.00523713, dtype=float32)), ('task_embedding/kl_loss', array(0.00471472, dtype=float32)), ('task_embedding/reward_prediction_loss', array(0.00071963, dtype=float32)), ('task_embedding/context_loss', array(0.00543436, dtype=float32))])\n",
      "OrderedDict([('task_embedding/kl_lambda', 0.9046505351008906), ('task_embedding/kl_divergence', array(0.00335935, dtype=float32)), ('task_embedding/kl_loss', array(0.00303904, dtype=float32)), ('task_embedding/reward_prediction_loss', array(0.00085191, dtype=float32)), ('task_embedding/context_loss', array(0.00389094, dtype=float32))])\n",
      "OrderedDict([('task_embedding/kl_lambda', 0.9088770389851438), ('task_embedding/kl_divergence', array(0.00196623, dtype=float32)), ('task_embedding/kl_loss', array(0.00178707, dtype=float32)), ('task_embedding/reward_prediction_loss', array(0.00072443, dtype=float32)), ('task_embedding/context_loss', array(0.00251149, dtype=float32))])\n",
      "OrderedDict([('task_embedding/kl_lambda', 0.9129342275597286), ('task_embedding/kl_divergence', array(0.00100822, dtype=float32)), ('task_embedding/kl_loss', array(0.00092044, dtype=float32)), ('task_embedding/reward_prediction_loss', array(0.00085776, dtype=float32)), ('task_embedding/context_loss', array(0.00177819, dtype=float32))])\n",
      "OrderedDict([('task_embedding/kl_lambda', 0.9168273035060777), ('task_embedding/kl_divergence', array(0.00042269, dtype=float32)), ('task_embedding/kl_loss', array(0.00038753, dtype=float32)), ('task_embedding/reward_prediction_loss', array(0.00097659, dtype=float32)), ('task_embedding/context_loss', array(0.00136412, dtype=float32))])\n",
      "OrderedDict([('task_embedding/kl_lambda', 0.9205614508160216), ('task_embedding/kl_divergence', array(0.00012793, dtype=float32)), ('task_embedding/kl_loss', array(0.00011777, dtype=float32)), ('task_embedding/reward_prediction_loss', array(0.0006309, dtype=float32)), ('task_embedding/context_loss', array(0.00074867, dtype=float32))])\n",
      "OrderedDict([('task_embedding/kl_lambda', 0.9241418199787566), ('task_embedding/kl_divergence', array(2.2312859e-05, dtype=float32)), ('task_embedding/kl_loss', array(2.0620246e-05, dtype=float32)), ('task_embedding/reward_prediction_loss', array(0.00214322, dtype=float32)), ('task_embedding/context_loss', array(0.00216384, dtype=float32))])\n",
      "OrderedDict([('task_embedding/kl_lambda', 0.9275735146384823), ('task_embedding/kl_divergence', array(1.5240221e-06, dtype=float32)), ('task_embedding/kl_loss', array(1.4136425e-06, dtype=float32)), ('task_embedding/reward_prediction_loss', array(0.0008171, dtype=float32)), ('task_embedding/context_loss', array(0.00081852, dtype=float32))])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-90-d965f37a24a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m         replay_buffer.sample_context(\n\u001b[1;32m     14\u001b[0m         \u001b[0mtask_indices\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mTASK_EMBEDDING_BATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     ))\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/railrl-private/rlkit/data_management/multitask_replay_buffer.py\u001b[0m in \u001b[0;36msample_context\u001b[0;34m(self, indices, batch_size)\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;31m# context = [torch.cat(x, dim=0) for x in context]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m         \u001b[0;31m# full context consists of [obs, act, rewards, next_obs, terms]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;31m# if dynamics don't change across tasks, don't include next_obs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/railrl-private/rlkit/data_management/multitask_replay_buffer.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    150\u001b[0m         \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;31m# context = [torch.cat(x, dim=0) for x in context]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 152\u001b[0;31m         \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    153\u001b[0m         \u001b[0;31m# full context consists of [obs, act, rewards, next_obs, terms]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \u001b[0;31m# if dynamics don't change across tasks, don't include next_obs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mconcatenate\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(NUM_BATCHES):\n",
    "    if i % LOGGING_PERIOD == 0:\n",
    "        trainer._need_to_update_eval_statistics = True\n",
    "    task_indices = np.random.choice(\n",
    "        TRAIN_TASKS, META_BATCH_SIZE,\n",
    "    )\n",
    "    train_data = replay_buffer.sample_batch(\n",
    "        task_indices,\n",
    "        BATCH_SIZE,\n",
    "    )\n",
    "    train_data = np_to_pytorch_batch(train_data)\n",
    "    train_data['context'] = (\n",
    "        replay_buffer.sample_context(\n",
    "        task_indices,\n",
    "        TASK_EMBEDDING_BATCH_SIZE,\n",
    "    ))\n",
    "\n",
    "    trainer.train_from_torch(train_data, learn_task_z=True)\n",
    "    if i % LOGGING_PERIOD == 0:\n",
    "        print(trainer.eval_statistics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(trainer.context_encoder.state_dict(), 'encoder.pt')\n",
    "torch.save(trainer.reward_predictor.state_dict(), 'decoder.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [-0.],\n",
      "        [0.],\n",
      "        [1.],\n",
      "        [0.],\n",
      "        [-0.],\n",
      "        [-0.],\n",
      "        [0.]], device='cuda:0', grad_fn=<RoundBackward>)\n",
      "tensor([[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [1.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [1.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [1.]]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "train_data = validation_buffer.sample_batch(\n",
    "    [0],\n",
    "    10,\n",
    ")\n",
    "train_data = np_to_pytorch_batch(train_data)\n",
    "\n",
    "\n",
    "obs = train_data['observations']\n",
    "nobs = train_data['next_observations']\n",
    "actions = train_data['actions']\n",
    "rewards = train_data['rewards']\n",
    "contexts = (\n",
    "    replay_buffer.sample_context(\n",
    "    [0],\n",
    "    TASK_EMBEDDING_BATCH_SIZE,\n",
    "))\n",
    "task_embeddings = agent.latent_posterior(contexts)\n",
    "task_z = task_embeddings.rsample()\n",
    "t, b, _ = obs.size()\n",
    "obs = obs.view(t * b, -1)\n",
    "nobs = nobs.view(t * b, -1)\n",
    "actions = actions.view(t * b, -1)\n",
    "task_z = [z.repeat(b, 1) for z in task_z]\n",
    "task_z = torch.cat(task_z, dim=0)\n",
    "reward_pred = context_decoder(obs, actions, task_z)\n",
    "\n",
    "print(torch.round(reward_pred))\n",
    "print(rewards)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
