{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "base_dir = '/home/vitchyr/git/rllab-rail/railrl/data/replay_buffer'\n",
    "\n",
    "all_actions = np.loadtxt(base_dir + \"/actions.csv\", delimiter=',')\n",
    "all_obs = np.loadtxt(base_dir + \"/obs.csv\", delimiter=',')\n",
    "all_rewards = np.loadtxt(base_dir + \"/rewards.csv\", delimiter=',')\n",
    "all_terminals = np.loadtxt(base_dir + \"/terminals.csv\", delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "nonzero = [i for i, e in enumerate(all_terminals) if e != 0]\n",
    "last_full_episode_idx = nonzero[-1] + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "terminals = all_terminals[:last_full_episode_idx]\n",
    "obs = all_obs[:last_full_episode_idx]\n",
    "next_obs = all_obs[1:last_full_episode_idx+1]\n",
    "actions = all_actions[:last_full_episode_idx]\n",
    "rewards = all_rewards[:last_full_episode_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "positive_idxs = np.array([i for i, reward in enumerate(rewards) if reward == 1.0])\n",
    "negative_idxs = np.array([i for i, reward in enumerate(rewards) if reward == -1.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# -5 to give the first observation in the sequence\n",
    "Xpos = np.hstack((\n",
    "    actions[positive_idxs][:, :],\n",
    "    obs[positive_idxs-5][:, :],\n",
    "    next_obs[positive_idxs]\n",
    "))\n",
    "Xneg = np.hstack((\n",
    "    actions[negative_idxs][:, :],\n",
    "    obs[negative_idxs-5][:, :],\n",
    "    next_obs[negative_idxs]\n",
    "))\n",
    "num_pos = Xpos.shape[0]\n",
    "num_neg = Xneg.shape[0]\n",
    "num_total = num_pos + num_neg\n",
    "\n",
    "\n",
    "raw_X_posneg = np.vstack((Xpos, Xneg))\n",
    "raw_y_posneg = np.hstack((np.ones(num_pos), np.zeros(num_neg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_posneg, y_posneg = shuffle(raw_X_posneg, raw_y_posneg, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# TensorFlow model to train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Shuffle and build data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.81944444  0.75694444  0.79861111  0.8041958   0.72027972  0.78873239\n",
      "  0.78169014  0.71830986  0.74647887  0.77464789]\n",
      "0.770933467941\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(LogisticRegression(), X_posneg, y_posneg, scoring='accuracy', cv=10)\n",
    "print(scores)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.771241830065\n",
      "0.759103641457\n",
      "0.77497665733\n"
     ]
    }
   ],
   "source": [
    "rs = ShuffleSplit(n_splits=3, test_size=.25, random_state=0)\n",
    "for train_index, test_index in rs.split(X_posneg):\n",
    "    X = X_posneg[train_index]\n",
    "    y = y_posneg[train_index]\n",
    "    model = LogisticRegression()\n",
    "    model = model.fit(X, y)\n",
    "    print(model.score(X, y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "\n",
    "import torch\n",
    "import torch.autograd\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def get_torch_iterator(X, y, batch_size=32):\n",
    "    i = 0\n",
    "    num_elements = len(X)\n",
    "    while True:\n",
    "        yield Variable(X[i:i+batch_size]), Variable(y[i:i+batch_size])\n",
    "        i = (i + batch_size) % num_elements\n",
    "def label(y):\n",
    "    return np.round(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "X_torch_pn = torch.from_numpy(X_posneg).float()\n",
    "y_torch_pn = torch.from_numpy(y_posneg).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RegressionNet(nn.Module):\n",
    "    def __init__(self, feature_dim, hidden_sizes):\n",
    "        super().__init__()\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fcs = []\n",
    "        last_size = feature_dim\n",
    "        for size in hidden_sizes:\n",
    "            self.fcs.append(nn.Linear(last_size, size))\n",
    "            last_size = size\n",
    "        self.last_fc = nn.Linear(last_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        for fc in self.fcs:\n",
    "            x = F.relu(fc(x))\n",
    "        x = self.last_fc(x)\n",
    "        x = F.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "feature_dim = X_posneg.shape[1]\n",
    "hidden_sizes = [100, 64, 32]\n",
    "regression_net = RegressionNet(feature_dim, hidden_sizes)\n",
    "batch_iterator = get_torch_iterator(X_torch_pn, y_torch_pn)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(regression_net.parameters(), lr=0.001)\n",
    "for _ in range(10000):\n",
    "    # Get data\n",
    "    batch_x, batch_y = next(batch_iterator)\n",
    "\n",
    "    # Reset gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    output = regression_net(batch_x)\n",
    "    loss = criterion(output, batch_y)\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Apply gradients\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Loss 0.238116\n",
      "Accuracy 0.579831932773\n"
     ]
    }
   ],
   "source": [
    "yhats_var_pn = regression_net(Variable(X_torch_pn))\n",
    "loss = criterion(yhats_var_pn, Variable(y_torch_pn)).data.numpy()[0]\n",
    "\n",
    "yhats_numpy_pn = yhats_var_pn.data.numpy().flatten()\n",
    "\n",
    "print(\"MSE Loss\", loss)\n",
    "print(\"Accuracy\", np.mean(label(y_posneg) == label(yhats_numpy_pn)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One-hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class OneHotNet(nn.Module):\n",
    "\n",
    "    def __init__(self, feature_dim, hidden_sizes, num_classes):\n",
    "        super().__init__()\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fcs = []\n",
    "        last_size = feature_dim\n",
    "        for size in hidden_sizes:\n",
    "            self.fcs.append(nn.Linear(last_size, size))\n",
    "            last_size = size\n",
    "        self.last_fc = nn.Linear(last_size, num_classes)\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        for fc in self.fcs:\n",
    "            x = F.relu(fc(x))\n",
    "        x = self.last_fc(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def to_onehot_n(inds, dim):\n",
    "    ret = np.zeros((len(inds), dim))\n",
    "    ret[np.arange(len(inds)), inds] = 1\n",
    "    return ret\n",
    "all_y_onehot_3 = to_onehot_n((rewards+1).astype(int), 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## two-way one-hot vector for rward of +/- 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "feature_dim = X_posneg.shape[1]\n",
    "num_classes = 2\n",
    "hidden_sizes = [100, 64, 64]\n",
    "net2 = OneHotNet(feature_dim, hidden_sizes, num_classes)\n",
    "batch_iterator2 = get_torch_iterator(X_torch_pn, y_torch_pn.long())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net2.parameters(), lr=0.001)\n",
    "for _ in range(10000):\n",
    "    # Get data\n",
    "    batch_x, batch_y = next(batch_iterator2)\n",
    "\n",
    "    # Reset gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    output = net2(batch_x)\n",
    "    loss = criterion(output, batch_y)\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Apply gradients\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.59243697479\n",
      "Cross Entropy 0.665516\n",
      "MSE Onehots 0.237172399611\n"
     ]
    }
   ],
   "source": [
    "yhat2_torch = net2(Variable(X_torch_pn))\n",
    "yhat2_numpy = yhat2_torch.data.numpy()\n",
    "yhat2_label = np.argmax(yhat2_numpy, axis=1)\n",
    "y_onehot_pn = to_onehot_n(y_posneg.astype(int), 2)\n",
    "\n",
    "\n",
    "loss = criterion(yhat2_torch, Variable(y_torch_pn.long()))\n",
    "print(\"Accuracy\", np.mean(y_posneg == yhat2_label))\n",
    "print(\"Cross Entropy\", loss.data.numpy()[0])\n",
    "print(\"MSE Onehots\", np.mean((yhat2_numpy-y_onehot_pn)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[723   0]\n",
      " [705   0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cnf_matrix = confusion_matrix(y_posneg, yhat2_label)\n",
    "print(cnf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Three-way one-hot vector for reward of +1, 0, or -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def to_onehot_n(inds, dim):\n",
    "    ret = np.zeros((len(inds), dim))\n",
    "    ret[np.arange(len(inds)), inds] = 1\n",
    "    return ret\n",
    "all_y_onehot = to_onehot_n((rewards+1).astype(int), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_X = np.hstack((\n",
    "    actions,\n",
    "    obs,\n",
    "    next_obs\n",
    "))\n",
    "all_X_torch = torch.from_numpy(all_X).float()\n",
    "all_y_torch = torch.from_numpy(rewards + 1).long()\n",
    "all_Xv = Variable(all_X_torch)\n",
    "all_Yv = Variable(all_y_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "feature_dim = all_X_torch.size()[1]\n",
    "num_classes = 3\n",
    "hidden_sizes = [100, 3]\n",
    "net = OneHotNet(feature_dim, hidden_sizes, num_classes)\n",
    "batch_iterator = get_torch_iterator(all_X_torch, all_y_torch)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "for _ in range(1000):\n",
    "    # Get data\n",
    "    batch_x, batch_y = next(batch_iterator)\n",
    "    \n",
    "    # Reset gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    output = net(batch_x)\n",
    "    loss = criterion(output, batch_y)\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Apply gradients\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyze model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy 0.857128564282\n",
      "Cross Entropy 0.772818\n",
      "MSE Onehots 0.0924693607724\n"
     ]
    }
   ],
   "source": [
    "loss = criterion(net(all_Xv), all_Yv).data.numpy()\n",
    "yhat_soft = net(all_Xv).data.numpy()\n",
    "y = all_Yv.data.numpy().astype(int)\n",
    "yhat = np.argmax(yhat_soft, axis=1)\n",
    "print(\"Accuracy\", np.mean(y == yhat))\n",
    "print(\"Cross Entropy\", np.mean(loss))\n",
    "print(\"MSE Onehots\", np.mean((yhat_soft-all_y_onehot)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0  723    0]\n",
      " [   0 8567    0]\n",
      " [   0  705    0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cnf_matrix = confusion_matrix(y, yhat)\n",
    "print(cnf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
