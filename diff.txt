diff --git a/.gitignore b/.gitignore
index 50d1f40..6877510 100644
--- a/.gitignore
+++ b/.gitignore
@@ -30,6 +30,3 @@ railrl/demos/spacemouse/config.py
 *.img
 MANIFEST
 *.egg-info
-demos/
-reset.p
-gitignore/
diff --git a/demo_rewards (another copy).png b/demo_rewards (another copy).png
new file mode 100644
index 0000000..78f7d47
Binary files /dev/null and b/demo_rewards (another copy).png differ
diff --git a/demo_rewards (copy).png b/demo_rewards (copy).png
new file mode 100644
index 0000000..292ce54
Binary files /dev/null and b/demo_rewards (copy).png differ
diff --git a/experiments/ashvin/corl2019/debug/pointmass2/ccrig1.py b/experiments/ashvin/corl2019/debug/pointmass2/ccrig1.py
deleted file mode 100644
index 6149622..0000000
--- a/experiments/ashvin/corl2019/debug/pointmass2/ccrig1.py
+++ /dev/null
@@ -1,206 +0,0 @@
-import railrl.misc.hyperparameter as hyp
-from experiments.murtaza.multiworld.skew_fit.reacher.generate_uniform_dataset import generate_uniform_dataset_reacher
-from multiworld.envs.mujoco.cameras import sawyer_init_camera_zoomed_in, sawyer_pusher_camera_upright_v2
-from railrl.launchers.launcher_util import run_experiment
-import railrl.torch.vae.vae_schedules as vae_schedules
-from railrl.torch.vae.conv_vae import imsize48_default_architecture, imsize48_default_architecture_with_more_hidden_layers
-from railrl.launchers.arglauncher import run_variants
-from railrl.torch.grill.cvae_experiments import (
-    grill_her_td3_offpolicy_online_vae_full_experiment,
-)
-from multiworld.envs.pygame.multiobject_pygame_env import Multiobj2DWallEnv
-from multiworld.envs.mujoco.sawyer_xyz.sawyer_push_multiobj_subset import SawyerMultiobjectEnv
-from railrl.torch.vae.conditional_conv_vae import DeltaCVAE
-from railrl.torch.vae.conditional_vae_trainer import DeltaCVAETrainer
-from railrl.data_management.online_conditional_vae_replay_buffer import \
-        OnlineConditionalVaeRelabelingBuffer
-
-from multiworld.envs.pygame.point2d import Point2DWallEnv
-
-x_var = 0.2
-x_low = -x_var
-x_high = x_var
-y_low = 0.5
-y_high = 0.7
-t = 0.05
-
-if __name__ == "__main__":
-    variant = dict(
-        double_algo=False,
-        online_vae_exploration=False,
-        imsize=48,
-        init_camera=sawyer_init_camera_zoomed_in,
-        
-        env_class=Multiobj2DWallEnv,
-        env_kwargs=dict(
-           render_onscreen=False,
-           ball_radius=1.2,
-           wall_thickness=1.5,
-           inner_wall_max_dist=1.5,
-           images_are_rgb=True,
-           show_goal=False,
-           change_colors=True,
-           change_walls=True,
-       ),
-
-        grill_variant=dict(
-            save_video=True,
-            custom_goal_sampler='replay_buffer',
-            online_vae_trainer_kwargs=dict(
-                beta=20,
-                lr=0,
-            ),
-            save_video_period=50,
-            qf_kwargs=dict(
-                hidden_sizes=[400, 300],
-            ),
-            policy_kwargs=dict(
-                hidden_sizes=[400, 300],
-            ),
-            vf_kwargs=dict(
-                hidden_sizes=[400, 300],
-            ),
-            max_path_length=100,
-            algo_kwargs=dict(
-                batch_size=128,
-                num_epochs=1000,
-                num_eval_steps_per_epoch=1000,
-                num_expl_steps_per_train_loop=1000,
-                num_trains_per_train_loop=1000,
-                min_num_steps_before_training=1000,
-                vae_training_schedule=vae_schedules.never_train,
-                oracle_data=False,
-                vae_save_period=25,
-                parallel_vae_train=False,
-                dataset_path=None,
-                rl_offpolicy_num_training_steps=0,
-            ),
-            td3_trainer_kwargs=dict(
-                discount=0.99,
-                # min_num_steps_before_training=4000,
-                reward_scale=1.0,
-                # render=False,
-                tau=1e-2,
-            ),
-            replay_buffer_class=OnlineConditionalVaeRelabelingBuffer,
-            replay_buffer_kwargs=dict(
-                start_skew_epoch=10,
-                max_size=int(100000),
-                fraction_goals_rollout_goals=0.2,
-                fraction_goals_env_goals=0.5,
-                exploration_rewards_type='None',
-                vae_priority_type='vae_prob',
-                priority_function_kwargs=dict(
-                    sampling_method='importance_sampling',
-                    decoder_distribution='gaussian_identity_variance',
-                    # decoder_distribution='bernoulli',
-                    num_latents_to_sample=10,
-                ),
-                power=-1,
-                relabeling_goal_sampling_mode='vae_prior',
-            ),
-            exploration_goal_sampling_mode='vae_prior',
-            evaluation_goal_sampling_mode='reset_of_env',
-            normalize=False,
-            render=False,
-            exploration_noise=0.2,
-            exploration_type='ou',
-            training_mode='train',
-            testing_mode='test',
-            reward_params=dict(
-                epsilon=0.05,
-            ),
-            observation_key='latent_observation',
-            desired_goal_key='latent_desired_goal',
-            vae_wrapped_env_kwargs=dict(
-                sample_from_true_prior=True,
-            ),
-            algorithm='ONLINE-VAE-SAC-BERNOULLI',
-            # vae_path="datasets/pusher_color_spectrum/itr_1900.pkl",
-        ),
-        train_vae_variant=dict(
-            latent_sizes=4,
-            beta=10,
-            beta_schedule_kwargs=dict(
-                x_values=(0, 1500),
-                y_values=(1, 50),
-            ),
-            num_epochs=1000,
-            dump_skew_debug_plots=False,
-            # decoder_activation='gaussian',
-            decoder_activation='sigmoid',
-            use_linear_dynamics=False,
-            generate_vae_dataset_kwargs=dict(
-                N=102000,
-                n_random_steps=51,
-                test_p=.9,
-                use_cached=False,
-                show=False,
-                oracle_dataset=False,
-                oracle_dataset_using_set_to_goal=False,
-                non_presampled_goal_img_is_garbage=False,
-                random_rollout_data=True,
-                random_rollout_data_set_to_goal=False,
-                conditional_vae_dataset=True,
-                save_trajectories=False,
-                enviorment_dataset=False,
-                tag="ccrig1",
-            ),
-            vae_trainer_class=DeltaCVAETrainer,
-            vae_class=DeltaCVAE,
-            vae_kwargs=dict(
-                input_channels=3,
-                architecture=imsize48_default_architecture_with_more_hidden_layers,
-                decoder_distribution='gaussian_identity_variance',
-            ),
-            # TODO: why the redundancy?
-            algo_kwargs=dict(
-                start_skew_epoch=5000,
-                is_auto_encoder=False,
-                batch_size=32,
-                lr=1e-3,
-                skew_config=dict(
-                    method='vae_prob',
-                    power=0,
-                ),
-                skew_dataset=False,
-                priority_function_kwargs=dict(
-                    decoder_distribution='gaussian_identity_variance',
-                    sampling_method='importance_sampling',
-                    # sampling_method='true_prior_sampling',
-                    num_latents_to_sample=10,
-                ),
-                use_parallel_dataloading=False,
-            ),
-
-            save_period=25,
-        ),
-        region='us-west-2',
-
-        logger_variant=dict(
-            tensorboard=True,
-        ),
-
-        slurm_variant=dict(
-            timeout_min=48 * 60,
-            cpus_per_task=10,
-            gpus_per_node=1,
-        ),
-    )
-
-    search_space = {
-        'seedid': range(5),
-        'grill_variant.exploration_noise': [0.2, ],
-        'train_vae_variant.latent_sizes': [(4, 4)],
-        'grill_variant.algo_kwargs.num_trains_per_train_loop':[1000, ],
-        'grill_variant.algo_kwargs.batch_size': [128, ],
-    }
-    sweeper = hyp.DeterministicHyperparameterSweeper(
-        search_space, default_parameters=variant,
-    )
-
-    variants = []
-    for variant in sweeper.iterate_hyperparameters():
-        variants.append(variant)
-
-    run_variants(grill_her_td3_offpolicy_online_vae_full_experiment, variants, run_id=0)
diff --git a/experiments/ashvin/corl2019/debug/pointmass2/ccrig2.py b/experiments/ashvin/corl2019/debug/pointmass2/ccrig2.py
deleted file mode 100644
index f68f3c8..0000000
--- a/experiments/ashvin/corl2019/debug/pointmass2/ccrig2.py
+++ /dev/null
@@ -1,206 +0,0 @@
-import railrl.misc.hyperparameter as hyp
-from experiments.murtaza.multiworld.skew_fit.reacher.generate_uniform_dataset import generate_uniform_dataset_reacher
-from multiworld.envs.mujoco.cameras import sawyer_init_camera_zoomed_in, sawyer_pusher_camera_upright_v2
-from railrl.launchers.launcher_util import run_experiment
-import railrl.torch.vae.vae_schedules as vae_schedules
-from railrl.torch.vae.conv_vae import imsize48_default_architecture, imsize48_default_architecture_with_more_hidden_layers
-from railrl.launchers.arglauncher import run_variants
-from railrl.torch.grill.cvae_experiments import (
-    grill_her_td3_offpolicy_online_vae_full_experiment,
-)
-from multiworld.envs.pygame.multiobject_pygame_env import Multiobj2DWallEnv
-from multiworld.envs.mujoco.sawyer_xyz.sawyer_push_multiobj_subset import SawyerMultiobjectEnv
-from railrl.torch.vae.conditional_conv_vae import DeltaCVAE
-from railrl.torch.vae.conditional_vae_trainer import DeltaCVAETrainer
-from railrl.data_management.online_conditional_vae_replay_buffer import \
-        OnlineConditionalVaeRelabelingBuffer
-
-from multiworld.envs.pygame.point2d import Point2DWallEnv
-
-x_var = 0.2
-x_low = -x_var
-x_high = x_var
-y_low = 0.5
-y_high = 0.7
-t = 0.05
-
-if __name__ == "__main__":
-    variant = dict(
-        double_algo=False,
-        online_vae_exploration=False,
-        imsize=48,
-        init_camera=sawyer_init_camera_zoomed_in,
-        
-        env_class=Multiobj2DWallEnv,
-        env_kwargs=dict(
-           render_onscreen=False,
-           ball_radius=1.2,
-           wall_thickness=1.5,
-           inner_wall_max_dist=1.5,
-           images_are_rgb=True,
-           show_goal=False,
-           change_colors=True,
-           change_walls=True,
-       ),
-
-        grill_variant=dict(
-            save_video=True,
-            custom_goal_sampler='replay_buffer',
-            online_vae_trainer_kwargs=dict(
-                beta=20,
-                lr=0,
-            ),
-            save_video_period=50,
-            qf_kwargs=dict(
-                hidden_sizes=[400, 300],
-            ),
-            policy_kwargs=dict(
-                hidden_sizes=[400, 300],
-            ),
-            vf_kwargs=dict(
-                hidden_sizes=[400, 300],
-            ),
-            max_path_length=100,
-            algo_kwargs=dict(
-                batch_size=128,
-                num_epochs=1000,
-                num_eval_steps_per_epoch=1000,
-                num_expl_steps_per_train_loop=1000,
-                num_trains_per_train_loop=1000,
-                min_num_steps_before_training=1000,
-                vae_training_schedule=vae_schedules.never_train,
-                oracle_data=False,
-                vae_save_period=25,
-                parallel_vae_train=False,
-                dataset_path=None,
-                rl_offpolicy_num_training_steps=0,
-            ),
-            td3_trainer_kwargs=dict(
-                discount=0.99,
-                # min_num_steps_before_training=4000,
-                reward_scale=1.0,
-                # render=False,
-                tau=1e-2,
-            ),
-            replay_buffer_class=OnlineConditionalVaeRelabelingBuffer,
-            replay_buffer_kwargs=dict(
-                start_skew_epoch=10,
-                max_size=int(100000),
-                fraction_goals_rollout_goals=0.2,
-                fraction_goals_env_goals=0.5,
-                exploration_rewards_type='None',
-                vae_priority_type='vae_prob',
-                priority_function_kwargs=dict(
-                    sampling_method='importance_sampling',
-                    decoder_distribution='gaussian_identity_variance',
-                    # decoder_distribution='bernoulli',
-                    num_latents_to_sample=10,
-                ),
-                power=-1,
-                relabeling_goal_sampling_mode='vae_prior',
-            ),
-            exploration_goal_sampling_mode='vae_prior',
-            evaluation_goal_sampling_mode='reset_of_env',
-            normalize=False,
-            render=False,
-            exploration_noise=0.2,
-            exploration_type='ou',
-            training_mode='train',
-            testing_mode='test',
-            reward_params=dict(
-                epsilon=0.05,
-            ),
-            observation_key='latent_observation',
-            desired_goal_key='latent_desired_goal',
-            vae_wrapped_env_kwargs=dict(
-                sample_from_true_prior=True,
-            ),
-            algorithm='ONLINE-VAE-SAC-BERNOULLI',
-            # vae_path="datasets/pusher_color_spectrum/itr_1900.pkl",
-        ),
-        train_vae_variant=dict(
-            latent_sizes=4,
-            beta=10,
-            beta_schedule_kwargs=dict(
-                x_values=(0, 1500),
-                y_values=(1, 50),
-            ),
-            num_epochs=1500,
-            dump_skew_debug_plots=False,
-            # decoder_activation='gaussian',
-            decoder_activation='sigmoid',
-            use_linear_dynamics=False,
-            generate_vae_dataset_kwargs=dict(
-                N=102000,
-                n_random_steps=51,
-                test_p=.9,
-                use_cached=False,
-                show=False,
-                oracle_dataset=False,
-                oracle_dataset_using_set_to_goal=False,
-                non_presampled_goal_img_is_garbage=False,
-                random_rollout_data=True,
-                random_rollout_data_set_to_goal=False,
-                conditional_vae_dataset=True,
-                save_trajectories=False,
-                enviorment_dataset=False,
-                tag="ccrig1",
-            ),
-            vae_trainer_class=DeltaCVAETrainer,
-            vae_class=DeltaCVAE,
-            vae_kwargs=dict(
-                input_channels=3,
-                architecture=imsize48_default_architecture_with_more_hidden_layers,
-                decoder_distribution='gaussian_identity_variance',
-            ),
-            # TODO: why the redundancy?
-            algo_kwargs=dict(
-                start_skew_epoch=5000,
-                is_auto_encoder=False,
-                batch_size=32,
-                lr=1e-3,
-                skew_config=dict(
-                    method='vae_prob',
-                    power=0,
-                ),
-                skew_dataset=False,
-                priority_function_kwargs=dict(
-                    decoder_distribution='gaussian_identity_variance',
-                    sampling_method='importance_sampling',
-                    # sampling_method='true_prior_sampling',
-                    num_latents_to_sample=10,
-                ),
-                use_parallel_dataloading=False,
-            ),
-
-            save_period=25,
-        ),
-        region='us-west-2',
-
-        logger_variant=dict(
-            tensorboard=True,
-        ),
-
-        slurm_variant=dict(
-            timeout_min=48 * 60,
-            cpus_per_task=10,
-            gpus_per_node=1,
-        ),
-    )
-
-    search_space = {
-        'seedid': range(5),
-        'grill_variant.exploration_noise': [0.2, ],
-        'train_vae_variant.latent_sizes': [(4, 4), (2, 4)],
-        'grill_variant.algo_kwargs.num_trains_per_train_loop':[1000, ],
-        'grill_variant.algo_kwargs.batch_size': [128, ],
-    }
-    sweeper = hyp.DeterministicHyperparameterSweeper(
-        search_space, default_parameters=variant,
-    )
-
-    variants = []
-    for variant in sweeper.iterate_hyperparameters():
-        variants.append(variant)
-
-    run_variants(grill_her_td3_offpolicy_online_vae_full_experiment, variants, run_id=1)
diff --git a/experiments/ashvin/corl2019/debug/pointmass2/ccrig_debug.py b/experiments/ashvin/corl2019/debug/pointmass2/ccrig_debug.py
deleted file mode 100644
index 514570b..0000000
--- a/experiments/ashvin/corl2019/debug/pointmass2/ccrig_debug.py
+++ /dev/null
@@ -1,206 +0,0 @@
-import railrl.misc.hyperparameter as hyp
-from experiments.murtaza.multiworld.skew_fit.reacher.generate_uniform_dataset import generate_uniform_dataset_reacher
-from multiworld.envs.mujoco.cameras import sawyer_init_camera_zoomed_in, sawyer_pusher_camera_upright_v2
-from railrl.launchers.launcher_util import run_experiment
-import railrl.torch.vae.vae_schedules as vae_schedules
-from railrl.torch.vae.conv_vae import imsize48_default_architecture, imsize48_default_architecture_with_more_hidden_layers
-from railrl.launchers.arglauncher import run_variants
-from railrl.torch.grill.cvae_experiments import (
-    grill_her_td3_offpolicy_online_vae_full_experiment,
-)
-from multiworld.envs.pygame.multiobject_pygame_env import Multiobj2DWallEnv
-from multiworld.envs.mujoco.sawyer_xyz.sawyer_push_multiobj_subset import SawyerMultiobjectEnv
-from railrl.torch.vae.conditional_conv_vae import DeltaCVAE
-from railrl.torch.vae.conditional_vae_trainer import DeltaCVAETrainer
-from railrl.data_management.online_conditional_vae_replay_buffer import \
-        OnlineConditionalVaeRelabelingBuffer
-
-from multiworld.envs.pygame.point2d import Point2DWallEnv
-
-x_var = 0.2
-x_low = -x_var
-x_high = x_var
-y_low = 0.5
-y_high = 0.7
-t = 0.05
-
-if __name__ == "__main__":
-    variant = dict(
-        double_algo=False,
-        online_vae_exploration=False,
-        imsize=48,
-        init_camera=sawyer_init_camera_zoomed_in,
-        
-        env_class=Multiobj2DWallEnv,
-        env_kwargs=dict(
-           render_onscreen=False,
-           ball_radius=1.2,
-           wall_thickness=1.5,
-           inner_wall_max_dist=1.5,
-           images_are_rgb=True,
-           show_goal=False,
-           change_colors=True,
-           change_walls=True,
-       ),
-
-        grill_variant=dict(
-            save_video=True,
-            custom_goal_sampler='replay_buffer',
-            online_vae_trainer_kwargs=dict(
-                beta=20,
-                lr=0,
-            ),
-            save_video_period=50,
-            qf_kwargs=dict(
-                hidden_sizes=[400, 300],
-            ),
-            policy_kwargs=dict(
-                hidden_sizes=[400, 300],
-            ),
-            vf_kwargs=dict(
-                hidden_sizes=[400, 300],
-            ),
-            max_path_length=100,
-            algo_kwargs=dict(
-                batch_size=128,
-                num_epochs=100,
-                num_eval_steps_per_epoch=1000,
-                num_expl_steps_per_train_loop=1000,
-                num_trains_per_train_loop=1000,
-                min_num_steps_before_training=1000,
-                vae_training_schedule=vae_schedules.never_train,
-                oracle_data=False,
-                vae_save_period=25,
-                parallel_vae_train=False,
-                dataset_path=None,
-                rl_offpolicy_num_training_steps=0,
-            ),
-            td3_trainer_kwargs=dict(
-                discount=0.99,
-                # min_num_steps_before_training=4000,
-                reward_scale=1.0,
-                # render=False,
-                tau=1e-2,
-            ),
-            replay_buffer_class=OnlineConditionalVaeRelabelingBuffer,
-            replay_buffer_kwargs=dict(
-                start_skew_epoch=10,
-                max_size=int(100000),
-                fraction_goals_rollout_goals=0.2,
-                fraction_goals_env_goals=0.5,
-                exploration_rewards_type='None',
-                vae_priority_type='vae_prob',
-                priority_function_kwargs=dict(
-                    sampling_method='importance_sampling',
-                    decoder_distribution='gaussian_identity_variance',
-                    # decoder_distribution='bernoulli',
-                    num_latents_to_sample=10,
-                ),
-                power=-1,
-                relabeling_goal_sampling_mode='vae_prior',
-            ),
-            exploration_goal_sampling_mode='vae_prior',
-            evaluation_goal_sampling_mode='reset_of_env',
-            normalize=False,
-            render=False,
-            exploration_noise=0.2,
-            exploration_type='ou',
-            training_mode='train',
-            testing_mode='test',
-            reward_params=dict(
-                epsilon=0.05,
-            ),
-            observation_key='latent_observation',
-            desired_goal_key='latent_desired_goal',
-            vae_wrapped_env_kwargs=dict(
-                sample_from_true_prior=True,
-            ),
-            algorithm='ONLINE-VAE-SAC-BERNOULLI',
-            # vae_path="datasets/pusher_color_spectrum/itr_1900.pkl",
-        ),
-        train_vae_variant=dict(
-            latent_sizes=4,
-            beta=10,
-            beta_schedule_kwargs=dict(
-                x_values=(0, 1500),
-                y_values=(1, 50),
-            ),
-            num_epochs=5,
-            dump_skew_debug_plots=False,
-            # decoder_activation='gaussian',
-            decoder_activation='sigmoid',
-            use_linear_dynamics=False,
-            generate_vae_dataset_kwargs=dict(
-                N=1020,
-                n_random_steps=51,
-                test_p=.9,
-                use_cached=False,
-                show=False,
-                oracle_dataset=False,
-                oracle_dataset_using_set_to_goal=False,
-                non_presampled_goal_img_is_garbage=False,
-                random_rollout_data=True,
-                random_rollout_data_set_to_goal=False,
-                conditional_vae_dataset=True,
-                save_trajectories=False,
-                enviorment_dataset=False,
-                tag="ccrig1",
-            ),
-            vae_trainer_class=DeltaCVAETrainer,
-            vae_class=DeltaCVAE,
-            vae_kwargs=dict(
-                input_channels=3,
-                architecture=imsize48_default_architecture_with_more_hidden_layers,
-                decoder_distribution='gaussian_identity_variance',
-            ),
-            # TODO: why the redundancy?
-            algo_kwargs=dict(
-                start_skew_epoch=5000,
-                is_auto_encoder=False,
-                batch_size=32,
-                lr=1e-3,
-                skew_config=dict(
-                    method='vae_prob',
-                    power=0,
-                ),
-                skew_dataset=False,
-                priority_function_kwargs=dict(
-                    decoder_distribution='gaussian_identity_variance',
-                    sampling_method='importance_sampling',
-                    # sampling_method='true_prior_sampling',
-                    num_latents_to_sample=10,
-                ),
-                use_parallel_dataloading=False,
-            ),
-
-            save_period=25,
-        ),
-        region='us-west-2',
-
-        logger_variant=dict(
-            tensorboard=True,
-        ),
-
-        slurm_variant=dict(
-            timeout_min=48 * 60,
-            cpus_per_task=10,
-            gpus_per_node=1,
-        ),
-    )
-
-    search_space = {
-        'seedid': range(5),
-        'grill_variant.exploration_noise': [0.2, ],
-        'train_vae_variant.latent_sizes': [(4, 4)],
-        'grill_variant.algo_kwargs.num_trains_per_train_loop':[1000, ],
-        'grill_variant.algo_kwargs.batch_size': [128, ],
-    }
-    sweeper = hyp.DeterministicHyperparameterSweeper(
-        search_space, default_parameters=variant,
-    )
-
-    variants = []
-    for variant in sweeper.iterate_hyperparameters():
-        variants.append(variant)
-
-    run_variants(grill_her_td3_offpolicy_online_vae_full_experiment, variants, run_id=2)
diff --git a/experiments/ashvin/corl2019/debug/pointmass2/offpolicy_ccrig1.py b/experiments/ashvin/corl2019/debug/pointmass2/offpolicy_ccrig1.py
deleted file mode 100644
index 8fdf3cb..0000000
--- a/experiments/ashvin/corl2019/debug/pointmass2/offpolicy_ccrig1.py
+++ /dev/null
@@ -1,205 +0,0 @@
-import railrl.misc.hyperparameter as hyp
-from experiments.murtaza.multiworld.skew_fit.reacher.generate_uniform_dataset import generate_uniform_dataset_reacher
-from multiworld.envs.mujoco.cameras import sawyer_init_camera_zoomed_in, sawyer_pusher_camera_upright_v2
-from railrl.launchers.launcher_util import run_experiment
-from railrl.torch.grill.launcher import *
-import railrl.torch.vae.vae_schedules as vae_schedules
-from railrl.torch.vae.conv_vae import imsize48_default_architecture, imsize48_default_architecture_with_more_hidden_layers
-from railrl.launchers.arglauncher import run_variants
-from railrl.torch.grill.cvae_experiments import (
-    grill_her_td3_offpolicy_online_vae_full_experiment,
-)
-from multiworld.envs.pygame.multiobject_pygame_env import Multiobj2DWallEnv
-from multiworld.envs.mujoco.sawyer_xyz.sawyer_push_multiobj_subset import SawyerMultiobjectEnv
-from railrl.torch.vae.conditional_conv_vae import DeltaCVAE
-from railrl.torch.vae.vae_trainer import DeltaCVAETrainer
-from railrl.data_management.online_conditional_vae_replay_buffer import \
-        OnlineConditionalVaeRelabelingBuffer
-
-x_var = 0.2
-x_low = -x_var
-x_high = x_var
-y_low = 0.5
-y_high = 0.7
-t = 0
-
-if __name__ == "__main__":
-    variant = dict(
-        double_algo=False,
-        online_vae_exploration=False,
-        imsize=48,
-        init_camera=sawyer_init_camera_zoomed_in,
-
-        env_class=Multiobj2DWallEnv,
-        env_kwargs=dict(
-           render_onscreen=False,
-           ball_radius=1.2,
-           wall_thickness=1.5,
-           inner_wall_max_dist=1.5,
-           images_are_rgb=True,
-           show_goal=False,
-           change_colors=True,
-           change_walls=True,
-       ),
-
-        grill_variant=dict(
-            save_video=True,
-            custom_goal_sampler='replay_buffer',
-            online_vae_trainer_kwargs=dict(
-                beta=20,
-                lr=0,
-            ),
-            save_video_period=50,
-            qf_kwargs=dict(
-                hidden_sizes=[400, 300],
-            ),
-            policy_kwargs=dict(
-                hidden_sizes=[400, 300],
-            ),
-            vf_kwargs=dict(
-                hidden_sizes=[400, 300],
-            ),
-            max_path_length=50,
-            algo_kwargs=dict(
-                batch_size=128,
-                num_epochs=1001,
-                num_eval_steps_per_epoch=1000,
-                num_expl_steps_per_train_loop=1000,
-                num_trains_per_train_loop=500,
-                min_num_steps_before_training=1000,
-                vae_training_schedule=vae_schedules.never_train,
-                oracle_data=False,
-                vae_save_period=25,
-                parallel_vae_train=False,
-                dataset_path="/tmp/Multiobj2DWallEnv_N102000_sawyer_init_camera_zoomed_in_imsize48_random_oracle_split_0.npy",
-                rl_offpolicy_num_training_steps=0,
-            ),
-            td3_trainer_kwargs=dict(
-                discount=0.99,
-                # min_num_steps_before_training=4000,
-                reward_scale=1.0,
-                # render=False,
-                tau=1e-2,
-            ),
-            replay_buffer_class=OnlineConditionalVaeRelabelingBuffer,
-            replay_buffer_kwargs=dict(
-                start_skew_epoch=10,
-                max_size=int(100000),
-                fraction_goals_rollout_goals=0.2,
-                fraction_goals_env_goals=0.5,
-                exploration_rewards_type='None',
-                vae_priority_type='vae_prob',
-                priority_function_kwargs=dict(
-                    sampling_method='importance_sampling',
-                    decoder_distribution='gaussian_identity_variance',
-                    # decoder_distribution='bernoulli',
-                    num_latents_to_sample=10,
-                ),
-                power=-1,
-                relabeling_goal_sampling_mode='vae_prior',
-                save_decoded_to_internal_keys=False,
-            ),
-            exploration_goal_sampling_mode='vae_prior',
-            evaluation_goal_sampling_mode='reset_of_env',
-            normalize=False,
-            render=False,
-            exploration_noise=0.2,
-            exploration_type='ou',
-            training_mode='train',
-            testing_mode='test',
-            reward_params=dict(
-                epsilon=0.05,
-            ),
-            observation_key='latent_observation',
-            desired_goal_key='latent_desired_goal',
-            vae_wrapped_env_kwargs=dict(
-                sample_from_true_prior=True,
-            ),
-            algorithm='ONLINE-VAE-SAC-BERNOULLI',
-#             vae_path="ashvin/corl2019/offpolicy/dcvae2/run0/id0/itr_800.pkl",
-        ),
-        train_vae_variant=dict(
-#             representation_size=4,
-            beta=10,
-            beta_schedule_kwargs=dict(
-                x_values=(0, 1500),
-                y_values=(1, 50),
-            ),
-            num_epochs=1500,
-            dump_skew_debug_plots=False,
-            # decoder_activation='gaussian',
-            decoder_activation='sigmoid',
-            use_linear_dynamics=False,
-            generate_vae_dataset_kwargs=dict(
-                N=102000,
-                n_random_steps=51,
-                test_p=.9,
-                use_cached=False,
-                show=False,
-                oracle_dataset=True,
-                oracle_dataset_using_set_to_goal=True,
-                non_presampled_goal_img_is_garbage=False,
-                random_rollout_data=True,
-                conditional_vae_dataset=True,
-                save_trajectories=True,
-                enviorment_dataset=False,
-            ),
-            vae_trainer_class=DeltaCVAETrainer,
-            vae_class=DeltaCVAE,
-            vae_kwargs=dict(
-                input_channels=3,
-                architecture=imsize48_default_architecture_with_more_hidden_layers,
-                decoder_distribution='gaussian_identity_variance',
-            ),
-            # TODO: why the redundancy?
-            algo_kwargs=dict(
-                start_skew_epoch=5000,
-                is_auto_encoder=False,
-                batch_size=32,
-                lr=1e-3,
-                skew_config=dict(
-                    method='vae_prob',
-                    power=0,
-                ),
-                skew_dataset=False,
-                priority_function_kwargs=dict(
-                    decoder_distribution='gaussian_identity_variance',
-                    sampling_method='importance_sampling',
-                    # sampling_method='true_prior_sampling',
-                    num_latents_to_sample=10,
-                ),
-                use_parallel_dataloading=False,
-            ),
-
-            save_period=25,
-        ),
-        region="us-west-2",
-
-        logger_variant=dict(
-            tensorboard=True,
-        ),
-
-        slurm_variant=dict(
-            timeout_min=48 * 60,
-            cpus_per_task=10,
-            gpus_per_node=1,
-        ),
-    )
-
-    search_space = {
-        'seedid': range(5),
-        'grill_variant.algo_kwargs.rl_offpolicy_num_training_steps': [0, 100, 1000, 10000, 100000],
-        # 'grill_variant.reward_params.type':['latent_bound'], #, 'latent_distance'
-        'train_vae_variant.latent_sizes': [(2, 4)], #(3 * objects, 3 * colors)
-        # 'train_vae_variant.beta': [1],
-        # 'train_vae_variant.generate_vae_dataset_kwargs.n_random_steps': [100]
-    }
-    sweeper = hyp.DeterministicHyperparameterSweeper(
-        search_space, default_parameters=variant,
-    )
-
-    variants = []
-    for variant in sweeper.iterate_hyperparameters():
-        variants.append(variant)
-
-    run_variants(grill_her_td3_offpolicy_online_vae_full_experiment, variants, run_id=1)
diff --git a/experiments/ashvin/corl2019/debug/pointmass2/offpolicy_ccrig_debug.py b/experiments/ashvin/corl2019/debug/pointmass2/offpolicy_ccrig_debug.py
deleted file mode 100644
index 1f93763..0000000
--- a/experiments/ashvin/corl2019/debug/pointmass2/offpolicy_ccrig_debug.py
+++ /dev/null
@@ -1,205 +0,0 @@
-import railrl.misc.hyperparameter as hyp
-from experiments.murtaza.multiworld.skew_fit.reacher.generate_uniform_dataset import generate_uniform_dataset_reacher
-from multiworld.envs.mujoco.cameras import sawyer_init_camera_zoomed_in, sawyer_pusher_camera_upright_v2
-from railrl.launchers.launcher_util import run_experiment
-from railrl.torch.grill.launcher import *
-import railrl.torch.vae.vae_schedules as vae_schedules
-from railrl.torch.vae.conv_vae import imsize48_default_architecture, imsize48_default_architecture_with_more_hidden_layers
-from railrl.launchers.arglauncher import run_variants
-from railrl.torch.grill.cvae_experiments import (
-    grill_her_td3_offpolicy_online_vae_full_experiment,
-)
-from multiworld.envs.pygame.multiobject_pygame_env import Multiobj2DWallEnv
-from multiworld.envs.mujoco.sawyer_xyz.sawyer_push_multiobj_subset import SawyerMultiobjectEnv
-from railrl.torch.vae.conditional_conv_vae import DeltaCVAE
-from railrl.torch.vae.vae_trainer import DeltaCVAETrainer
-from railrl.data_management.online_conditional_vae_replay_buffer import \
-        OnlineConditionalVaeRelabelingBuffer
-
-x_var = 0.2
-x_low = -x_var
-x_high = x_var
-y_low = 0.5
-y_high = 0.7
-t = 0
-
-if __name__ == "__main__":
-    variant = dict(
-        double_algo=False,
-        online_vae_exploration=False,
-        imsize=48,
-        init_camera=sawyer_init_camera_zoomed_in,
-
-        env_class=Multiobj2DWallEnv,
-        env_kwargs=dict(
-           render_onscreen=False,
-           ball_radius=1.2,
-           wall_thickness=1.5,
-           inner_wall_max_dist=1.5,
-           images_are_rgb=True,
-           show_goal=False,
-           change_colors=True,
-           change_walls=True,
-       ),
-
-        grill_variant=dict(
-            save_video=True,
-            custom_goal_sampler='replay_buffer',
-            online_vae_trainer_kwargs=dict(
-                beta=20,
-                lr=0,
-            ),
-            save_video_period=50,
-            qf_kwargs=dict(
-                hidden_sizes=[400, 300],
-            ),
-            policy_kwargs=dict(
-                hidden_sizes=[400, 300],
-            ),
-            vf_kwargs=dict(
-                hidden_sizes=[400, 300],
-            ),
-            max_path_length=50,
-            algo_kwargs=dict(
-                batch_size=128,
-                num_epochs=101,
-                num_eval_steps_per_epoch=1000,
-                num_expl_steps_per_train_loop=1000,
-                num_trains_per_train_loop=500,
-                min_num_steps_before_training=1000,
-                vae_training_schedule=vae_schedules.never_train,
-                oracle_data=False,
-                vae_save_period=25,
-                parallel_vae_train=False,
-                dataset_path="/tmp/Multiobj2DWallEnv_N1020_sawyer_init_camera_zoomed_in_imsize48_random_oracle_split_0.npy",
-                rl_offpolicy_num_training_steps=0,
-            ),
-            td3_trainer_kwargs=dict(
-                discount=0.99,
-                # min_num_steps_before_training=4000,
-                reward_scale=1.0,
-                # render=False,
-                tau=1e-2,
-            ),
-            replay_buffer_class=OnlineConditionalVaeRelabelingBuffer,
-            replay_buffer_kwargs=dict(
-                start_skew_epoch=10,
-                max_size=int(100000),
-                fraction_goals_rollout_goals=0.2,
-                fraction_goals_env_goals=0.5,
-                exploration_rewards_type='None',
-                vae_priority_type='vae_prob',
-                priority_function_kwargs=dict(
-                    sampling_method='importance_sampling',
-                    decoder_distribution='gaussian_identity_variance',
-                    # decoder_distribution='bernoulli',
-                    num_latents_to_sample=10,
-                ),
-                power=-1,
-                relabeling_goal_sampling_mode='vae_prior',
-                save_decoded_to_internal_keys=False,
-            ),
-            exploration_goal_sampling_mode='vae_prior',
-            evaluation_goal_sampling_mode='reset_of_env',
-            normalize=False,
-            render=False,
-            exploration_noise=0.2,
-            exploration_type='ou',
-            training_mode='train',
-            testing_mode='test',
-            reward_params=dict(
-                epsilon=0.05,
-            ),
-            observation_key='latent_observation',
-            desired_goal_key='latent_desired_goal',
-            vae_wrapped_env_kwargs=dict(
-                sample_from_true_prior=True,
-            ),
-            algorithm='ONLINE-VAE-SAC-BERNOULLI',
-#             vae_path="ashvin/corl2019/offpolicy/dcvae2/run0/id0/itr_800.pkl",
-        ),
-        train_vae_variant=dict(
-#             representation_size=4,
-            beta=10,
-            beta_schedule_kwargs=dict(
-                x_values=(0, 1500),
-                y_values=(1, 50),
-            ),
-            num_epochs=10,
-            dump_skew_debug_plots=False,
-            # decoder_activation='gaussian',
-            decoder_activation='sigmoid',
-            use_linear_dynamics=False,
-            generate_vae_dataset_kwargs=dict(
-                N=1020,
-                n_random_steps=51,
-                test_p=.9,
-                use_cached=False,
-                show=False,
-                oracle_dataset=True,
-                oracle_dataset_using_set_to_goal=True,
-                non_presampled_goal_img_is_garbage=False,
-                random_rollout_data=True,
-                conditional_vae_dataset=True,
-                save_trajectories=True,
-                enviorment_dataset=False,
-            ),
-            vae_trainer_class=DeltaCVAETrainer,
-            vae_class=DeltaCVAE,
-            vae_kwargs=dict(
-                input_channels=3,
-                architecture=imsize48_default_architecture_with_more_hidden_layers,
-                decoder_distribution='gaussian_identity_variance',
-            ),
-            # TODO: why the redundancy?
-            algo_kwargs=dict(
-                start_skew_epoch=5000,
-                is_auto_encoder=False,
-                batch_size=32,
-                lr=1e-3,
-                skew_config=dict(
-                    method='vae_prob',
-                    power=0,
-                ),
-                skew_dataset=False,
-                priority_function_kwargs=dict(
-                    decoder_distribution='gaussian_identity_variance',
-                    sampling_method='importance_sampling',
-                    # sampling_method='true_prior_sampling',
-                    num_latents_to_sample=10,
-                ),
-                use_parallel_dataloading=False,
-            ),
-
-            save_period=25,
-        ),
-        region="us-west-2",
-
-        logger_variant=dict(
-            tensorboard=True,
-        ),
-
-        slurm_variant=dict(
-            timeout_min=48 * 60,
-            cpus_per_task=10,
-            gpus_per_node=1,
-        ),
-    )
-
-    search_space = {
-        'seedid': range(5),
-        'grill_variant.algo_kwargs.rl_offpolicy_num_training_steps': [10, ],
-        # 'grill_variant.reward_params.type':['latent_bound'], #, 'latent_distance'
-        'train_vae_variant.latent_sizes': [(2, 4)], #(3 * objects, 3 * colors)
-        # 'train_vae_variant.beta': [1],
-        # 'train_vae_variant.generate_vae_dataset_kwargs.n_random_steps': [100]
-    }
-    sweeper = hyp.DeterministicHyperparameterSweeper(
-        search_space, default_parameters=variant,
-    )
-
-    variants = []
-    for variant in sweeper.iterate_hyperparameters():
-        variants.append(variant)
-
-    run_variants(grill_her_td3_offpolicy_online_vae_full_experiment, variants, run_id=1)
diff --git a/experiments/ashvin/corl2019/debug/pointmass2/rig1.py b/experiments/ashvin/corl2019/debug/pointmass2/rig1.py
deleted file mode 100644
index a159c58..0000000
--- a/experiments/ashvin/corl2019/debug/pointmass2/rig1.py
+++ /dev/null
@@ -1,203 +0,0 @@
-import railrl.misc.hyperparameter as hyp
-from experiments.murtaza.multiworld.skew_fit.reacher.generate_uniform_dataset import generate_uniform_dataset_reacher
-from multiworld.envs.mujoco.cameras import sawyer_init_camera_zoomed_in, sawyer_pusher_camera_upright_v2
-from railrl.launchers.launcher_util import run_experiment
-import railrl.torch.vae.vae_schedules as vae_schedules
-from railrl.torch.vae.conv_vae import imsize48_default_architecture, imsize48_default_architecture_with_more_hidden_layers
-from railrl.launchers.arglauncher import run_variants
-from railrl.torch.grill.cvae_experiments import (
-    grill_her_td3_offpolicy_online_vae_full_experiment,
-)
-from multiworld.envs.pygame.multiobject_pygame_env import Multiobj2DWallEnv
-from multiworld.envs.mujoco.sawyer_xyz.sawyer_push_multiobj_subset import SawyerMultiobjectEnv
-from railrl.torch.vae.conditional_conv_vae import DeltaCVAE
-from railrl.torch.vae.conditional_vae_trainer import DeltaCVAETrainer
-from railrl.data_management.online_conditional_vae_replay_buffer import \
-        OnlineConditionalVaeRelabelingBuffer
-
-x_var = 0.2
-x_low = -x_var
-x_high = x_var
-y_low = 0.5
-y_high = 0.7
-t = 0.05
-
-if __name__ == "__main__":
-    variant = dict(
-        double_algo=False,
-        online_vae_exploration=False,
-        imsize=48,
-        init_camera=sawyer_init_camera_zoomed_in,
-        
-        env_class=Multiobj2DWallEnv,
-        env_kwargs=dict(
-           render_onscreen=False,
-           ball_radius=1.2,
-           wall_thickness=1.5,
-           inner_wall_max_dist=1.5,
-           images_are_rgb=True,
-           show_goal=False,
-           change_colors=True,
-           change_walls=True,
-       ),
-
-        grill_variant=dict(
-            save_video=True,
-            custom_goal_sampler='replay_buffer',
-            online_vae_trainer_kwargs=dict(
-                beta=20,
-                lr=0,
-            ),
-            save_video_period=50,
-            qf_kwargs=dict(
-                hidden_sizes=[400, 300],
-            ),
-            policy_kwargs=dict(
-                hidden_sizes=[400, 300],
-            ),
-            vf_kwargs=dict(
-                hidden_sizes=[400, 300],
-            ),
-            max_path_length=100,
-            algo_kwargs=dict(
-                batch_size=128,
-                num_epochs=1000,
-                num_eval_steps_per_epoch=1000,
-                num_expl_steps_per_train_loop=1000,
-                num_trains_per_train_loop=1000,
-                min_num_steps_before_training=1000,
-                vae_training_schedule=vae_schedules.never_train,
-                oracle_data=False,
-                vae_save_period=25,
-                parallel_vae_train=False,
-                dataset_path=None,
-                rl_offpolicy_num_training_steps=0,
-            ),
-            td3_trainer_kwargs=dict(
-                discount=0.99,
-                # min_num_steps_before_training=4000,
-                reward_scale=1.0,
-                # render=False,
-                tau=1e-2,
-            ),
-            replay_buffer_class=OnlineConditionalVaeRelabelingBuffer,
-            replay_buffer_kwargs=dict(
-                start_skew_epoch=10,
-                max_size=int(100000),
-                fraction_goals_rollout_goals=0.2,
-                fraction_goals_env_goals=0.5,
-                exploration_rewards_type='None',
-                vae_priority_type='vae_prob',
-                priority_function_kwargs=dict(
-                    sampling_method='importance_sampling',
-                    decoder_distribution='gaussian_identity_variance',
-                    # decoder_distribution='bernoulli',
-                    num_latents_to_sample=10,
-                ),
-                power=-1,
-                relabeling_goal_sampling_mode='vae_prior',
-            ),
-            exploration_goal_sampling_mode='vae_prior',
-            evaluation_goal_sampling_mode='reset_of_env',
-            normalize=False,
-            render=False,
-            exploration_noise=0.2,
-            exploration_type='ou',
-            training_mode='train',
-            testing_mode='test',
-            reward_params=dict(
-                epsilon=0.05,
-            ),
-            observation_key='latent_observation',
-            desired_goal_key='latent_desired_goal',
-            vae_wrapped_env_kwargs=dict(
-                sample_from_true_prior=True,
-            ),
-            algorithm='ONLINE-VAE-SAC-BERNOULLI',
-            # vae_path="datasets/pusher_color_spectrum/itr_1900.pkl",
-        ),
-        train_vae_variant=dict(
-            beta=10,
-            beta_schedule_kwargs=dict(
-                x_values=(0, 1500),
-                y_values=(1, 50),
-            ),
-            num_epochs=1000,
-            dump_skew_debug_plots=False,
-            # decoder_activation='gaussian',
-            decoder_activation='sigmoid',
-            use_linear_dynamics=False,
-            generate_vae_dataset_kwargs=dict(
-                N=102000,
-                n_random_steps=51,
-                test_p=.9,
-                use_cached=False,
-                show=False,
-                oracle_dataset=False,
-                oracle_dataset_using_set_to_goal=False,
-                non_presampled_goal_img_is_garbage=False,
-                random_rollout_data=True,
-                random_rollout_data_set_to_goal=False,
-                conditional_vae_dataset=True,
-                save_trajectories=False,
-                enviorment_dataset=False,
-                tag="ccrig1",
-            ),
-            # vae_trainer_class=DeltaCVAETrainer,
-            # vae_class=DeltaCVAE,
-            vae_kwargs=dict(
-                input_channels=3,
-                architecture=imsize48_default_architecture_with_more_hidden_layers,
-                decoder_distribution='gaussian_identity_variance',
-            ),
-            # TODO: why the redundancy?
-            algo_kwargs=dict(
-                start_skew_epoch=5000,
-                is_auto_encoder=False,
-                batch_size=32,
-                lr=1e-3,
-                skew_config=dict(
-                    method='vae_prob',
-                    power=0,
-                ),
-                skew_dataset=False,
-                priority_function_kwargs=dict(
-                    decoder_distribution='gaussian_identity_variance',
-                    sampling_method='importance_sampling',
-                    # sampling_method='true_prior_sampling',
-                    num_latents_to_sample=10,
-                ),
-                use_parallel_dataloading=False,
-            ),
-
-            save_period=25,
-        ),
-        region='us-west-2',
-
-        logger_variant=dict(
-            tensorboard=True,
-        ),
-
-        slurm_variant=dict(
-            timeout_min=48 * 60,
-            cpus_per_task=10,
-            gpus_per_node=1,
-        ),
-    )
-
-    search_space = {
-        'seedid': range(5),
-        'grill_variant.exploration_noise': [0.2, ],
-        'train_vae_variant.representation_size': [8,], #(3 * objects, 3 * colors)
-        'grill_variant.algo_kwargs.num_trains_per_train_loop':[1000, ],
-        'grill_variant.algo_kwargs.batch_size': [128, ],
-    }
-    sweeper = hyp.DeterministicHyperparameterSweeper(
-        search_space, default_parameters=variant,
-    )
-
-    variants = []
-    for variant in sweeper.iterate_hyperparameters():
-        variants.append(variant)
-
-    run_variants(grill_her_td3_offpolicy_online_vae_full_experiment, variants, run_id=0)
diff --git a/experiments/ashvin/corl2019/debug/pointmass2/rig_debug.py b/experiments/ashvin/corl2019/debug/pointmass2/rig_debug.py
deleted file mode 100644
index 15d2a2a..0000000
--- a/experiments/ashvin/corl2019/debug/pointmass2/rig_debug.py
+++ /dev/null
@@ -1,203 +0,0 @@
-import railrl.misc.hyperparameter as hyp
-from experiments.murtaza.multiworld.skew_fit.reacher.generate_uniform_dataset import generate_uniform_dataset_reacher
-from multiworld.envs.mujoco.cameras import sawyer_init_camera_zoomed_in, sawyer_pusher_camera_upright_v2
-from railrl.launchers.launcher_util import run_experiment
-import railrl.torch.vae.vae_schedules as vae_schedules
-from railrl.torch.vae.conv_vae import imsize48_default_architecture, imsize48_default_architecture_with_more_hidden_layers
-from railrl.launchers.arglauncher import run_variants
-from railrl.torch.grill.cvae_experiments import (
-    grill_her_td3_offpolicy_online_vae_full_experiment,
-)
-from multiworld.envs.pygame.multiobject_pygame_env import Multiobj2DWallEnv
-from multiworld.envs.mujoco.sawyer_xyz.sawyer_push_multiobj_subset import SawyerMultiobjectEnv
-from railrl.torch.vae.conditional_conv_vae import DeltaCVAE
-from railrl.torch.vae.conditional_vae_trainer import DeltaCVAETrainer
-from railrl.data_management.online_conditional_vae_replay_buffer import \
-        OnlineConditionalVaeRelabelingBuffer
-
-x_var = 0.2
-x_low = -x_var
-x_high = x_var
-y_low = 0.5
-y_high = 0.7
-t = 0.05
-
-if __name__ == "__main__":
-    variant = dict(
-        double_algo=False,
-        online_vae_exploration=False,
-        imsize=48,
-        init_camera=sawyer_init_camera_zoomed_in,
-        
-        env_class=Multiobj2DWallEnv,
-        env_kwargs=dict(
-           render_onscreen=False,
-           ball_radius=1.2,
-           wall_thickness=1.5,
-           inner_wall_max_dist=1.5,
-           images_are_rgb=True,
-           show_goal=False,
-           change_colors=True,
-           change_walls=True,
-       ),
-
-        grill_variant=dict(
-            save_video=True,
-            custom_goal_sampler='replay_buffer',
-            online_vae_trainer_kwargs=dict(
-                beta=20,
-                lr=0,
-            ),
-            save_video_period=50,
-            qf_kwargs=dict(
-                hidden_sizes=[400, 300],
-            ),
-            policy_kwargs=dict(
-                hidden_sizes=[400, 300],
-            ),
-            vf_kwargs=dict(
-                hidden_sizes=[400, 300],
-            ),
-            max_path_length=100,
-            algo_kwargs=dict(
-                batch_size=128,
-                num_epochs=1000,
-                num_eval_steps_per_epoch=1000,
-                num_expl_steps_per_train_loop=1000,
-                num_trains_per_train_loop=1000,
-                min_num_steps_before_training=1000,
-                vae_training_schedule=vae_schedules.never_train,
-                oracle_data=False,
-                vae_save_period=25,
-                parallel_vae_train=False,
-                dataset_path=None,
-                rl_offpolicy_num_training_steps=0,
-            ),
-            td3_trainer_kwargs=dict(
-                discount=0.99,
-                # min_num_steps_before_training=4000,
-                reward_scale=1.0,
-                # render=False,
-                tau=1e-2,
-            ),
-            replay_buffer_class=OnlineConditionalVaeRelabelingBuffer,
-            replay_buffer_kwargs=dict(
-                start_skew_epoch=10,
-                max_size=int(100000),
-                fraction_goals_rollout_goals=0.2,
-                fraction_goals_env_goals=0.5,
-                exploration_rewards_type='None',
-                vae_priority_type='vae_prob',
-                priority_function_kwargs=dict(
-                    sampling_method='importance_sampling',
-                    decoder_distribution='gaussian_identity_variance',
-                    # decoder_distribution='bernoulli',
-                    num_latents_to_sample=10,
-                ),
-                power=-1,
-                relabeling_goal_sampling_mode='vae_prior',
-            ),
-            exploration_goal_sampling_mode='vae_prior',
-            evaluation_goal_sampling_mode='reset_of_env',
-            normalize=False,
-            render=False,
-            exploration_noise=0.2,
-            exploration_type='ou',
-            training_mode='train',
-            testing_mode='test',
-            reward_params=dict(
-                epsilon=0.05,
-            ),
-            observation_key='latent_observation',
-            desired_goal_key='latent_desired_goal',
-            vae_wrapped_env_kwargs=dict(
-                sample_from_true_prior=True,
-            ),
-            algorithm='ONLINE-VAE-SAC-BERNOULLI',
-            # vae_path="datasets/pusher_color_spectrum/itr_1900.pkl",
-        ),
-        train_vae_variant=dict(
-            beta=10,
-            beta_schedule_kwargs=dict(
-                x_values=(0, 1500),
-                y_values=(1, 50),
-            ),
-            num_epochs=5,
-            dump_skew_debug_plots=False,
-            # decoder_activation='gaussian',
-            decoder_activation='sigmoid',
-            use_linear_dynamics=False,
-            generate_vae_dataset_kwargs=dict(
-                N=1020,
-                n_random_steps=51,
-                test_p=.9,
-                use_cached=False,
-                show=False,
-                oracle_dataset=False,
-                oracle_dataset_using_set_to_goal=False,
-                non_presampled_goal_img_is_garbage=False,
-                random_rollout_data=True,
-                random_rollout_data_set_to_goal=False,
-                conditional_vae_dataset=True,
-                save_trajectories=False,
-                enviorment_dataset=False,
-                tag="ccrig1",
-            ),
-            # vae_trainer_class=DeltaCVAETrainer,
-            # vae_class=DeltaCVAE,
-            vae_kwargs=dict(
-                input_channels=3,
-                architecture=imsize48_default_architecture_with_more_hidden_layers,
-                decoder_distribution='gaussian_identity_variance',
-            ),
-            # TODO: why the redundancy?
-            algo_kwargs=dict(
-                start_skew_epoch=5000,
-                is_auto_encoder=False,
-                batch_size=32,
-                lr=1e-3,
-                skew_config=dict(
-                    method='vae_prob',
-                    power=0,
-                ),
-                skew_dataset=False,
-                priority_function_kwargs=dict(
-                    decoder_distribution='gaussian_identity_variance',
-                    sampling_method='importance_sampling',
-                    # sampling_method='true_prior_sampling',
-                    num_latents_to_sample=10,
-                ),
-                use_parallel_dataloading=False,
-            ),
-
-            save_period=25,
-        ),
-        region='us-west-2',
-
-        logger_variant=dict(
-            tensorboard=True,
-        ),
-
-        slurm_variant=dict(
-            timeout_min=48 * 60,
-            cpus_per_task=10,
-            gpus_per_node=1,
-        ),
-    )
-
-    search_space = {
-        'seedid': range(5),
-        'grill_variant.exploration_noise': [0.2, ],
-        'train_vae_variant.representation_size': [8,], #(3 * objects, 3 * colors)
-        'grill_variant.algo_kwargs.num_trains_per_train_loop':[1000, ],
-        'grill_variant.algo_kwargs.batch_size': [128, ],
-    }
-    sweeper = hyp.DeterministicHyperparameterSweeper(
-        search_space, default_parameters=variant,
-    )
-
-    variants = []
-    for variant in sweeper.iterate_hyperparameters():
-        variants.append(variant)
-
-    run_variants(grill_her_td3_offpolicy_online_vae_full_experiment, variants, run_id=0)
diff --git a/experiments/ashvin/corl2019/debug/pusher/._ccrig10.py b/experiments/ashvin/corl2019/debug/pusher/._ccrig10.py
deleted file mode 100644
index ebac333..0000000
Binary files a/experiments/ashvin/corl2019/debug/pusher/._ccrig10.py and /dev/null differ
diff --git a/experiments/ashvin/corl2019/debug/pusher/ccrig10.py b/experiments/ashvin/corl2019/debug/pusher/ccrig10.py
deleted file mode 100644
index 69e83c4..0000000
--- a/experiments/ashvin/corl2019/debug/pusher/ccrig10.py
+++ /dev/null
@@ -1,221 +0,0 @@
-import railrl.misc.hyperparameter as hyp
-from experiments.murtaza.multiworld.skew_fit.reacher.generate_uniform_dataset import generate_uniform_dataset_reacher
-from multiworld.envs.mujoco.cameras import sawyer_init_camera_zoomed_in, sawyer_pusher_camera_upright_v2
-from railrl.launchers.launcher_util import run_experiment
-import railrl.torch.vae.vae_schedules as vae_schedules
-from railrl.torch.vae.conv_vae import imsize48_default_architecture, imsize48_default_architecture_with_more_hidden_layers
-from railrl.launchers.arglauncher import run_variants
-from railrl.torch.grill.cvae_experiments import (
-    grill_her_td3_offpolicy_online_vae_full_experiment,
-)
-from multiworld.envs.pygame.multiobject_pygame_env import Multiobj2DEnv
-from multiworld.envs.mujoco.sawyer_xyz.sawyer_push_multiobj_subset import SawyerMultiobjectEnv
-from railrl.torch.vae.conditional_conv_vae import DeltaCVAE
-from railrl.torch.vae.conditional_vae_trainer import DeltaCVAETrainer
-from railrl.data_management.online_conditional_vae_replay_buffer import \
-        OnlineConditionalVaeRelabelingBuffer
-
-x_var = 0.2
-x_low = -x_var
-x_high = x_var
-y_low = 0.5
-y_high = 0.7
-t = 0.05
-
-if __name__ == "__main__":
-    variant = dict(
-        double_algo=False,
-        online_vae_exploration=False,
-        imsize=48,
-        init_camera=sawyer_init_camera_zoomed_in,
-        env_class=SawyerMultiobjectEnv,
-        env_kwargs=dict(
-            num_objects=10,
-            object_meshes=None,
-            fixed_start=True,
-            num_scene_objects=[1],
-            maxlen=0.1,
-            action_repeat=1,
-            puck_goal_low=(x_low + 0.01, y_low + 0.01),
-            puck_goal_high=(x_high - 0.01, y_high - 0.01),
-            hand_goal_low=(x_low + 3*t, y_low + t),
-            hand_goal_high=(x_high - 3*t, y_high -t),
-            mocap_low=(x_low + 2*t, y_low , 0.0),
-            mocap_high=(x_high - 2*t, y_high, 0.5),
-            object_low=(x_low + 0.01, y_low + 0.01, 0.02),
-            object_high=(x_high - 0.01, y_high - 0.01, 0.02),
-            preload_obj_dict=[
-                dict(color2=(1, 0, 0)),
-                dict(color2=(0, 1, 0)),
-                dict(color2=(0, 0, 1)),
-                dict(color2=(1, .4, .7)),
-                dict(color2=(0, .4, .8)),
-                dict(color2=(.8, .8, 0)),
-                dict(color2=(1, .5, 0)),
-                dict(color2=(.4, 0, .4)),
-                dict(color2=(.4, .2, 0)),
-                dict(color2=(0, .4, .4)),
-            ],
-        ),
-
-        grill_variant=dict(
-            save_video=True,
-            custom_goal_sampler='replay_buffer',
-            online_vae_trainer_kwargs=dict(
-                beta=20,
-                lr=0,
-            ),
-            save_video_period=50,
-            qf_kwargs=dict(
-                hidden_sizes=[400, 300],
-            ),
-            policy_kwargs=dict(
-                hidden_sizes=[400, 300],
-            ),
-            vf_kwargs=dict(
-                hidden_sizes=[400, 300],
-            ),
-            max_path_length=100,
-            algo_kwargs=dict(
-                batch_size=128,
-                num_epochs=1000,
-                num_eval_steps_per_epoch=1000,
-                num_expl_steps_per_train_loop=1000,
-                num_trains_per_train_loop=1000,
-                min_num_steps_before_training=1000,
-                vae_training_schedule=vae_schedules.never_train,
-                oracle_data=False,
-                vae_save_period=25,
-                parallel_vae_train=False,
-                dataset_path=None,
-                rl_offpolicy_num_training_steps=0,
-            ),
-            td3_trainer_kwargs=dict(
-                discount=0.99,
-                # min_num_steps_before_training=4000,
-                reward_scale=1.0,
-                # render=False,
-                tau=1e-2,
-            ),
-            replay_buffer_class=OnlineConditionalVaeRelabelingBuffer,
-            replay_buffer_kwargs=dict(
-                start_skew_epoch=10,
-                max_size=int(100000),
-                fraction_goals_rollout_goals=0.2,
-                fraction_goals_env_goals=0.5,
-                exploration_rewards_type='None',
-                vae_priority_type='vae_prob',
-                priority_function_kwargs=dict(
-                    sampling_method='importance_sampling',
-                    decoder_distribution='gaussian_identity_variance',
-                    # decoder_distribution='bernoulli',
-                    num_latents_to_sample=10,
-                ),
-                power=-1,
-                relabeling_goal_sampling_mode='vae_prior',
-            ),
-            exploration_goal_sampling_mode='vae_prior',
-            evaluation_goal_sampling_mode='reset_of_env',
-            normalize=False,
-            render=False,
-            exploration_noise=0.2,
-            exploration_type='ou',
-            training_mode='train',
-            testing_mode='test',
-            reward_params=dict(
-                epsilon=0.05,
-            ),
-            observation_key='latent_observation',
-            desired_goal_key='latent_desired_goal',
-            vae_wrapped_env_kwargs=dict(
-                sample_from_true_prior=True,
-            ),
-            algorithm='ONLINE-VAE-SAC-BERNOULLI',
-            # vae_path="datasets/pusher_color_spectrum/itr_1900.pkl",
-        ),
-        train_vae_variant=dict(
-            latent_sizes=4,
-            beta=10,
-            beta_schedule_kwargs=dict(
-                x_values=(0, 1500),
-                y_values=(1, 50),
-            ),
-            num_epochs=1000,
-            dump_skew_debug_plots=False,
-            # decoder_activation='gaussian',
-            decoder_activation='sigmoid',
-            use_linear_dynamics=False,
-            generate_vae_dataset_kwargs=dict(
-                N=102000,
-                n_random_steps=51,
-                test_p=.9,
-                use_cached=False,
-                show=False,
-                oracle_dataset=False,
-                oracle_dataset_using_set_to_goal=False,
-                non_presampled_goal_img_is_garbage=False,
-                random_rollout_data=True,
-                random_rollout_data_set_to_goal=False,
-                conditional_vae_dataset=True,
-                save_trajectories=False,
-                enviorment_dataset=False,
-                tag="ccrig1",
-            ),
-            vae_trainer_class=DeltaCVAETrainer,
-            vae_class=DeltaCVAE,
-            vae_kwargs=dict(
-                input_channels=3,
-                architecture=imsize48_default_architecture_with_more_hidden_layers,
-                decoder_distribution='gaussian_identity_variance',
-            ),
-            # TODO: why the redundancy?
-            algo_kwargs=dict(
-                start_skew_epoch=5000,
-                is_auto_encoder=False,
-                batch_size=32,
-                lr=1e-3,
-                skew_config=dict(
-                    method='vae_prob',
-                    power=0,
-                ),
-                skew_dataset=False,
-                priority_function_kwargs=dict(
-                    decoder_distribution='gaussian_identity_variance',
-                    sampling_method='importance_sampling',
-                    # sampling_method='true_prior_sampling',
-                    num_latents_to_sample=10,
-                ),
-                use_parallel_dataloading=False,
-            ),
-
-            save_period=25,
-        ),
-        region='us-west-2',
-
-        logger_variant=dict(
-            tensorboard=True,
-        ),
-
-        slurm_variant=dict(
-            timeout_min=48 * 60,
-            cpus_per_task=10,
-            gpus_per_node=1,
-        ),
-    )
-
-    search_space = {
-        'seedid': range(5),
-        'grill_variant.exploration_noise': [0.2, ],
-        'train_vae_variant.latent_sizes': [(8, 8),], #(3 * objects, 3 * colors)
-        'grill_variant.algo_kwargs.num_trains_per_train_loop':[1000, ],
-        'grill_variant.algo_kwargs.batch_size': [128, 1024, ],
-    }
-    sweeper = hyp.DeterministicHyperparameterSweeper(
-        search_space, default_parameters=variant,
-    )
-
-    variants = []
-    for variant in sweeper.iterate_hyperparameters():
-        variants.append(variant)
-
-    run_variants(grill_her_td3_offpolicy_online_vae_full_experiment, variants, run_id=0)
diff --git a/experiments/ashvin/corl2019/debug/pusher/ccrig6.py b/experiments/ashvin/corl2019/debug/pusher/ccrig6.py
deleted file mode 100644
index 33077d4..0000000
--- a/experiments/ashvin/corl2019/debug/pusher/ccrig6.py
+++ /dev/null
@@ -1,220 +0,0 @@
-import railrl.misc.hyperparameter as hyp
-from experiments.murtaza.multiworld.skew_fit.reacher.generate_uniform_dataset import generate_uniform_dataset_reacher
-from multiworld.envs.mujoco.cameras import sawyer_init_camera_zoomed_in, sawyer_pusher_camera_upright_v2
-from railrl.launchers.launcher_util import run_experiment
-import railrl.torch.vae.vae_schedules as vae_schedules
-from railrl.torch.vae.conv_vae import imsize48_default_architecture, imsize48_default_architecture_with_more_hidden_layers
-from railrl.launchers.arglauncher import run_variants
-from railrl.torch.grill.cvae_experiments import (
-    grill_her_td3_offpolicy_online_vae_full_experiment,
-)
-from multiworld.envs.pygame.multiobject_pygame_env import Multiobj2DEnv
-from multiworld.envs.mujoco.sawyer_xyz.sawyer_push_multiobj_subset import SawyerMultiobjectEnv
-from railrl.torch.vae.conditional_conv_vae import DeltaCVAE
-from railrl.torch.vae.conditional_vae_trainer import DeltaCVAETrainer
-from railrl.data_management.online_conditional_vae_replay_buffer import \
-        OnlineConditionalVaeRelabelingBuffer
-
-x_var = 0.2
-x_low = -x_var
-x_high = x_var
-y_low = 0.5
-y_high = 0.7
-t = 0.05
-
-if __name__ == "__main__":
-    variant = dict(
-        double_algo=False,
-        online_vae_exploration=False,
-        imsize=48,
-        init_camera=sawyer_init_camera_zoomed_in,
-        env_class=SawyerMultiobjectEnv,
-        env_kwargs=dict(
-            num_objects=10,
-            object_meshes=None,
-            fixed_start=True,
-            num_scene_objects=[1],
-            maxlen=0.1,
-            action_repeat=1,
-            puck_goal_low=(x_low + 0.01, y_low + 0.01),
-            puck_goal_high=(x_high - 0.01, y_high - 0.01),
-            hand_goal_low=(x_low + 3*t, y_low + t),
-            hand_goal_high=(x_high - 3*t, y_high -t),
-            mocap_low=(x_low + 2*t, y_low , 0.0),
-            mocap_high=(x_high - 2*t, y_high, 0.5),
-            object_low=(x_low + 0.01, y_low + 0.01, 0.02),
-            object_high=(x_high - 0.01, y_high - 0.01, 0.02),
-            preload_obj_dict=[
-                dict(color2=(1, 0, 0)),
-                dict(color2=(0, 1, 0)),
-                dict(color2=(0, 0, 1)),
-                dict(color2=(1, .4, .7)),
-                dict(color2=(0, .4, .8)),
-                dict(color2=(.8, .8, 0)),
-                dict(color2=(1, .5, 0)),
-                dict(color2=(.4, 0, .4)),
-                dict(color2=(.4, .2, 0)),
-                dict(color2=(0, .4, .4)),
-            ],
-        ),
-
-        grill_variant=dict(
-            save_video=True,
-            custom_goal_sampler='replay_buffer',
-            online_vae_trainer_kwargs=dict(
-                beta=20,
-                lr=0,
-            ),
-            save_video_period=50,
-            qf_kwargs=dict(
-                hidden_sizes=[400, 300],
-            ),
-            policy_kwargs=dict(
-                hidden_sizes=[400, 300],
-            ),
-            vf_kwargs=dict(
-                hidden_sizes=[400, 300],
-            ),
-            max_path_length=100,
-            algo_kwargs=dict(
-                batch_size=128,
-                num_epochs=3000,
-                num_eval_steps_per_epoch=1000,
-                num_expl_steps_per_train_loop=1000,
-                num_trains_per_train_loop=1000,
-                min_num_steps_before_training=1000,
-                vae_training_schedule=vae_schedules.never_train,
-                oracle_data=False,
-                vae_save_period=25,
-                parallel_vae_train=False,
-                dataset_path=None,
-                rl_offpolicy_num_training_steps=0,
-            ),
-            td3_trainer_kwargs=dict(
-                discount=0.99,
-                # min_num_steps_before_training=4000,
-                reward_scale=1.0,
-                # render=False,
-                tau=1e-2,
-            ),
-            replay_buffer_class=OnlineConditionalVaeRelabelingBuffer,
-            replay_buffer_kwargs=dict(
-                start_skew_epoch=10,
-                max_size=int(100000),
-                fraction_goals_rollout_goals=0.2,
-                fraction_goals_env_goals=0.5,
-                exploration_rewards_type='None',
-                vae_priority_type='vae_prob',
-                priority_function_kwargs=dict(
-                    sampling_method='importance_sampling',
-                    decoder_distribution='gaussian_identity_variance',
-                    # decoder_distribution='bernoulli',
-                    num_latents_to_sample=10,
-                ),
-                power=-1,
-                relabeling_goal_sampling_mode='vae_prior',
-            ),
-            exploration_goal_sampling_mode='vae_prior',
-            evaluation_goal_sampling_mode='reset_of_env',
-            normalize=False,
-            render=False,
-            exploration_noise=0.2,
-            exploration_type='ou',
-            training_mode='train',
-            testing_mode='test',
-            reward_params=dict(
-                epsilon=0.05,
-            ),
-            observation_key='latent_observation',
-            desired_goal_key='latent_desired_goal',
-            vae_wrapped_env_kwargs=dict(
-                sample_from_true_prior=True,
-            ),
-            algorithm='ONLINE-VAE-SAC-BERNOULLI',
-            # vae_path="datasets/pusher_color_spectrum/itr_1900.pkl",
-        ),
-        train_vae_variant=dict(
-            latent_sizes=4,
-            beta=10,
-            beta_schedule_kwargs=dict(
-                x_values=(0, 1500),
-                y_values=(1, 50),
-            ),
-            num_epochs=1000,
-            dump_skew_debug_plots=False,
-            # decoder_activation='gaussian',
-            decoder_activation='sigmoid',
-            use_linear_dynamics=False,
-            generate_vae_dataset_kwargs=dict(
-                N=102000,
-                n_random_steps=51,
-                test_p=.9,
-                use_cached=False,
-                show=False,
-                oracle_dataset=False,
-                oracle_dataset_using_set_to_goal=False,
-                non_presampled_goal_img_is_garbage=False,
-                random_rollout_data=True,
-                random_rollout_data_set_to_goal=False,
-                conditional_vae_dataset=True,
-                save_trajectories=False,
-                enviorment_dataset=False,
-                tag="ccrig1",
-            ),
-            vae_trainer_class=DeltaCVAETrainer,
-            vae_class=DeltaCVAE,
-            vae_kwargs=dict(
-                input_channels=3,
-                architecture=imsize48_default_architecture_with_more_hidden_layers,
-                decoder_distribution='gaussian_identity_variance',
-            ),
-            # TODO: why the redundancy?
-            algo_kwargs=dict(
-                start_skew_epoch=5000,
-                is_auto_encoder=False,
-                batch_size=32,
-                lr=1e-3,
-                skew_config=dict(
-                    method='vae_prob',
-                    power=0,
-                ),
-                skew_dataset=False,
-                priority_function_kwargs=dict(
-                    decoder_distribution='gaussian_identity_variance',
-                    sampling_method='importance_sampling',
-                    # sampling_method='true_prior_sampling',
-                    num_latents_to_sample=10,
-                ),
-                use_parallel_dataloading=False,
-            ),
-
-            save_period=25,
-        ),
-        region='us-west-2',
-
-        logger_variant=dict(
-            tensorboard=True,
-        ),
-
-        slurm_variant=dict(
-            timeout_min=48 * 60,
-            cpus_per_task=10,
-            gpus_per_node=1,
-        ),
-    )
-
-    search_space = {
-        'seedid': range(5),
-        'grill_variant.exploration_noise': [0.2, ],
-        'train_vae_variant.latent_sizes': [(8, 8),], #(3 * objects, 3 * colors)
-        'grill_variant.algo_kwargs.num_trains_per_train_loop':[1000, ],
-    }
-    sweeper = hyp.DeterministicHyperparameterSweeper(
-        search_space, default_parameters=variant,
-    )
-
-    variants = []
-    for variant in sweeper.iterate_hyperparameters():
-        variants.append(variant)
-
-    run_variants(grill_her_td3_offpolicy_online_vae_full_experiment, variants, run_id=1)
diff --git a/experiments/ashvin/corl2019/debug/pusher/ccrig7.py b/experiments/ashvin/corl2019/debug/pusher/ccrig7.py
deleted file mode 100644
index 8b4e187..0000000
--- a/experiments/ashvin/corl2019/debug/pusher/ccrig7.py
+++ /dev/null
@@ -1,220 +0,0 @@
-import railrl.misc.hyperparameter as hyp
-from experiments.murtaza.multiworld.skew_fit.reacher.generate_uniform_dataset import generate_uniform_dataset_reacher
-from multiworld.envs.mujoco.cameras import sawyer_init_camera_zoomed_in, sawyer_pusher_camera_upright_v2
-from railrl.launchers.launcher_util import run_experiment
-import railrl.torch.vae.vae_schedules as vae_schedules
-from railrl.torch.vae.conv_vae import imsize48_default_architecture, imsize48_default_architecture_with_more_hidden_layers
-from railrl.launchers.arglauncher import run_variants
-from railrl.torch.grill.cvae_experiments import (
-    grill_her_td3_offpolicy_online_vae_full_experiment,
-)
-from multiworld.envs.pygame.multiobject_pygame_env import Multiobj2DEnv
-from multiworld.envs.mujoco.sawyer_xyz.sawyer_push_multiobj_subset import SawyerMultiobjectEnv
-from railrl.torch.vae.conditional_conv_vae import DeltaCVAE
-from railrl.torch.vae.conditional_vae_trainer import DeltaCVAETrainer
-from railrl.data_management.online_conditional_vae_replay_buffer import \
-        OnlineConditionalVaeRelabelingBuffer
-
-x_var = 0.2
-x_low = -x_var
-x_high = x_var
-y_low = 0.5
-y_high = 0.7
-t = 0.05
-
-if __name__ == "__main__":
-    variant = dict(
-        double_algo=False,
-        online_vae_exploration=False,
-        imsize=48,
-        init_camera=sawyer_init_camera_zoomed_in,
-        env_class=SawyerMultiobjectEnv,
-        env_kwargs=dict(
-            num_objects=10,
-            object_meshes=None,
-            fixed_start=True,
-            num_scene_objects=[1],
-            maxlen=0.1,
-            action_repeat=1,
-            puck_goal_low=(x_low + 0.01, y_low + 0.01),
-            puck_goal_high=(x_high - 0.01, y_high - 0.01),
-            hand_goal_low=(x_low + 3*t, y_low + t),
-            hand_goal_high=(x_high - 3*t, y_high -t),
-            mocap_low=(x_low + 2*t, y_low , 0.0),
-            mocap_high=(x_high - 2*t, y_high, 0.5),
-            object_low=(x_low + 0.01, y_low + 0.01, 0.02),
-            object_high=(x_high - 0.01, y_high - 0.01, 0.02),
-            preload_obj_dict=[
-                dict(color2=(1, 0, 0)),
-                dict(color2=(0, 1, 0)),
-                dict(color2=(0, 0, 1)),
-                dict(color2=(1, .4, .7)),
-                dict(color2=(0, .4, .8)),
-                dict(color2=(.8, .8, 0)),
-                dict(color2=(1, .5, 0)),
-                dict(color2=(.4, 0, .4)),
-                dict(color2=(.4, .2, 0)),
-                dict(color2=(0, .4, .4)),
-            ],
-        ),
-
-        grill_variant=dict(
-            save_video=True,
-            custom_goal_sampler='replay_buffer',
-            online_vae_trainer_kwargs=dict(
-                beta=20,
-                lr=0,
-            ),
-            save_video_period=50,
-            qf_kwargs=dict(
-                hidden_sizes=[400, 300, 300, ],
-            ),
-            policy_kwargs=dict(
-                hidden_sizes=[400, 300, 300, ],
-            ),
-            vf_kwargs=dict(
-                hidden_sizes=[400, 300, 300, ],
-            ),
-            max_path_length=100,
-            algo_kwargs=dict(
-                batch_size=128,
-                num_epochs=3000,
-                num_eval_steps_per_epoch=1000,
-                num_expl_steps_per_train_loop=1000,
-                num_trains_per_train_loop=1000,
-                min_num_steps_before_training=1000,
-                vae_training_schedule=vae_schedules.never_train,
-                oracle_data=False,
-                vae_save_period=25,
-                parallel_vae_train=False,
-                dataset_path=None,
-                rl_offpolicy_num_training_steps=0,
-            ),
-            td3_trainer_kwargs=dict(
-                discount=0.99,
-                # min_num_steps_before_training=4000,
-                reward_scale=1.0,
-                # render=False,
-                tau=1e-2,
-            ),
-            replay_buffer_class=OnlineConditionalVaeRelabelingBuffer,
-            replay_buffer_kwargs=dict(
-                start_skew_epoch=10,
-                max_size=int(100000),
-                fraction_goals_rollout_goals=0.2,
-                fraction_goals_env_goals=0.5,
-                exploration_rewards_type='None',
-                vae_priority_type='vae_prob',
-                priority_function_kwargs=dict(
-                    sampling_method='importance_sampling',
-                    decoder_distribution='gaussian_identity_variance',
-                    # decoder_distribution='bernoulli',
-                    num_latents_to_sample=10,
-                ),
-                power=-1,
-                relabeling_goal_sampling_mode='vae_prior',
-            ),
-            exploration_goal_sampling_mode='vae_prior',
-            evaluation_goal_sampling_mode='reset_of_env',
-            normalize=False,
-            render=False,
-            exploration_noise=0.2,
-            exploration_type='ou',
-            training_mode='train',
-            testing_mode='test',
-            reward_params=dict(
-                epsilon=0.05,
-            ),
-            observation_key='latent_observation',
-            desired_goal_key='latent_desired_goal',
-            vae_wrapped_env_kwargs=dict(
-                sample_from_true_prior=True,
-            ),
-            algorithm='ONLINE-VAE-SAC-BERNOULLI',
-            # vae_path="datasets/pusher_color_spectrum/itr_1900.pkl",
-        ),
-        train_vae_variant=dict(
-            latent_sizes=4,
-            beta=10,
-            beta_schedule_kwargs=dict(
-                x_values=(0, 1500),
-                y_values=(1, 50),
-            ),
-            num_epochs=1000,
-            dump_skew_debug_plots=False,
-            # decoder_activation='gaussian',
-            decoder_activation='sigmoid',
-            use_linear_dynamics=False,
-            generate_vae_dataset_kwargs=dict(
-                N=102000,
-                n_random_steps=51,
-                test_p=.9,
-                use_cached=False,
-                show=False,
-                oracle_dataset=False,
-                oracle_dataset_using_set_to_goal=False,
-                non_presampled_goal_img_is_garbage=False,
-                random_rollout_data=True,
-                random_rollout_data_set_to_goal=False,
-                conditional_vae_dataset=True,
-                save_trajectories=False,
-                enviorment_dataset=False,
-                tag="ccrig1",
-            ),
-            vae_trainer_class=DeltaCVAETrainer,
-            vae_class=DeltaCVAE,
-            vae_kwargs=dict(
-                input_channels=3,
-                architecture=imsize48_default_architecture_with_more_hidden_layers,
-                decoder_distribution='gaussian_identity_variance',
-            ),
-            # TODO: why the redundancy?
-            algo_kwargs=dict(
-                start_skew_epoch=5000,
-                is_auto_encoder=False,
-                batch_size=32,
-                lr=1e-3,
-                skew_config=dict(
-                    method='vae_prob',
-                    power=0,
-                ),
-                skew_dataset=False,
-                priority_function_kwargs=dict(
-                    decoder_distribution='gaussian_identity_variance',
-                    sampling_method='importance_sampling',
-                    # sampling_method='true_prior_sampling',
-                    num_latents_to_sample=10,
-                ),
-                use_parallel_dataloading=False,
-            ),
-
-            save_period=25,
-        ),
-        region='us-west-2',
-
-        logger_variant=dict(
-            tensorboard=True,
-        ),
-
-        slurm_variant=dict(
-            timeout_min=48 * 60,
-            cpus_per_task=10,
-            gpus_per_node=1,
-        ),
-    )
-
-    search_space = {
-        'seedid': range(5),
-        'grill_variant.exploration_noise': [0.2, ],
-        'train_vae_variant.latent_sizes': [(8, 8),], #(3 * objects, 3 * colors)
-        'grill_variant.algo_kwargs.num_trains_per_train_loop':[1000, ],
-    }
-    sweeper = hyp.DeterministicHyperparameterSweeper(
-        search_space, default_parameters=variant,
-    )
-
-    variants = []
-    for variant in sweeper.iterate_hyperparameters():
-        variants.append(variant)
-
-    run_variants(grill_her_td3_offpolicy_online_vae_full_experiment, variants, run_id=1)
diff --git a/experiments/ashvin/corl2019/debug/pusher/ccrig8.py b/experiments/ashvin/corl2019/debug/pusher/ccrig8.py
deleted file mode 100644
index 096444f..0000000
--- a/experiments/ashvin/corl2019/debug/pusher/ccrig8.py
+++ /dev/null
@@ -1,212 +0,0 @@
-import railrl.misc.hyperparameter as hyp
-from experiments.murtaza.multiworld.skew_fit.reacher.generate_uniform_dataset import generate_uniform_dataset_reacher
-from multiworld.envs.mujoco.cameras import sawyer_init_camera_zoomed_in, sawyer_pusher_camera_upright_v2
-from railrl.launchers.launcher_util import run_experiment
-import railrl.torch.vae.vae_schedules as vae_schedules
-from railrl.torch.vae.conv_vae import imsize48_default_architecture, imsize48_default_architecture_with_more_hidden_layers
-from railrl.launchers.arglauncher import run_variants
-from railrl.torch.grill.cvae_experiments import (
-    grill_her_td3_offpolicy_online_vae_full_experiment,
-)
-from multiworld.envs.pygame.multiobject_pygame_env import Multiobj2DEnv
-from multiworld.envs.mujoco.sawyer_xyz.sawyer_push_multiobj_subset import SawyerMultiobjectEnv
-from railrl.torch.vae.conditional_conv_vae import DeltaCVAE
-from railrl.torch.vae.conditional_vae_trainer import DeltaCVAETrainer
-from railrl.data_management.online_conditional_vae_replay_buffer import \
-        OnlineConditionalVaeRelabelingBuffer
-
-x_var = 0.2
-x_low = -x_var
-x_high = x_var
-y_low = 0.5
-y_high = 0.7
-t = 0.05
-
-if __name__ == "__main__":
-    variant = dict(
-        double_algo=False,
-        online_vae_exploration=False,
-        imsize=48,
-        init_camera=sawyer_init_camera_zoomed_in,
-        env_class=SawyerMultiobjectEnv,
-        env_kwargs=dict(
-            num_objects=10,
-            object_meshes=None,
-            fixed_start=True,
-            num_scene_objects=[1],
-            maxlen=0.1,
-            action_repeat=1,
-            puck_goal_low=(x_low + 0.01, y_low + 0.01),
-            puck_goal_high=(x_high - 0.01, y_high - 0.01),
-            hand_goal_low=(x_low + 3*t, y_low + t),
-            hand_goal_high=(x_high - 3*t, y_high -t),
-            mocap_low=(x_low + 2*t, y_low , 0.0),
-            mocap_high=(x_high - 2*t, y_high, 0.5),
-            object_low=(x_low + 0.01, y_low + 0.01, 0.02),
-            object_high=(x_high - 0.01, y_high - 0.01, 0.02),
-            preload_obj_dict=[
-                dict(color2=(1, 0, 0)),
-                dict(color2=(0, 1, 0)),
-                dict(color2=(0, 0, 1)),
-                dict(color2=(1, .4, .7)),
-                dict(color2=(0, .4, .8)),
-                dict(color2=(.8, .8, 0)),
-                dict(color2=(1, .5, 0)),
-                dict(color2=(.4, 0, .4)),
-                dict(color2=(.4, .2, 0)),
-                dict(color2=(0, .4, .4)),
-            ],
-        ),
-
-        grill_variant=dict(
-            save_video=True,
-            custom_goal_sampler='replay_buffer',
-            online_vae_trainer_kwargs=dict(
-                beta=20,
-                lr=0,
-            ),
-            save_video_period=50,
-            hidden_sizes=[400, 300, 300, ],
-            max_path_length=100,
-            algo_kwargs=dict(
-                batch_size=128,
-                num_epochs=3000,
-                num_eval_steps_per_epoch=1000,
-                num_expl_steps_per_train_loop=1000,
-                num_trains_per_train_loop=1000,
-                min_num_steps_before_training=1000,
-                vae_training_schedule=vae_schedules.never_train,
-                oracle_data=False,
-                vae_save_period=25,
-                parallel_vae_train=False,
-                dataset_path=None,
-                rl_offpolicy_num_training_steps=0,
-            ),
-            td3_trainer_kwargs=dict(
-                discount=0.99,
-                # min_num_steps_before_training=4000,
-                reward_scale=1.0,
-                # render=False,
-                tau=1e-2,
-            ),
-            replay_buffer_class=OnlineConditionalVaeRelabelingBuffer,
-            replay_buffer_kwargs=dict(
-                start_skew_epoch=10,
-                max_size=int(100000),
-                fraction_goals_rollout_goals=0.2,
-                fraction_goals_env_goals=0.5,
-                exploration_rewards_type='None',
-                vae_priority_type='vae_prob',
-                priority_function_kwargs=dict(
-                    sampling_method='importance_sampling',
-                    decoder_distribution='gaussian_identity_variance',
-                    # decoder_distribution='bernoulli',
-                    num_latents_to_sample=10,
-                ),
-                power=-1,
-                relabeling_goal_sampling_mode='vae_prior',
-            ),
-            exploration_goal_sampling_mode='vae_prior',
-            evaluation_goal_sampling_mode='reset_of_env',
-            normalize=False,
-            render=False,
-            exploration_noise=0.2,
-            exploration_type='ou',
-            training_mode='train',
-            testing_mode='test',
-            reward_params=dict(
-                epsilon=0.05,
-            ),
-            observation_key='latent_observation',
-            desired_goal_key='latent_desired_goal',
-            vae_wrapped_env_kwargs=dict(
-                sample_from_true_prior=True,
-            ),
-            algorithm='ONLINE-VAE-SAC-BERNOULLI',
-            # vae_path="datasets/pusher_color_spectrum/itr_1900.pkl",
-        ),
-        train_vae_variant=dict(
-            latent_sizes=4,
-            beta=10,
-            beta_schedule_kwargs=dict(
-                x_values=(0, 1500),
-                y_values=(1, 50),
-            ),
-            num_epochs=1000,
-            dump_skew_debug_plots=False,
-            # decoder_activation='gaussian',
-            decoder_activation='sigmoid',
-            use_linear_dynamics=False,
-            generate_vae_dataset_kwargs=dict(
-                N=102000,
-                n_random_steps=51,
-                test_p=.9,
-                use_cached=False,
-                show=False,
-                oracle_dataset=False,
-                oracle_dataset_using_set_to_goal=False,
-                non_presampled_goal_img_is_garbage=False,
-                random_rollout_data=True,
-                random_rollout_data_set_to_goal=False,
-                conditional_vae_dataset=True,
-                save_trajectories=False,
-                enviorment_dataset=False,
-                tag="ccrig1",
-            ),
-            vae_trainer_class=DeltaCVAETrainer,
-            vae_class=DeltaCVAE,
-            vae_kwargs=dict(
-                input_channels=3,
-                architecture=imsize48_default_architecture_with_more_hidden_layers,
-                decoder_distribution='gaussian_identity_variance',
-            ),
-            # TODO: why the redundancy?
-            algo_kwargs=dict(
-                start_skew_epoch=5000,
-                is_auto_encoder=False,
-                batch_size=32,
-                lr=1e-3,
-                skew_config=dict(
-                    method='vae_prob',
-                    power=0,
-                ),
-                skew_dataset=False,
-                priority_function_kwargs=dict(
-                    decoder_distribution='gaussian_identity_variance',
-                    sampling_method='importance_sampling',
-                    # sampling_method='true_prior_sampling',
-                    num_latents_to_sample=10,
-                ),
-                use_parallel_dataloading=False,
-            ),
-
-            save_period=25,
-        ),
-        region='us-west-2',
-
-        logger_variant=dict(
-            tensorboard=True,
-        ),
-
-        slurm_variant=dict(
-            timeout_min=48 * 60,
-            cpus_per_task=10,
-            gpus_per_node=1,
-        ),
-    )
-
-    search_space = {
-        'seedid': range(5),
-        'grill_variant.exploration_noise': [0.2, ],
-        'train_vae_variant.latent_sizes': [(8, 8),], #(3 * objects, 3 * colors)
-        'grill_variant.algo_kwargs.num_trains_per_train_loop':[1000, ],
-    }
-    sweeper = hyp.DeterministicHyperparameterSweeper(
-        search_space, default_parameters=variant,
-    )
-
-    variants = []
-    for variant in sweeper.iterate_hyperparameters():
-        variants.append(variant)
-
-    run_variants(grill_her_td3_offpolicy_online_vae_full_experiment, variants, run_id=1)
diff --git a/experiments/ashvin/corl2019/debug/pusher/ccrig8b.py b/experiments/ashvin/corl2019/debug/pusher/ccrig8b.py
deleted file mode 100644
index 742c680..0000000
--- a/experiments/ashvin/corl2019/debug/pusher/ccrig8b.py
+++ /dev/null
@@ -1,212 +0,0 @@
-import railrl.misc.hyperparameter as hyp
-from experiments.murtaza.multiworld.skew_fit.reacher.generate_uniform_dataset import generate_uniform_dataset_reacher
-from multiworld.envs.mujoco.cameras import sawyer_init_camera_zoomed_in, sawyer_pusher_camera_upright_v2
-from railrl.launchers.launcher_util import run_experiment
-import railrl.torch.vae.vae_schedules as vae_schedules
-from railrl.torch.vae.conv_vae import imsize48_default_architecture, imsize48_default_architecture_with_more_hidden_layers
-from railrl.launchers.arglauncher import run_variants
-from railrl.torch.grill.cvae_experiments import (
-    grill_her_td3_offpolicy_online_vae_full_experiment,
-)
-from multiworld.envs.pygame.multiobject_pygame_env import Multiobj2DEnv
-from multiworld.envs.mujoco.sawyer_xyz.sawyer_push_multiobj_subset import SawyerMultiobjectEnv
-from railrl.torch.vae.conditional_conv_vae import DeltaCVAE
-from railrl.torch.vae.conditional_vae_trainer import DeltaCVAETrainer
-from railrl.data_management.online_conditional_vae_replay_buffer import \
-        OnlineConditionalVaeRelabelingBuffer
-
-x_var = 0.2
-x_low = -x_var
-x_high = x_var
-y_low = 0.5
-y_high = 0.7
-t = 0.05
-
-if __name__ == "__main__":
-    variant = dict(
-        double_algo=False,
-        online_vae_exploration=False,
-        imsize=48,
-        init_camera=sawyer_init_camera_zoomed_in,
-        env_class=SawyerMultiobjectEnv,
-        env_kwargs=dict(
-            num_objects=10,
-            object_meshes=None,
-            fixed_start=True,
-            num_scene_objects=[1],
-            maxlen=0.1,
-            action_repeat=1,
-            puck_goal_low=(x_low + 0.01, y_low + 0.01),
-            puck_goal_high=(x_high - 0.01, y_high - 0.01),
-            hand_goal_low=(x_low + 3*t, y_low + t),
-            hand_goal_high=(x_high - 3*t, y_high -t),
-            mocap_low=(x_low + 2*t, y_low , 0.0),
-            mocap_high=(x_high - 2*t, y_high, 0.5),
-            object_low=(x_low + 0.01, y_low + 0.01, 0.02),
-            object_high=(x_high - 0.01, y_high - 0.01, 0.02),
-            preload_obj_dict=[
-                dict(color2=(1, 0, 0)),
-                dict(color2=(0, 1, 0)),
-                dict(color2=(0, 0, 1)),
-                dict(color2=(1, .4, .7)),
-                dict(color2=(0, .4, .8)),
-                dict(color2=(.8, .8, 0)),
-                dict(color2=(1, .5, 0)),
-                dict(color2=(.4, 0, .4)),
-                dict(color2=(.4, .2, 0)),
-                dict(color2=(0, .4, .4)),
-            ],
-        ),
-
-        grill_variant=dict(
-            save_video=True,
-            custom_goal_sampler='replay_buffer',
-            online_vae_trainer_kwargs=dict(
-                beta=20,
-                lr=0,
-            ),
-            save_video_period=50,
-            hidden_sizes=[400, 300, 300, 300, ],
-            max_path_length=100,
-            algo_kwargs=dict(
-                batch_size=128,
-                num_epochs=3000,
-                num_eval_steps_per_epoch=1000,
-                num_expl_steps_per_train_loop=1000,
-                num_trains_per_train_loop=1000,
-                min_num_steps_before_training=1000,
-                vae_training_schedule=vae_schedules.never_train,
-                oracle_data=False,
-                vae_save_period=25,
-                parallel_vae_train=False,
-                dataset_path=None,
-                rl_offpolicy_num_training_steps=0,
-            ),
-            td3_trainer_kwargs=dict(
-                discount=0.99,
-                # min_num_steps_before_training=4000,
-                reward_scale=1.0,
-                # render=False,
-                tau=1e-2,
-            ),
-            replay_buffer_class=OnlineConditionalVaeRelabelingBuffer,
-            replay_buffer_kwargs=dict(
-                start_skew_epoch=10,
-                max_size=int(100000),
-                fraction_goals_rollout_goals=0.2,
-                fraction_goals_env_goals=0.5,
-                exploration_rewards_type='None',
-                vae_priority_type='vae_prob',
-                priority_function_kwargs=dict(
-                    sampling_method='importance_sampling',
-                    decoder_distribution='gaussian_identity_variance',
-                    # decoder_distribution='bernoulli',
-                    num_latents_to_sample=10,
-                ),
-                power=-1,
-                relabeling_goal_sampling_mode='vae_prior',
-            ),
-            exploration_goal_sampling_mode='vae_prior',
-            evaluation_goal_sampling_mode='reset_of_env',
-            normalize=False,
-            render=False,
-            exploration_noise=0.2,
-            exploration_type='ou',
-            training_mode='train',
-            testing_mode='test',
-            reward_params=dict(
-                epsilon=0.05,
-            ),
-            observation_key='latent_observation',
-            desired_goal_key='latent_desired_goal',
-            vae_wrapped_env_kwargs=dict(
-                sample_from_true_prior=True,
-            ),
-            algorithm='ONLINE-VAE-SAC-BERNOULLI',
-            # vae_path="datasets/pusher_color_spectrum/itr_1900.pkl",
-        ),
-        train_vae_variant=dict(
-            latent_sizes=4,
-            beta=10,
-            beta_schedule_kwargs=dict(
-                x_values=(0, 1500),
-                y_values=(1, 50),
-            ),
-            num_epochs=1000,
-            dump_skew_debug_plots=False,
-            # decoder_activation='gaussian',
-            decoder_activation='sigmoid',
-            use_linear_dynamics=False,
-            generate_vae_dataset_kwargs=dict(
-                N=102000,
-                n_random_steps=51,
-                test_p=.9,
-                use_cached=False,
-                show=False,
-                oracle_dataset=False,
-                oracle_dataset_using_set_to_goal=False,
-                non_presampled_goal_img_is_garbage=False,
-                random_rollout_data=True,
-                random_rollout_data_set_to_goal=False,
-                conditional_vae_dataset=True,
-                save_trajectories=False,
-                enviorment_dataset=False,
-                tag="ccrig1",
-            ),
-            vae_trainer_class=DeltaCVAETrainer,
-            vae_class=DeltaCVAE,
-            vae_kwargs=dict(
-                input_channels=3,
-                architecture=imsize48_default_architecture_with_more_hidden_layers,
-                decoder_distribution='gaussian_identity_variance',
-            ),
-            # TODO: why the redundancy?
-            algo_kwargs=dict(
-                start_skew_epoch=5000,
-                is_auto_encoder=False,
-                batch_size=32,
-                lr=1e-3,
-                skew_config=dict(
-                    method='vae_prob',
-                    power=0,
-                ),
-                skew_dataset=False,
-                priority_function_kwargs=dict(
-                    decoder_distribution='gaussian_identity_variance',
-                    sampling_method='importance_sampling',
-                    # sampling_method='true_prior_sampling',
-                    num_latents_to_sample=10,
-                ),
-                use_parallel_dataloading=False,
-            ),
-
-            save_period=25,
-        ),
-        region='us-west-2',
-
-        logger_variant=dict(
-            tensorboard=True,
-        ),
-
-        slurm_variant=dict(
-            timeout_min=48 * 60,
-            cpus_per_task=10,
-            gpus_per_node=1,
-        ),
-    )
-
-    search_space = {
-        'seedid': range(5),
-        'grill_variant.exploration_noise': [0.2, ],
-        'train_vae_variant.latent_sizes': [(8, 8),], #(3 * objects, 3 * colors)
-        'grill_variant.algo_kwargs.num_trains_per_train_loop':[1000, ],
-    }
-    sweeper = hyp.DeterministicHyperparameterSweeper(
-        search_space, default_parameters=variant,
-    )
-
-    variants = []
-    for variant in sweeper.iterate_hyperparameters():
-        variants.append(variant)
-
-    run_variants(grill_her_td3_offpolicy_online_vae_full_experiment, variants, run_id=1)
diff --git a/experiments/ashvin/corl2019/debug/pusher/ccrig9.py b/experiments/ashvin/corl2019/debug/pusher/ccrig9.py
deleted file mode 100644
index 5ddec89..0000000
--- a/experiments/ashvin/corl2019/debug/pusher/ccrig9.py
+++ /dev/null
@@ -1,213 +0,0 @@
-import railrl.misc.hyperparameter as hyp
-from experiments.murtaza.multiworld.skew_fit.reacher.generate_uniform_dataset import generate_uniform_dataset_reacher
-from multiworld.envs.mujoco.cameras import sawyer_init_camera_zoomed_in, sawyer_pusher_camera_upright_v2
-from railrl.launchers.launcher_util import run_experiment
-import railrl.torch.vae.vae_schedules as vae_schedules
-from railrl.torch.vae.conv_vae import imsize48_default_architecture, imsize48_default_architecture_with_more_hidden_layers
-from railrl.launchers.arglauncher import run_variants
-from railrl.torch.grill.cvae_experiments import (
-    grill_her_td3_offpolicy_online_vae_full_experiment,
-)
-from multiworld.envs.pygame.multiobject_pygame_env import Multiobj2DEnv
-from multiworld.envs.mujoco.sawyer_xyz.sawyer_push_multiobj_subset import SawyerMultiobjectEnv
-from railrl.torch.vae.conditional_conv_vae import DeltaCVAE
-from railrl.torch.vae.conditional_vae_trainer import DeltaCVAETrainer
-from railrl.data_management.online_conditional_vae_replay_buffer import \
-        OnlineConditionalVaeRelabelingBuffer
-
-x_var = 0.2
-x_low = -x_var
-x_high = x_var
-y_low = 0.5
-y_high = 0.7
-t = 0.05
-
-if __name__ == "__main__":
-    variant = dict(
-        double_algo=False,
-        online_vae_exploration=False,
-        imsize=48,
-        init_camera=sawyer_init_camera_zoomed_in,
-        env_class=SawyerMultiobjectEnv,
-        env_kwargs=dict(
-            num_objects=10,
-            object_meshes=None,
-            fixed_start=True,
-            num_scene_objects=[1],
-            maxlen=0.1,
-            action_repeat=1,
-            puck_goal_low=(x_low + 0.01, y_low + 0.01),
-            puck_goal_high=(x_high - 0.01, y_high - 0.01),
-            hand_goal_low=(x_low + 3*t, y_low + t),
-            hand_goal_high=(x_high - 3*t, y_high -t),
-            mocap_low=(x_low + 2*t, y_low , 0.0),
-            mocap_high=(x_high - 2*t, y_high, 0.5),
-            object_low=(x_low + 0.01, y_low + 0.01, 0.02),
-            object_high=(x_high - 0.01, y_high - 0.01, 0.02),
-            preload_obj_dict=[
-                dict(color2=(1, 0, 0)),
-                dict(color2=(0, 1, 0)),
-                dict(color2=(0, 0, 1)),
-                dict(color2=(1, .4, .7)),
-                dict(color2=(0, .4, .8)),
-                dict(color2=(.8, .8, 0)),
-                dict(color2=(1, .5, 0)),
-                dict(color2=(.4, 0, .4)),
-                dict(color2=(.4, .2, 0)),
-                dict(color2=(0, .4, .4)),
-            ],
-        ),
-
-        grill_variant=dict(
-            save_video=True,
-            custom_goal_sampler='replay_buffer',
-            online_vae_trainer_kwargs=dict(
-                beta=20,
-                lr=0,
-            ),
-            save_video_period=50,
-            hidden_sizes=[400, 300, ],
-            max_path_length=100,
-            algo_kwargs=dict(
-                batch_size=128,
-                num_epochs=3000,
-                num_eval_steps_per_epoch=1000,
-                num_expl_steps_per_train_loop=1000,
-                num_trains_per_train_loop=1000,
-                min_num_steps_before_training=1000,
-                vae_training_schedule=vae_schedules.never_train,
-                oracle_data=False,
-                vae_save_period=25,
-                parallel_vae_train=False,
-                dataset_path=None,
-                rl_offpolicy_num_training_steps=0,
-            ),
-            td3_trainer_kwargs=dict(
-                discount=0.99,
-                # min_num_steps_before_training=4000,
-                reward_scale=1.0,
-                # render=False,
-                tau=1e-2,
-            ),
-            replay_buffer_class=OnlineConditionalVaeRelabelingBuffer,
-            replay_buffer_kwargs=dict(
-                start_skew_epoch=10,
-                max_size=int(100000),
-                fraction_goals_rollout_goals=0.2,
-                fraction_goals_env_goals=0.5,
-                exploration_rewards_type='None',
-                vae_priority_type='vae_prob',
-                priority_function_kwargs=dict(
-                    sampling_method='importance_sampling',
-                    decoder_distribution='gaussian_identity_variance',
-                    # decoder_distribution='bernoulli',
-                    num_latents_to_sample=10,
-                ),
-                power=-1,
-                relabeling_goal_sampling_mode='vae_prior',
-            ),
-            exploration_goal_sampling_mode='vae_prior',
-            evaluation_goal_sampling_mode='reset_of_env',
-            normalize=False,
-            render=False,
-            exploration_noise=0.2,
-            exploration_type='ou',
-            training_mode='train',
-            testing_mode='test',
-            reward_params=dict(
-                epsilon=0.05,
-            ),
-            observation_key='latent_observation',
-            desired_goal_key='latent_desired_goal',
-            vae_wrapped_env_kwargs=dict(
-                sample_from_true_prior=True,
-            ),
-            algorithm='ONLINE-VAE-SAC-BERNOULLI',
-            # vae_path="datasets/pusher_color_spectrum/itr_1900.pkl",
-        ),
-        train_vae_variant=dict(
-            latent_sizes=4,
-            beta=10,
-            beta_schedule_kwargs=dict(
-                x_values=(0, 1500),
-                y_values=(1, 50),
-            ),
-            num_epochs=1000,
-            dump_skew_debug_plots=False,
-            # decoder_activation='gaussian',
-            decoder_activation='sigmoid',
-            use_linear_dynamics=False,
-            generate_vae_dataset_kwargs=dict(
-                N=102000,
-                n_random_steps=51,
-                test_p=.9,
-                use_cached=False,
-                show=False,
-                oracle_dataset=False,
-                oracle_dataset_using_set_to_goal=False,
-                non_presampled_goal_img_is_garbage=False,
-                random_rollout_data=True,
-                random_rollout_data_set_to_goal=False,
-                conditional_vae_dataset=True,
-                save_trajectories=False,
-                enviorment_dataset=False,
-                tag="ccrig1",
-            ),
-            vae_trainer_class=DeltaCVAETrainer,
-            vae_class=DeltaCVAE,
-            vae_kwargs=dict(
-                input_channels=3,
-                architecture=imsize48_default_architecture_with_more_hidden_layers,
-                decoder_distribution='gaussian_identity_variance',
-            ),
-            # TODO: why the redundancy?
-            algo_kwargs=dict(
-                start_skew_epoch=5000,
-                is_auto_encoder=False,
-                batch_size=32,
-                lr=1e-3,
-                skew_config=dict(
-                    method='vae_prob',
-                    power=0,
-                ),
-                skew_dataset=False,
-                priority_function_kwargs=dict(
-                    decoder_distribution='gaussian_identity_variance',
-                    sampling_method='importance_sampling',
-                    # sampling_method='true_prior_sampling',
-                    num_latents_to_sample=10,
-                ),
-                use_parallel_dataloading=False,
-            ),
-
-            save_period=25,
-        ),
-        region='us-west-2',
-
-        logger_variant=dict(
-            tensorboard=True,
-        ),
-
-        slurm_variant=dict(
-            timeout_min=48 * 60,
-            cpus_per_task=10,
-            gpus_per_node=1,
-        ),
-    )
-
-    search_space = {
-        'seedid': range(5),
-        'grill_variant.exploration_noise': [0.2, ],
-        'train_vae_variant.latent_sizes': [(8, 8),], #(3 * objects, 3 * colors)
-        'grill_variant.algo_kwargs.num_trains_per_train_loop':[1000, ],
-        'grill_variant.algo_kwargs.batch_size': [1024, ],
-    }
-    sweeper = hyp.DeterministicHyperparameterSweeper(
-        search_space, default_parameters=variant,
-    )
-
-    variants = []
-    for variant in sweeper.iterate_hyperparameters():
-        variants.append(variant)
-
-    run_variants(grill_her_td3_offpolicy_online_vae_full_experiment, variants, run_id=1)
diff --git a/experiments/ashvin/corl2019/debug/pusher/ccrig_debug2.py b/experiments/ashvin/corl2019/debug/pusher/ccrig_debug2.py
index 9f924b4..ff77249 100644
--- a/experiments/ashvin/corl2019/debug/pusher/ccrig_debug2.py
+++ b/experiments/ashvin/corl2019/debug/pusher/ccrig_debug2.py
@@ -190,10 +190,7 @@ if __name__ == "__main__":
 
             save_period=25,
         ),
-        region='us-west-2',
-        logger_variant=dict(
-            tensorboard=True,
-        ),
+        region='us-west-2'
     )
 
     search_space = {
@@ -210,4 +207,4 @@ if __name__ == "__main__":
     for variant in sweeper.iterate_hyperparameters():
         variants.append(variant)
 
-    run_variants(grill_her_td3_offpolicy_online_vae_full_experiment, variants, run_id=4)
+    run_variants(grill_her_td3_offpolicy_online_vae_full_experiment, variants, run_id=2)
diff --git a/experiments/ashvin/corl2019/debug/pusher/ccrig_debug5.py b/experiments/ashvin/corl2019/debug/pusher/ccrig_debug5.py
index 99cc03e..64e5f96 100644
--- a/experiments/ashvin/corl2019/debug/pusher/ccrig_debug5.py
+++ b/experiments/ashvin/corl2019/debug/pusher/ccrig_debug5.py
@@ -195,12 +195,6 @@ if __name__ == "__main__":
         logger_variant=dict(
             tensorboard=True,
         ),
-
-        slurm_variant=dict(
-            timeout_min=1 * 60,
-            cpus_per_task=10,
-            gpus_per_node=1,
-        ),
     )
 
     search_space = {
@@ -217,4 +211,4 @@ if __name__ == "__main__":
     for variant in sweeper.iterate_hyperparameters():
         variants.append(variant)
 
-    run_variants(grill_her_td3_offpolicy_online_vae_full_experiment, variants, run_id=2)
+    run_variants(grill_her_td3_offpolicy_online_vae_full_experiment, variants, run_id=0)
diff --git a/experiments/ashvin/corl2019/debug/pusher2/ccrig1.py b/experiments/ashvin/corl2019/debug/pusher2/ccrig1.py
deleted file mode 100644
index ee44c33..0000000
--- a/experiments/ashvin/corl2019/debug/pusher2/ccrig1.py
+++ /dev/null
@@ -1,223 +0,0 @@
-import railrl.misc.hyperparameter as hyp
-from experiments.murtaza.multiworld.skew_fit.reacher.generate_uniform_dataset import generate_uniform_dataset_reacher
-from multiworld.envs.mujoco.cameras import sawyer_init_camera_zoomed_in, sawyer_pusher_camera_upright_v2
-from railrl.launchers.launcher_util import run_experiment
-import railrl.torch.vae.vae_schedules as vae_schedules
-from railrl.torch.vae.conv_vae import imsize48_default_architecture, imsize48_default_architecture_with_more_hidden_layers
-from railrl.launchers.arglauncher import run_variants
-from railrl.torch.grill.cvae_experiments import (
-    grill_her_td3_offpolicy_online_vae_full_experiment,
-)
-from multiworld.envs.pygame.multiobject_pygame_env import Multiobj2DEnv
-from multiworld.envs.mujoco.sawyer_xyz.sawyer_push_multiobj_subset import SawyerMultiobjectEnv
-from railrl.torch.vae.conditional_conv_vae import DeltaCVAE
-from railrl.torch.vae.conditional_vae_trainer import DeltaCVAETrainer
-from railrl.data_management.online_conditional_vae_replay_buffer import \
-        OnlineConditionalVaeRelabelingBuffer
-
-x_var = 0.2
-x_low = -x_var
-x_high = x_var
-y_low = 0.5
-y_high = 0.7
-t = 0.05
-
-if __name__ == "__main__":
-    variant = dict(
-        double_algo=False,
-        online_vae_exploration=False,
-        imsize=48,
-        init_camera=sawyer_init_camera_zoomed_in,
-        env_class=SawyerMultiobjectEnv,
-        env_kwargs=dict(
-            num_objects=1,
-            object_meshes=None,
-            fixed_start=True,
-            num_scene_objects=[1],
-            maxlen=0.1,
-            action_repeat=1,
-            puck_goal_low=(x_low + 0.01, y_low + 0.01),
-            puck_goal_high=(x_high - 0.01, y_high - 0.01),
-            hand_goal_low=(x_low + 3*t, y_low + t),
-            hand_goal_high=(x_high - 3*t, y_high -t),
-            mocap_low=(x_low + 2*t, y_low , 0.0),
-            mocap_high=(x_high - 2*t, y_high, 0.5),
-            object_low=(x_low + 0.01, y_low + 0.01, 0.02),
-            object_high=(x_high - 0.01, y_high - 0.01, 0.02),
-            use_textures=True,
-            init_camera=sawyer_init_camera_zoomed_in,
-            # preload_obj_dict=[
-            #     dict(color2=(1, 0, 0)),
-            #     dict(color2=(0, 1, 0)),
-            #     dict(color2=(0, 0, 1)),
-            #     dict(color2=(1, .4, .7)),
-            #     dict(color2=(0, .4, .8)),
-            #     dict(color2=(.8, .8, 0)),
-            #     dict(color2=(1, .5, 0)),
-            #     dict(color2=(.4, 0, .4)),
-            #     dict(color2=(.4, .2, 0)),
-            #     dict(color2=(0, .4, .4)),
-            # ],
-        ),
-
-        grill_variant=dict(
-            save_video=True,
-            custom_goal_sampler='replay_buffer',
-            online_vae_trainer_kwargs=dict(
-                beta=20,
-                lr=0,
-            ),
-            save_video_period=50,
-            qf_kwargs=dict(
-                hidden_sizes=[400, 300],
-            ),
-            policy_kwargs=dict(
-                hidden_sizes=[400, 300],
-            ),
-            vf_kwargs=dict(
-                hidden_sizes=[400, 300],
-            ),
-            max_path_length=100,
-            algo_kwargs=dict(
-                batch_size=128,
-                num_epochs=1000,
-                num_eval_steps_per_epoch=1000,
-                num_expl_steps_per_train_loop=1000,
-                num_trains_per_train_loop=1000,
-                min_num_steps_before_training=1000,
-                vae_training_schedule=vae_schedules.never_train,
-                oracle_data=False,
-                vae_save_period=25,
-                parallel_vae_train=False,
-                dataset_path=None,
-                rl_offpolicy_num_training_steps=0,
-            ),
-            td3_trainer_kwargs=dict(
-                discount=0.99,
-                # min_num_steps_before_training=4000,
-                reward_scale=1.0,
-                # render=False,
-                tau=1e-2,
-            ),
-            replay_buffer_class=OnlineConditionalVaeRelabelingBuffer,
-            replay_buffer_kwargs=dict(
-                start_skew_epoch=10,
-                max_size=int(100000),
-                fraction_goals_rollout_goals=0.2,
-                fraction_goals_env_goals=0.5,
-                exploration_rewards_type='None',
-                vae_priority_type='vae_prob',
-                priority_function_kwargs=dict(
-                    sampling_method='importance_sampling',
-                    decoder_distribution='gaussian_identity_variance',
-                    # decoder_distribution='bernoulli',
-                    num_latents_to_sample=10,
-                ),
-                power=-1,
-                relabeling_goal_sampling_mode='vae_prior',
-            ),
-            exploration_goal_sampling_mode='vae_prior',
-            evaluation_goal_sampling_mode='reset_of_env',
-            normalize=False,
-            render=False,
-            exploration_noise=0.2,
-            exploration_type='ou',
-            training_mode='train',
-            testing_mode='test',
-            reward_params=dict(
-                epsilon=0.05,
-            ),
-            observation_key='latent_observation',
-            desired_goal_key='latent_desired_goal',
-            vae_wrapped_env_kwargs=dict(
-                sample_from_true_prior=True,
-            ),
-            algorithm='ONLINE-VAE-SAC-BERNOULLI',
-            # vae_path="datasets/pusher_color_spectrum/itr_1900.pkl",
-        ),
-        train_vae_variant=dict(
-            latent_sizes=4,
-            beta=10,
-            beta_schedule_kwargs=dict(
-                x_values=(0, 1500),
-                y_values=(1, 50),
-            ),
-            num_epochs=1000,
-            dump_skew_debug_plots=False,
-            # decoder_activation='gaussian',
-            decoder_activation='sigmoid',
-            use_linear_dynamics=False,
-            generate_vae_dataset_kwargs=dict(
-                N=102000,
-                n_random_steps=51,
-                test_p=.9,
-                use_cached=False,
-                show=False,
-                oracle_dataset=False,
-                oracle_dataset_using_set_to_goal=False,
-                non_presampled_goal_img_is_garbage=False,
-                random_rollout_data=True,
-                random_rollout_data_set_to_goal=False,
-                conditional_vae_dataset=True,
-                save_trajectories=False,
-                enviorment_dataset=False,
-                tag="ccrig1",
-            ),
-            vae_trainer_class=DeltaCVAETrainer,
-            vae_class=DeltaCVAE,
-            vae_kwargs=dict(
-                input_channels=3,
-                architecture=imsize48_default_architecture_with_more_hidden_layers,
-                decoder_distribution='gaussian_identity_variance',
-            ),
-            # TODO: why the redundancy?
-            algo_kwargs=dict(
-                start_skew_epoch=5000,
-                is_auto_encoder=False,
-                batch_size=32,
-                lr=1e-3,
-                skew_config=dict(
-                    method='vae_prob',
-                    power=0,
-                ),
-                skew_dataset=False,
-                priority_function_kwargs=dict(
-                    decoder_distribution='gaussian_identity_variance',
-                    sampling_method='importance_sampling',
-                    # sampling_method='true_prior_sampling',
-                    num_latents_to_sample=10,
-                ),
-                use_parallel_dataloading=False,
-            ),
-
-            save_period=25,
-        ),
-        region='us-west-2',
-
-        logger_variant=dict(
-            tensorboard=True,
-        ),
-
-        slurm_variant=dict(
-            timeout_min=48 * 60,
-            cpus_per_task=10,
-            gpus_per_node=1,
-        ),
-    )
-
-    search_space = {
-        'seedid': range(5),
-        'grill_variant.exploration_noise': [0.2, ],
-        'train_vae_variant.latent_sizes': [(8, 8),], #(3 * objects, 3 * colors)
-        'grill_variant.algo_kwargs.num_trains_per_train_loop':[1000, 4000, ],
-        'grill_variant.algo_kwargs.batch_size': [128, 1024, ],
-    }
-    sweeper = hyp.DeterministicHyperparameterSweeper(
-        search_space, default_parameters=variant,
-    )
-
-    variants = []
-    for variant in sweeper.iterate_hyperparameters():
-        variants.append(variant)
-
-    run_variants(grill_her_td3_offpolicy_online_vae_full_experiment, variants, run_id=2)
diff --git a/experiments/ashvin/corl2019/debug/pusher2/ccrig2.py b/experiments/ashvin/corl2019/debug/pusher2/ccrig2.py
deleted file mode 100644
index fd7a9bf..0000000
--- a/experiments/ashvin/corl2019/debug/pusher2/ccrig2.py
+++ /dev/null
@@ -1,223 +0,0 @@
-import railrl.misc.hyperparameter as hyp
-from experiments.murtaza.multiworld.skew_fit.reacher.generate_uniform_dataset import generate_uniform_dataset_reacher
-from multiworld.envs.mujoco.cameras import sawyer_init_camera_zoomed_in, sawyer_pusher_camera_upright_v2
-from railrl.launchers.launcher_util import run_experiment
-import railrl.torch.vae.vae_schedules as vae_schedules
-from railrl.torch.vae.conv_vae import imsize48_default_architecture, imsize48_default_architecture_with_more_hidden_layers
-from railrl.launchers.arglauncher import run_variants
-from railrl.torch.grill.cvae_experiments import (
-    grill_her_td3_offpolicy_online_vae_full_experiment,
-)
-from multiworld.envs.pygame.multiobject_pygame_env import Multiobj2DEnv
-from multiworld.envs.mujoco.sawyer_xyz.sawyer_push_multiobj_subset import SawyerMultiobjectEnv
-from railrl.torch.vae.conditional_conv_vae import DeltaCVAE
-from railrl.torch.vae.conditional_vae_trainer import DeltaCVAETrainer
-from railrl.data_management.online_conditional_vae_replay_buffer import \
-        OnlineConditionalVaeRelabelingBuffer
-
-x_var = 0.2
-x_low = -x_var
-x_high = x_var
-y_low = 0.5
-y_high = 0.7
-t = 0.05
-
-if __name__ == "__main__":
-    variant = dict(
-        double_algo=False,
-        online_vae_exploration=False,
-        imsize=48,
-        init_camera=sawyer_init_camera_zoomed_in,
-        env_class=SawyerMultiobjectEnv,
-        env_kwargs=dict(
-            num_objects=1,
-            object_meshes=None,
-            fixed_start=True,
-            num_scene_objects=[1],
-            maxlen=0.1,
-            action_repeat=1,
-            puck_goal_low=(x_low + 0.01, y_low + 0.01),
-            puck_goal_high=(x_high - 0.01, y_high - 0.01),
-            hand_goal_low=(x_low + 3*t, y_low + t),
-            hand_goal_high=(x_high - 3*t, y_high -t),
-            mocap_low=(x_low + 2*t, y_low , 0.0),
-            mocap_high=(x_high - 2*t, y_high, 0.5),
-            object_low=(x_low + 0.01, y_low + 0.01, 0.02),
-            object_high=(x_high - 0.01, y_high - 0.01, 0.02),
-            use_textures=True,
-            init_camera=sawyer_init_camera_zoomed_in,
-            # preload_obj_dict=[
-            #     dict(color2=(1, 0, 0)),
-            #     dict(color2=(0, 1, 0)),
-            #     dict(color2=(0, 0, 1)),
-            #     dict(color2=(1, .4, .7)),
-            #     dict(color2=(0, .4, .8)),
-            #     dict(color2=(.8, .8, 0)),
-            #     dict(color2=(1, .5, 0)),
-            #     dict(color2=(.4, 0, .4)),
-            #     dict(color2=(.4, .2, 0)),
-            #     dict(color2=(0, .4, .4)),
-            # ],
-        ),
-
-        grill_variant=dict(
-            save_video=True,
-            custom_goal_sampler='replay_buffer',
-            online_vae_trainer_kwargs=dict(
-                beta=20,
-                lr=0,
-            ),
-            save_video_period=50,
-            qf_kwargs=dict(
-                hidden_sizes=[400, 300],
-            ),
-            policy_kwargs=dict(
-                hidden_sizes=[400, 300],
-            ),
-            vf_kwargs=dict(
-                hidden_sizes=[400, 300],
-            ),
-            max_path_length=100,
-            algo_kwargs=dict(
-                batch_size=128,
-                num_epochs=1000,
-                num_eval_steps_per_epoch=1000,
-                num_expl_steps_per_train_loop=1000,
-                num_trains_per_train_loop=1000,
-                min_num_steps_before_training=1000,
-                vae_training_schedule=vae_schedules.never_train,
-                oracle_data=False,
-                vae_save_period=25,
-                parallel_vae_train=False,
-                dataset_path=None,
-                rl_offpolicy_num_training_steps=0,
-            ),
-            td3_trainer_kwargs=dict(
-                discount=0.99,
-                # min_num_steps_before_training=4000,
-                reward_scale=1.0,
-                # render=False,
-                tau=1e-2,
-            ),
-            replay_buffer_class=OnlineConditionalVaeRelabelingBuffer,
-            replay_buffer_kwargs=dict(
-                start_skew_epoch=10,
-                max_size=int(100000),
-                fraction_goals_rollout_goals=0.2,
-                fraction_goals_env_goals=0.5,
-                exploration_rewards_type='None',
-                vae_priority_type='vae_prob',
-                priority_function_kwargs=dict(
-                    sampling_method='importance_sampling',
-                    decoder_distribution='gaussian_identity_variance',
-                    # decoder_distribution='bernoulli',
-                    num_latents_to_sample=10,
-                ),
-                power=-1,
-                relabeling_goal_sampling_mode='vae_prior',
-            ),
-            exploration_goal_sampling_mode='vae_prior',
-            evaluation_goal_sampling_mode='reset_of_env',
-            normalize=False,
-            render=False,
-            exploration_noise=0.2,
-            exploration_type='ou',
-            training_mode='train',
-            testing_mode='test',
-            reward_params=dict(
-                epsilon=0.05,
-            ),
-            observation_key='latent_observation',
-            desired_goal_key='latent_desired_goal',
-            vae_wrapped_env_kwargs=dict(
-                sample_from_true_prior=True,
-            ),
-            algorithm='ONLINE-VAE-SAC-BERNOULLI',
-            # vae_path="datasets/pusher_color_spectrum/itr_1900.pkl",
-        ),
-        train_vae_variant=dict(
-            latent_sizes=4,
-            beta=10,
-            beta_schedule_kwargs=dict(
-                x_values=(0, 1500),
-                y_values=(1, 50),
-            ),
-            num_epochs=1500,
-            dump_skew_debug_plots=False,
-            # decoder_activation='gaussian',
-            decoder_activation='sigmoid',
-            use_linear_dynamics=False,
-            generate_vae_dataset_kwargs=dict(
-                N=102000,
-                n_random_steps=51,
-                test_p=.9,
-                use_cached=False,
-                show=False,
-                oracle_dataset=False,
-                oracle_dataset_using_set_to_goal=False,
-                non_presampled_goal_img_is_garbage=False,
-                random_rollout_data=True,
-                random_rollout_data_set_to_goal=False,
-                conditional_vae_dataset=True,
-                save_trajectories=False,
-                enviorment_dataset=False,
-                tag="ccrig1",
-            ),
-            vae_trainer_class=DeltaCVAETrainer,
-            vae_class=DeltaCVAE,
-            vae_kwargs=dict(
-                input_channels=3,
-                architecture=imsize48_default_architecture_with_more_hidden_layers,
-                decoder_distribution='gaussian_identity_variance',
-            ),
-            # TODO: why the redundancy?
-            algo_kwargs=dict(
-                start_skew_epoch=5000,
-                is_auto_encoder=False,
-                batch_size=32,
-                lr=1e-3,
-                skew_config=dict(
-                    method='vae_prob',
-                    power=0,
-                ),
-                skew_dataset=False,
-                priority_function_kwargs=dict(
-                    decoder_distribution='gaussian_identity_variance',
-                    sampling_method='importance_sampling',
-                    # sampling_method='true_prior_sampling',
-                    num_latents_to_sample=10,
-                ),
-                use_parallel_dataloading=False,
-            ),
-
-            save_period=25,
-        ),
-        region='us-west-2',
-
-        logger_variant=dict(
-            tensorboard=True,
-        ),
-
-        slurm_variant=dict(
-            timeout_min=48 * 60,
-            cpus_per_task=10,
-            gpus_per_node=1,
-        ),
-    )
-
-    search_space = {
-        'seedid': range(5),
-        'grill_variant.exploration_noise': [0.2, ],
-        'train_vae_variant.latent_sizes': [(8, 8), (4, 4), (4, 8),], #(3 * objects, 3 * colors)
-        'grill_variant.algo_kwargs.num_trains_per_train_loop':[1000, 4000, ],
-        'grill_variant.algo_kwargs.batch_size': [128, ],
-    }
-    sweeper = hyp.DeterministicHyperparameterSweeper(
-        search_space, default_parameters=variant,
-    )
-
-    variants = []
-    for variant in sweeper.iterate_hyperparameters():
-        variants.append(variant)
-
-    run_variants(grill_her_td3_offpolicy_online_vae_full_experiment, variants, run_id=0)
diff --git a/experiments/ashvin/corl2019/debug/pusher2/offpolicy_ccrig1.py b/experiments/ashvin/corl2019/debug/pusher2/offpolicy_ccrig1.py
deleted file mode 100644
index 348e93d..0000000
--- a/experiments/ashvin/corl2019/debug/pusher2/offpolicy_ccrig1.py
+++ /dev/null
@@ -1,205 +0,0 @@
-import railrl.misc.hyperparameter as hyp
-from experiments.murtaza.multiworld.skew_fit.reacher.generate_uniform_dataset import generate_uniform_dataset_reacher
-from multiworld.envs.mujoco.cameras import sawyer_init_camera_zoomed_in, sawyer_pusher_camera_upright_v2
-from railrl.launchers.launcher_util import run_experiment
-from railrl.torch.grill.launcher import *
-import railrl.torch.vae.vae_schedules as vae_schedules
-from railrl.torch.vae.conv_vae import imsize48_default_architecture, imsize48_default_architecture_with_more_hidden_layers
-from railrl.launchers.arglauncher import run_variants
-from railrl.torch.grill.cvae_experiments import (
-    grill_her_td3_offpolicy_online_vae_full_experiment,
-)
-from multiworld.envs.pygame.multiobject_pygame_env import Multiobj2DWallEnv
-from multiworld.envs.mujoco.sawyer_xyz.sawyer_push_multiobj_subset import SawyerMultiobjectEnv
-from railrl.torch.vae.conditional_conv_vae import DeltaCVAE
-from railrl.torch.vae.vae_trainer import DeltaCVAETrainer
-from railrl.data_management.online_conditional_vae_replay_buffer import \
-        OnlineConditionalVaeRelabelingBuffer
-
-x_var = 0.2
-x_low = -x_var
-x_high = x_var
-y_low = 0.5
-y_high = 0.7
-t = 0
-
-if __name__ == "__main__":
-    variant = dict(
-        double_algo=False,
-        online_vae_exploration=False,
-        imsize=48,
-        init_camera=sawyer_init_camera_zoomed_in,
-
-        env_class=Multiobj2DWallEnv,
-        env_kwargs=dict(
-           render_onscreen=False,
-           ball_radius=1.2,
-           wall_thickness=1.5,
-           inner_wall_max_dist=1.5,
-           images_are_rgb=True,
-           show_goal=False,
-           change_colors=True,
-           change_walls=True,
-       ),
-
-        grill_variant=dict(
-            save_video=True,
-            custom_goal_sampler='replay_buffer',
-            online_vae_trainer_kwargs=dict(
-                beta=20,
-                lr=0,
-            ),
-            save_video_period=50,
-            qf_kwargs=dict(
-                hidden_sizes=[400, 300],
-            ),
-            policy_kwargs=dict(
-                hidden_sizes=[400, 300],
-            ),
-            vf_kwargs=dict(
-                hidden_sizes=[400, 300],
-            ),
-            max_path_length=50,
-            algo_kwargs=dict(
-                batch_size=128,
-                num_epochs=1001,
-                num_eval_steps_per_epoch=1000,
-                num_expl_steps_per_train_loop=1000,
-                num_trains_per_train_loop=500,
-                min_num_steps_before_training=1000,
-                vae_training_schedule=vae_schedules.never_train,
-                oracle_data=False,
-                vae_save_period=25,
-                parallel_vae_train=False,
-                dataset_path="/private/home/anair17/data/datasets/Multiobj2DWallEnv_N1020_sawyer_init_camera_zoomed_in_imsize48_random_oracle_split_0.npy",
-                rl_offpolicy_num_training_steps=0,
-            ),
-            td3_trainer_kwargs=dict(
-                discount=0.99,
-                # min_num_steps_before_training=4000,
-                reward_scale=1.0,
-                # render=False,
-                tau=1e-2,
-            ),
-            replay_buffer_class=OnlineConditionalVaeRelabelingBuffer,
-            replay_buffer_kwargs=dict(
-                start_skew_epoch=10,
-                max_size=int(100000),
-                fraction_goals_rollout_goals=0.2,
-                fraction_goals_env_goals=0.5,
-                exploration_rewards_type='None',
-                vae_priority_type='vae_prob',
-                priority_function_kwargs=dict(
-                    sampling_method='importance_sampling',
-                    decoder_distribution='gaussian_identity_variance',
-                    # decoder_distribution='bernoulli',
-                    num_latents_to_sample=10,
-                ),
-                power=-1,
-                relabeling_goal_sampling_mode='vae_prior',
-                save_decoded_to_internal_keys=False,
-            ),
-            exploration_goal_sampling_mode='vae_prior',
-            evaluation_goal_sampling_mode='reset_of_env',
-            normalize=False,
-            render=False,
-            exploration_noise=0.2,
-            exploration_type='ou',
-            training_mode='train',
-            testing_mode='test',
-            reward_params=dict(
-                epsilon=0.05,
-            ),
-            observation_key='latent_observation',
-            desired_goal_key='latent_desired_goal',
-            vae_wrapped_env_kwargs=dict(
-                sample_from_true_prior=True,
-            ),
-            algorithm='ONLINE-VAE-SAC-BERNOULLI',
-#             vae_path="ashvin/corl2019/offpolicy/dcvae2/run0/id0/itr_800.pkl",
-        ),
-        train_vae_variant=dict(
-#             representation_size=4,
-            beta=10,
-            beta_schedule_kwargs=dict(
-                x_values=(0, 1500),
-                y_values=(1, 50),
-            ),
-            num_epochs=10,
-            dump_skew_debug_plots=False,
-            # decoder_activation='gaussian',
-            decoder_activation='sigmoid',
-            use_linear_dynamics=False,
-            generate_vae_dataset_kwargs=dict(
-                N=102000,
-                n_random_steps=51,
-                test_p=.9,
-                use_cached=False,
-                show=False,
-                oracle_dataset=True,
-                oracle_dataset_using_set_to_goal=True,
-                non_presampled_goal_img_is_garbage=False,
-                random_rollout_data=True,
-                conditional_vae_dataset=True,
-                save_trajectories=True,
-                enviorment_dataset=False,
-            ),
-            vae_trainer_class=DeltaCVAETrainer,
-            vae_class=DeltaCVAE,
-            vae_kwargs=dict(
-                input_channels=3,
-                architecture=imsize48_default_architecture_with_more_hidden_layers,
-                decoder_distribution='gaussian_identity_variance',
-            ),
-            # TODO: why the redundancy?
-            algo_kwargs=dict(
-                start_skew_epoch=5000,
-                is_auto_encoder=False,
-                batch_size=32,
-                lr=1e-3,
-                skew_config=dict(
-                    method='vae_prob',
-                    power=0,
-                ),
-                skew_dataset=False,
-                priority_function_kwargs=dict(
-                    decoder_distribution='gaussian_identity_variance',
-                    sampling_method='importance_sampling',
-                    # sampling_method='true_prior_sampling',
-                    num_latents_to_sample=10,
-                ),
-                use_parallel_dataloading=False,
-            ),
-
-            save_period=25,
-        ),
-        region="us-west-2",
-
-        logger_variant=dict(
-            tensorboard=True,
-        ),
-
-        slurm_variant=dict(
-            timeout_min=48 * 60,
-            cpus_per_task=10,
-            gpus_per_node=1,
-        ),
-    )
-
-    search_space = {
-        'seedid': range(5),
-        'grill_variant.algo_kwargs.rl_offpolicy_num_training_steps': [0, 100, 1000, 10000, 100000],
-        # 'grill_variant.reward_params.type':['latent_bound'], #, 'latent_distance'
-        'train_vae_variant.latent_sizes': [(2, 4)], #(3 * objects, 3 * colors)
-        # 'train_vae_variant.beta': [1],
-        # 'train_vae_variant.generate_vae_dataset_kwargs.n_random_steps': [100]
-    }
-    sweeper = hyp.DeterministicHyperparameterSweeper(
-        search_space, default_parameters=variant,
-    )
-
-    variants = []
-    for variant in sweeper.iterate_hyperparameters():
-        variants.append(variant)
-
-    run_variants(grill_her_td3_offpolicy_online_vae_full_experiment, variants, run_id=0)
diff --git a/experiments/ashvin/corl2019/debug/pusher2/offpolicy_ccrig_debug.py b/experiments/ashvin/corl2019/debug/pusher2/offpolicy_ccrig_debug.py
deleted file mode 100644
index ace7f68..0000000
--- a/experiments/ashvin/corl2019/debug/pusher2/offpolicy_ccrig_debug.py
+++ /dev/null
@@ -1,225 +0,0 @@
-import railrl.misc.hyperparameter as hyp
-from experiments.murtaza.multiworld.skew_fit.reacher.generate_uniform_dataset import generate_uniform_dataset_reacher
-from multiworld.envs.mujoco.cameras import sawyer_init_camera_zoomed_in, sawyer_pusher_camera_upright_v2
-from railrl.launchers.launcher_util import run_experiment
-from railrl.torch.grill.launcher import *
-import railrl.torch.vae.vae_schedules as vae_schedules
-from railrl.torch.vae.conv_vae import imsize48_default_architecture, imsize48_default_architecture_with_more_hidden_layers
-from railrl.launchers.arglauncher import run_variants
-from railrl.torch.grill.cvae_experiments import (
-    grill_her_td3_offpolicy_online_vae_full_experiment,
-)
-from multiworld.envs.pygame.multiobject_pygame_env import Multiobj2DWallEnv
-from multiworld.envs.mujoco.sawyer_xyz.sawyer_push_multiobj_subset import SawyerMultiobjectEnv
-from railrl.torch.vae.conditional_conv_vae import DeltaCVAE
-from railrl.torch.vae.vae_trainer import DeltaCVAETrainer
-from railrl.data_management.online_conditional_vae_replay_buffer import \
-        OnlineConditionalVaeRelabelingBuffer
-
-x_var = 0.2
-x_low = -x_var
-x_high = x_var
-y_low = 0.5
-y_high = 0.7
-t = 0
-
-if __name__ == "__main__":
-    variant = dict(
-        double_algo=False,
-        online_vae_exploration=False,
-        imsize=48,
-        init_camera=sawyer_init_camera_zoomed_in,
-
-        env_class=SawyerMultiobjectEnv,
-        env_kwargs=dict(
-            num_objects=1,
-            object_meshes=None,
-            fixed_start=True,
-            num_scene_objects=[1],
-            maxlen=0.1,
-            action_repeat=1,
-            puck_goal_low=(x_low + 0.01, y_low + 0.01),
-            puck_goal_high=(x_high - 0.01, y_high - 0.01),
-            hand_goal_low=(x_low + 3*t, y_low + t),
-            hand_goal_high=(x_high - 3*t, y_high -t),
-            mocap_low=(x_low + 2*t, y_low , 0.0),
-            mocap_high=(x_high - 2*t, y_high, 0.5),
-            object_low=(x_low + 0.01, y_low + 0.01, 0.02),
-            object_high=(x_high - 0.01, y_high - 0.01, 0.02),
-            use_textures=True,
-            init_camera=sawyer_init_camera_zoomed_in,
-            # preload_obj_dict=[
-            #     dict(color2=(1, 0, 0)),
-            #     dict(color2=(0, 1, 0)),
-            #     dict(color2=(0, 0, 1)),
-            #     dict(color2=(1, .4, .7)),
-            #     dict(color2=(0, .4, .8)),
-            #     dict(color2=(.8, .8, 0)),
-            #     dict(color2=(1, .5, 0)),
-            #     dict(color2=(.4, 0, .4)),
-            #     dict(color2=(.4, .2, 0)),
-            #     dict(color2=(0, .4, .4)),
-            # ],
-        ),
-
-        grill_variant=dict(
-            save_video=True,
-            custom_goal_sampler='replay_buffer',
-            online_vae_trainer_kwargs=dict(
-                beta=20,
-                lr=0,
-            ),
-            save_video_period=50,
-            qf_kwargs=dict(
-                hidden_sizes=[400, 300],
-            ),
-            policy_kwargs=dict(
-                hidden_sizes=[400, 300],
-            ),
-            vf_kwargs=dict(
-                hidden_sizes=[400, 300],
-            ),
-            max_path_length=50,
-            algo_kwargs=dict(
-                batch_size=128,
-                num_epochs=101,
-                num_eval_steps_per_epoch=1000,
-                num_expl_steps_per_train_loop=1000,
-                num_trains_per_train_loop=500,
-                min_num_steps_before_training=1000,
-                vae_training_schedule=vae_schedules.never_train,
-                oracle_data=False,
-                vae_save_period=25,
-                parallel_vae_train=False,
-                dataset_path="/tmp/Multiobj2DWallEnv_N1020_sawyer_init_camera_zoomed_in_imsize48_random_oracle_split_0.npy",
-                rl_offpolicy_num_training_steps=0,
-            ),
-            td3_trainer_kwargs=dict(
-                discount=0.99,
-                # min_num_steps_before_training=4000,
-                reward_scale=1.0,
-                # render=False,
-                tau=1e-2,
-            ),
-            replay_buffer_class=OnlineConditionalVaeRelabelingBuffer,
-            replay_buffer_kwargs=dict(
-                start_skew_epoch=10,
-                max_size=int(100000),
-                fraction_goals_rollout_goals=0.2,
-                fraction_goals_env_goals=0.5,
-                exploration_rewards_type='None',
-                vae_priority_type='vae_prob',
-                priority_function_kwargs=dict(
-                    sampling_method='importance_sampling',
-                    decoder_distribution='gaussian_identity_variance',
-                    # decoder_distribution='bernoulli',
-                    num_latents_to_sample=10,
-                ),
-                power=-1,
-                relabeling_goal_sampling_mode='vae_prior',
-                save_decoded_to_internal_keys=False,
-            ),
-            exploration_goal_sampling_mode='vae_prior',
-            evaluation_goal_sampling_mode='reset_of_env',
-            normalize=False,
-            render=False,
-            exploration_noise=0.2,
-            exploration_type='ou',
-            training_mode='train',
-            testing_mode='test',
-            reward_params=dict(
-                epsilon=0.05,
-            ),
-            observation_key='latent_observation',
-            desired_goal_key='latent_desired_goal',
-            vae_wrapped_env_kwargs=dict(
-                sample_from_true_prior=True,
-            ),
-            algorithm='ONLINE-VAE-SAC-BERNOULLI',
-#             vae_path="ashvin/corl2019/offpolicy/dcvae2/run0/id0/itr_800.pkl",
-        ),
-        train_vae_variant=dict(
-#             representation_size=4,
-            beta=10,
-            beta_schedule_kwargs=dict(
-                x_values=(0, 1500),
-                y_values=(1, 50),
-            ),
-            num_epochs=10,
-            dump_skew_debug_plots=False,
-            # decoder_activation='gaussian',
-            decoder_activation='sigmoid',
-            use_linear_dynamics=False,
-            generate_vae_dataset_kwargs=dict(
-                N=1020,
-                n_random_steps=51,
-                test_p=.9,
-                use_cached=False,
-                show=False,
-                oracle_dataset=True,
-                oracle_dataset_using_set_to_goal=True,
-                non_presampled_goal_img_is_garbage=False,
-                random_rollout_data=True,
-                conditional_vae_dataset=True,
-                save_trajectories=True,
-                enviorment_dataset=False,
-            ),
-            vae_trainer_class=DeltaCVAETrainer,
-            vae_class=DeltaCVAE,
-            vae_kwargs=dict(
-                input_channels=3,
-                architecture=imsize48_default_architecture_with_more_hidden_layers,
-                decoder_distribution='gaussian_identity_variance',
-            ),
-            # TODO: why the redundancy?
-            algo_kwargs=dict(
-                start_skew_epoch=5000,
-                is_auto_encoder=False,
-                batch_size=32,
-                lr=1e-3,
-                skew_config=dict(
-                    method='vae_prob',
-                    power=0,
-                ),
-                skew_dataset=False,
-                priority_function_kwargs=dict(
-                    decoder_distribution='gaussian_identity_variance',
-                    sampling_method='importance_sampling',
-                    # sampling_method='true_prior_sampling',
-                    num_latents_to_sample=10,
-                ),
-                use_parallel_dataloading=False,
-            ),
-
-            save_period=25,
-        ),
-        region="us-west-2",
-
-        logger_variant=dict(
-            tensorboard=True,
-        ),
-
-        slurm_variant=dict(
-            timeout_min=48 * 60,
-            cpus_per_task=10,
-            gpus_per_node=1,
-        ),
-    )
-
-    search_space = {
-        'seedid': range(5),
-        'grill_variant.algo_kwargs.rl_offpolicy_num_training_steps': [10, ],
-        # 'grill_variant.reward_params.type':['latent_bound'], #, 'latent_distance'
-        'train_vae_variant.latent_sizes': [(2, 4)], #(3 * objects, 3 * colors)
-        # 'train_vae_variant.beta': [1],
-        # 'train_vae_variant.generate_vae_dataset_kwargs.n_random_steps': [100]
-    }
-    sweeper = hyp.DeterministicHyperparameterSweeper(
-        search_space, default_parameters=variant,
-    )
-
-    variants = []
-    for variant in sweeper.iterate_hyperparameters():
-        variants.append(variant)
-
-    run_variants(grill_her_td3_offpolicy_online_vae_full_experiment, variants, run_id=1)
diff --git a/experiments/ashvin/corl2019/debug/pusher2/oracle1.py b/experiments/ashvin/corl2019/debug/pusher2/oracle1.py
deleted file mode 100644
index f3e33c9..0000000
--- a/experiments/ashvin/corl2019/debug/pusher2/oracle1.py
+++ /dev/null
@@ -1,205 +0,0 @@
-"""
-This should results in an average return of ~3000 by the end of training.
-
-Usually hits 3000 around epoch 80-100. Within a see, the performance will be
-a bit noisy from one epoch to the next (occasionally dips dow to ~2000).
-
-Note that one epoch = 5k steps, so 200 epochs = 1 million steps.
-"""
-import gym
-
-import railrl.torch.pytorch_util as ptu
-from railrl.data_management.obs_dict_replay_buffer import ObsDictRelabelingBuffer
-from railrl.exploration_strategies.base import \
-    PolicyWrappedWithExplorationStrategy
-from railrl.exploration_strategies.gaussian_and_epislon import \
-    GaussianAndEpislonStrategy
-from railrl.launchers.launcher_util import setup_logger
-from railrl.samplers.data_collector import GoalConditionedPathCollector
-from railrl.torch.her.her import HERTrainer
-from railrl.torch.networks import FlattenMlp, TanhMlpPolicy
-from railrl.torch.td3.td3 import TD3
-from railrl.torch.torch_rl_algorithm import TorchBatchRLAlgorithm
-from multiworld.envs.mujoco.sawyer_xyz.sawyer_push_multiobj_subset import SawyerMultiobjectEnv
-
-from railrl.launchers.arglauncher import run_variants
-from railrl.launchers.launcher_util import run_experiment
-# import railrl.util.hyperparameter as hyp
-
-from multiworld.envs.pygame.point2d import Point2DWallEnv
-import railrl.misc.hyperparameter as hyp
-
-from multiworld.envs.mujoco.sawyer_xyz.sawyer_push_multiobj_subset import SawyerMultiobjectEnv
-from multiworld.envs.mujoco.cameras import sawyer_init_camera_zoomed_in, sawyer_pusher_camera_upright_v2
-
-def experiment(variant):
-    expl_env = variant['env_class'](**variant['env_kwargs'])
-    eval_env = variant['env_class'](**variant['env_kwargs'])
-
-    observation_key = 'state_observation'
-    desired_goal_key = 'state_desired_goal'
-    achieved_goal_key = desired_goal_key.replace("desired", "achieved")
-    es = GaussianAndEpislonStrategy(
-        action_space=expl_env.action_space,
-        max_sigma=.2,
-        min_sigma=.2,  # constant sigma
-        epsilon=.3,
-    )
-    obs_dim = expl_env.observation_space.spaces['observation'].low.size
-    goal_dim = expl_env.observation_space.spaces['desired_goal'].low.size
-    action_dim = expl_env.action_space.low.size
-    qf1 = FlattenMlp(
-        input_size=obs_dim + goal_dim + action_dim,
-        output_size=1,
-        **variant['qf_kwargs']
-    )
-    qf2 = FlattenMlp(
-        input_size=obs_dim + goal_dim + action_dim,
-        output_size=1,
-        **variant['qf_kwargs']
-    )
-    target_qf1 = FlattenMlp(
-        input_size=obs_dim + goal_dim + action_dim,
-        output_size=1,
-        **variant['qf_kwargs']
-    )
-    target_qf2 = FlattenMlp(
-        input_size=obs_dim + goal_dim + action_dim,
-        output_size=1,
-        **variant['qf_kwargs']
-    )
-    policy = TanhMlpPolicy(
-        input_size=obs_dim + goal_dim,
-        output_size=action_dim,
-        **variant['policy_kwargs']
-    )
-    target_policy = TanhMlpPolicy(
-        input_size=obs_dim + goal_dim,
-        output_size=action_dim,
-        **variant['policy_kwargs']
-    )
-    expl_policy = PolicyWrappedWithExplorationStrategy(
-        exploration_strategy=es,
-        policy=policy,
-    )
-    replay_buffer = ObsDictRelabelingBuffer(
-        env=eval_env,
-        observation_key=observation_key,
-        desired_goal_key=desired_goal_key,
-        achieved_goal_key=achieved_goal_key,
-        **variant['replay_buffer_kwargs']
-    )
-    trainer = TD3(
-        policy=policy,
-        qf1=qf1,
-        qf2=qf2,
-        target_qf1=target_qf1,
-        target_qf2=target_qf2,
-        target_policy=target_policy,
-        **variant['trainer_kwargs']
-    )
-    trainer = HERTrainer(trainer)
-    eval_path_collector = GoalConditionedPathCollector(
-        eval_env,
-        policy,
-        observation_key=observation_key,
-        desired_goal_key=desired_goal_key,
-    )
-    expl_path_collector = GoalConditionedPathCollector(
-        expl_env,
-        expl_policy,
-        observation_key=observation_key,
-        desired_goal_key=desired_goal_key,
-    )
-    algorithm = TorchBatchRLAlgorithm(
-        trainer=trainer,
-        exploration_env=expl_env,
-        evaluation_env=eval_env,
-        exploration_data_collector=expl_path_collector,
-        evaluation_data_collector=eval_path_collector,
-        replay_buffer=replay_buffer,
-        **variant['algo_kwargs']
-    )
-    algorithm.to(ptu.device)
-    algorithm.train()
-
-
-if __name__ == "__main__":
-    x_var=0.2
-    x_low = -x_var
-    x_high = x_var
-    y_low = 0.5
-    y_high = 0.7
-    t = 0.05
-    variant = dict(
-        env_class=SawyerMultiobjectEnv,
-        env_kwargs=dict(
-            num_objects=1,
-            object_meshes=None,
-            fixed_start=True,
-            num_scene_objects=[1],
-            maxlen=0.1,
-            action_repeat=1,
-            puck_goal_low=(x_low + 0.01, y_low + 0.01),
-            puck_goal_high=(x_high - 0.01, y_high - 0.01),
-            hand_goal_low=(x_low + 3*t, y_low + t),
-            hand_goal_high=(x_high - 3*t, y_high -t),
-            mocap_low=(x_low + 2*t, y_low , 0.0),
-            mocap_high=(x_high - 2*t, y_high, 0.5),
-            object_low=(x_low + 0.01, y_low + 0.01, 0.02),
-            object_high=(x_high - 0.01, y_high - 0.01, 0.02),
-            # preload_obj_dict=[
-            #     dict(color2=(1, 0, 0)),
-            #     dict(color2=(0, 1, 0)),
-            #     dict(color2=(0, 0, 1)),
-            #     dict(color2=(1, .4, .7)),
-            #     dict(color2=(0, .4, .8)),
-            #     dict(color2=(.8, .8, 0)),
-            #     dict(color2=(1, .5, 0)),
-            #     dict(color2=(.4, 0, .4)),
-            #     dict(color2=(.4, .2, 0)),
-            #     dict(color2=(0, .4, .4)),
-            # ],
-            # use_textures=True,
-            # init_camera=sawyer_init_camera_zoomed_in,
-        ),
-
-        algo_kwargs=dict(
-            num_epochs=1001,
-            max_path_length=20,
-            batch_size=128,
-            num_eval_steps_per_epoch=1000,
-            num_expl_steps_per_train_loop=1000,
-            num_trains_per_train_loop=1000,
-            min_num_steps_before_training=1000,
-        ),
-        trainer_kwargs=dict(
-            discount=0.99,
-        ),
-        replay_buffer_kwargs=dict(
-            max_size=100000,
-            fraction_goals_rollout_goals=0.2,
-            fraction_goals_env_goals=0.0,
-        ),
-        qf_kwargs=dict(
-            hidden_sizes=[400, 300],
-        ),
-        policy_kwargs=dict(
-            hidden_sizes=[400, 300],
-        ),
-        region='us-east-2',
-    )
-
-    search_space = {
-        'seedid': range(5),
-        'replay_buffer_kwargs.fraction_goals_env_goals': [0, 0.5],
-    }
-    sweeper = hyp.DeterministicHyperparameterSweeper(
-        search_space, default_parameters=variant,
-    )
-
-    variants = []
-    for variant in sweeper.iterate_hyperparameters():
-        variants.append(variant)
-
-    run_variants(experiment, variants, run_id=0)
diff --git a/experiments/ashvin/corl2019/debug/pusher2/rig1.py b/experiments/ashvin/corl2019/debug/pusher2/rig1.py
deleted file mode 100644
index 328dcf0..0000000
--- a/experiments/ashvin/corl2019/debug/pusher2/rig1.py
+++ /dev/null
@@ -1,222 +0,0 @@
-import railrl.misc.hyperparameter as hyp
-from experiments.murtaza.multiworld.skew_fit.reacher.generate_uniform_dataset import generate_uniform_dataset_reacher
-from multiworld.envs.mujoco.cameras import sawyer_init_camera_zoomed_in, sawyer_pusher_camera_upright_v2
-from railrl.launchers.launcher_util import run_experiment
-import railrl.torch.vae.vae_schedules as vae_schedules
-from railrl.torch.vae.conv_vae import imsize48_default_architecture, imsize48_default_architecture_with_more_hidden_layers
-from railrl.launchers.arglauncher import run_variants
-from railrl.torch.grill.cvae_experiments import (
-    grill_her_td3_offpolicy_online_vae_full_experiment,
-)
-from multiworld.envs.pygame.multiobject_pygame_env import Multiobj2DEnv
-from multiworld.envs.mujoco.sawyer_xyz.sawyer_push_multiobj_subset import SawyerMultiobjectEnv
-from railrl.torch.vae.conditional_conv_vae import DeltaCVAE
-from railrl.torch.vae.conditional_vae_trainer import DeltaCVAETrainer
-from railrl.data_management.online_conditional_vae_replay_buffer import \
-        OnlineConditionalVaeRelabelingBuffer
-
-x_var = 0.2
-x_low = -x_var
-x_high = x_var
-y_low = 0.5
-y_high = 0.7
-t = 0.05
-
-if __name__ == "__main__":
-    variant = dict(
-        double_algo=False,
-        online_vae_exploration=False,
-        imsize=48,
-        init_camera=sawyer_init_camera_zoomed_in,
-        env_class=SawyerMultiobjectEnv,
-        env_kwargs=dict(
-            num_objects=1,
-            object_meshes=None,
-            fixed_start=True,
-            num_scene_objects=[1],
-            maxlen=0.1,
-            action_repeat=1,
-            puck_goal_low=(x_low + 0.01, y_low + 0.01),
-            puck_goal_high=(x_high - 0.01, y_high - 0.01),
-            hand_goal_low=(x_low + 3*t, y_low + t),
-            hand_goal_high=(x_high - 3*t, y_high -t),
-            mocap_low=(x_low + 2*t, y_low , 0.0),
-            mocap_high=(x_high - 2*t, y_high, 0.5),
-            object_low=(x_low + 0.01, y_low + 0.01, 0.02),
-            object_high=(x_high - 0.01, y_high - 0.01, 0.02),
-            use_textures=True,
-            init_camera=sawyer_init_camera_zoomed_in,
-            # preload_obj_dict=[
-            #     dict(color2=(1, 0, 0)),
-            #     dict(color2=(0, 1, 0)),
-            #     dict(color2=(0, 0, 1)),
-            #     dict(color2=(1, .4, .7)),
-            #     dict(color2=(0, .4, .8)),
-            #     dict(color2=(.8, .8, 0)),
-            #     dict(color2=(1, .5, 0)),
-            #     dict(color2=(.4, 0, .4)),
-            #     dict(color2=(.4, .2, 0)),
-            #     dict(color2=(0, .4, .4)),
-            # ],
-        ),
-
-        grill_variant=dict(
-            save_video=True,
-            custom_goal_sampler='replay_buffer',
-            online_vae_trainer_kwargs=dict(
-                beta=20,
-                lr=0,
-            ),
-            save_video_period=50,
-            qf_kwargs=dict(
-                hidden_sizes=[400, 300],
-            ),
-            policy_kwargs=dict(
-                hidden_sizes=[400, 300],
-            ),
-            vf_kwargs=dict(
-                hidden_sizes=[400, 300],
-            ),
-            max_path_length=100,
-            algo_kwargs=dict(
-                batch_size=128,
-                num_epochs=1000,
-                num_eval_steps_per_epoch=1000,
-                num_expl_steps_per_train_loop=1000,
-                num_trains_per_train_loop=1000,
-                min_num_steps_before_training=1000,
-                vae_training_schedule=vae_schedules.never_train,
-                oracle_data=False,
-                vae_save_period=25,
-                parallel_vae_train=False,
-                dataset_path=None,
-                rl_offpolicy_num_training_steps=0,
-            ),
-            td3_trainer_kwargs=dict(
-                discount=0.99,
-                # min_num_steps_before_training=4000,
-                reward_scale=1.0,
-                # render=False,
-                tau=1e-2,
-            ),
-            replay_buffer_class=OnlineConditionalVaeRelabelingBuffer,
-            replay_buffer_kwargs=dict(
-                start_skew_epoch=10,
-                max_size=int(100000),
-                fraction_goals_rollout_goals=0.2,
-                fraction_goals_env_goals=0.5,
-                exploration_rewards_type='None',
-                vae_priority_type='vae_prob',
-                priority_function_kwargs=dict(
-                    sampling_method='importance_sampling',
-                    decoder_distribution='gaussian_identity_variance',
-                    # decoder_distribution='bernoulli',
-                    num_latents_to_sample=10,
-                ),
-                power=-1,
-                relabeling_goal_sampling_mode='vae_prior',
-            ),
-            exploration_goal_sampling_mode='vae_prior',
-            evaluation_goal_sampling_mode='reset_of_env',
-            normalize=False,
-            render=False,
-            exploration_noise=0.2,
-            exploration_type='ou',
-            training_mode='train',
-            testing_mode='test',
-            reward_params=dict(
-                epsilon=0.05,
-            ),
-            observation_key='latent_observation',
-            desired_goal_key='latent_desired_goal',
-            vae_wrapped_env_kwargs=dict(
-                sample_from_true_prior=True,
-            ),
-            algorithm='ONLINE-VAE-SAC-BERNOULLI',
-            # vae_path="datasets/pusher_color_spectrum/itr_1900.pkl",
-        ),
-        train_vae_variant=dict(
-            beta=10,
-            beta_schedule_kwargs=dict(
-                x_values=(0, 1500),
-                y_values=(1, 50),
-            ),
-            num_epochs=1000,
-            dump_skew_debug_plots=False,
-            # decoder_activation='gaussian',
-            decoder_activation='sigmoid',
-            use_linear_dynamics=False,
-            generate_vae_dataset_kwargs=dict(
-                N=102000,
-                n_random_steps=51,
-                test_p=.9,
-                use_cached=False,
-                show=False,
-                oracle_dataset=False,
-                oracle_dataset_using_set_to_goal=False,
-                non_presampled_goal_img_is_garbage=False,
-                random_rollout_data=True,
-                random_rollout_data_set_to_goal=False,
-                conditional_vae_dataset=True,
-                save_trajectories=False,
-                enviorment_dataset=False,
-                tag="ccrig1",
-            ),
-            # vae_trainer_class=DeltaCVAETrainer,
-            # vae_class=DeltaCVAE,
-            vae_kwargs=dict(
-                input_channels=3,
-                architecture=imsize48_default_architecture_with_more_hidden_layers,
-                decoder_distribution='gaussian_identity_variance',
-            ),
-            # TODO: why the redundancy?
-            algo_kwargs=dict(
-                start_skew_epoch=5000,
-                is_auto_encoder=False,
-                batch_size=32,
-                lr=1e-3,
-                skew_config=dict(
-                    method='vae_prob',
-                    power=0,
-                ),
-                skew_dataset=False,
-                priority_function_kwargs=dict(
-                    decoder_distribution='gaussian_identity_variance',
-                    sampling_method='importance_sampling',
-                    # sampling_method='true_prior_sampling',
-                    num_latents_to_sample=10,
-                ),
-                use_parallel_dataloading=False,
-            ),
-
-            save_period=25,
-        ),
-        region='us-west-2',
-
-        logger_variant=dict(
-            tensorboard=True,
-        ),
-
-        slurm_variant=dict(
-            timeout_min=48 * 60,
-            cpus_per_task=10,
-            gpus_per_node=1,
-        ),
-    )
-
-    search_space = {
-        'seedid': range(5),
-        'grill_variant.exploration_noise': [0.2, ],
-        'train_vae_variant.representation_size': [16,], #(3 * objects, 3 * colors)
-        'grill_variant.algo_kwargs.num_trains_per_train_loop':[4000, ],
-        'grill_variant.algo_kwargs.batch_size': [1024, ],
-    }
-    sweeper = hyp.DeterministicHyperparameterSweeper(
-        search_space, default_parameters=variant,
-    )
-
-    variants = []
-    for variant in sweeper.iterate_hyperparameters():
-        variants.append(variant)
-
-    run_variants(grill_her_td3_offpolicy_online_vae_full_experiment, variants, run_id=1)
diff --git a/experiments/ashvin/corl2019/debug/pusher2/rig_debug.py b/experiments/ashvin/corl2019/debug/pusher2/rig_debug.py
deleted file mode 100644
index 0925c08..0000000
--- a/experiments/ashvin/corl2019/debug/pusher2/rig_debug.py
+++ /dev/null
@@ -1,222 +0,0 @@
-import railrl.misc.hyperparameter as hyp
-from experiments.murtaza.multiworld.skew_fit.reacher.generate_uniform_dataset import generate_uniform_dataset_reacher
-from multiworld.envs.mujoco.cameras import sawyer_init_camera_zoomed_in, sawyer_pusher_camera_upright_v2
-from railrl.launchers.launcher_util import run_experiment
-import railrl.torch.vae.vae_schedules as vae_schedules
-from railrl.torch.vae.conv_vae import imsize48_default_architecture, imsize48_default_architecture_with_more_hidden_layers
-from railrl.launchers.arglauncher import run_variants
-from railrl.torch.grill.cvae_experiments import (
-    grill_her_td3_offpolicy_online_vae_full_experiment,
-)
-from multiworld.envs.pygame.multiobject_pygame_env import Multiobj2DEnv
-from multiworld.envs.mujoco.sawyer_xyz.sawyer_push_multiobj_subset import SawyerMultiobjectEnv
-from railrl.torch.vae.conditional_conv_vae import DeltaCVAE
-from railrl.torch.vae.conditional_vae_trainer import DeltaCVAETrainer
-from railrl.data_management.online_conditional_vae_replay_buffer import \
-        OnlineConditionalVaeRelabelingBuffer
-
-x_var = 0.2
-x_low = -x_var
-x_high = x_var
-y_low = 0.5
-y_high = 0.7
-t = 0.05
-
-if __name__ == "__main__":
-    variant = dict(
-        double_algo=False,
-        online_vae_exploration=False,
-        imsize=48,
-        init_camera=sawyer_init_camera_zoomed_in,
-        env_class=SawyerMultiobjectEnv,
-        env_kwargs=dict(
-            num_objects=1,
-            object_meshes=None,
-            fixed_start=True,
-            num_scene_objects=[1],
-            maxlen=0.1,
-            action_repeat=1,
-            puck_goal_low=(x_low + 0.01, y_low + 0.01),
-            puck_goal_high=(x_high - 0.01, y_high - 0.01),
-            hand_goal_low=(x_low + 3*t, y_low + t),
-            hand_goal_high=(x_high - 3*t, y_high -t),
-            mocap_low=(x_low + 2*t, y_low , 0.0),
-            mocap_high=(x_high - 2*t, y_high, 0.5),
-            object_low=(x_low + 0.01, y_low + 0.01, 0.02),
-            object_high=(x_high - 0.01, y_high - 0.01, 0.02),
-            use_textures=True,
-            init_camera=sawyer_init_camera_zoomed_in,
-            # preload_obj_dict=[
-            #     dict(color2=(1, 0, 0)),
-            #     dict(color2=(0, 1, 0)),
-            #     dict(color2=(0, 0, 1)),
-            #     dict(color2=(1, .4, .7)),
-            #     dict(color2=(0, .4, .8)),
-            #     dict(color2=(.8, .8, 0)),
-            #     dict(color2=(1, .5, 0)),
-            #     dict(color2=(.4, 0, .4)),
-            #     dict(color2=(.4, .2, 0)),
-            #     dict(color2=(0, .4, .4)),
-            # ],
-        ),
-
-        grill_variant=dict(
-            save_video=True,
-            custom_goal_sampler='replay_buffer',
-            online_vae_trainer_kwargs=dict(
-                beta=20,
-                lr=0,
-            ),
-            save_video_period=50,
-            qf_kwargs=dict(
-                hidden_sizes=[400, 300],
-            ),
-            policy_kwargs=dict(
-                hidden_sizes=[400, 300],
-            ),
-            vf_kwargs=dict(
-                hidden_sizes=[400, 300],
-            ),
-            max_path_length=100,
-            algo_kwargs=dict(
-                batch_size=128,
-                num_epochs=1000,
-                num_eval_steps_per_epoch=1000,
-                num_expl_steps_per_train_loop=1000,
-                num_trains_per_train_loop=1000,
-                min_num_steps_before_training=1000,
-                vae_training_schedule=vae_schedules.never_train,
-                oracle_data=False,
-                vae_save_period=25,
-                parallel_vae_train=False,
-                dataset_path=None,
-                rl_offpolicy_num_training_steps=0,
-            ),
-            td3_trainer_kwargs=dict(
-                discount=0.99,
-                # min_num_steps_before_training=4000,
-                reward_scale=1.0,
-                # render=False,
-                tau=1e-2,
-            ),
-            replay_buffer_class=OnlineConditionalVaeRelabelingBuffer,
-            replay_buffer_kwargs=dict(
-                start_skew_epoch=10,
-                max_size=int(100000),
-                fraction_goals_rollout_goals=0.2,
-                fraction_goals_env_goals=0.5,
-                exploration_rewards_type='None',
-                vae_priority_type='vae_prob',
-                priority_function_kwargs=dict(
-                    sampling_method='importance_sampling',
-                    decoder_distribution='gaussian_identity_variance',
-                    # decoder_distribution='bernoulli',
-                    num_latents_to_sample=10,
-                ),
-                power=-1,
-                relabeling_goal_sampling_mode='vae_prior',
-            ),
-            exploration_goal_sampling_mode='vae_prior',
-            evaluation_goal_sampling_mode='reset_of_env',
-            normalize=False,
-            render=False,
-            exploration_noise=0.2,
-            exploration_type='ou',
-            training_mode='train',
-            testing_mode='test',
-            reward_params=dict(
-                epsilon=0.05,
-            ),
-            observation_key='latent_observation',
-            desired_goal_key='latent_desired_goal',
-            vae_wrapped_env_kwargs=dict(
-                sample_from_true_prior=True,
-            ),
-            algorithm='ONLINE-VAE-SAC-BERNOULLI',
-            # vae_path="datasets/pusher_color_spectrum/itr_1900.pkl",
-        ),
-        train_vae_variant=dict(
-            beta=10,
-            beta_schedule_kwargs=dict(
-                x_values=(0, 1500),
-                y_values=(1, 50),
-            ),
-            num_epochs=5,
-            dump_skew_debug_plots=False,
-            # decoder_activation='gaussian',
-            decoder_activation='sigmoid',
-            use_linear_dynamics=False,
-            generate_vae_dataset_kwargs=dict(
-                N=1020,
-                n_random_steps=51,
-                test_p=.9,
-                use_cached=False,
-                show=False,
-                oracle_dataset=False,
-                oracle_dataset_using_set_to_goal=False,
-                non_presampled_goal_img_is_garbage=False,
-                random_rollout_data=True,
-                random_rollout_data_set_to_goal=False,
-                conditional_vae_dataset=True,
-                save_trajectories=False,
-                enviorment_dataset=False,
-                tag="ccrig1",
-            ),
-            # vae_trainer_class=DeltaCVAETrainer,
-            # vae_class=DeltaCVAE,
-            vae_kwargs=dict(
-                input_channels=3,
-                architecture=imsize48_default_architecture_with_more_hidden_layers,
-                decoder_distribution='gaussian_identity_variance',
-            ),
-            # TODO: why the redundancy?
-            algo_kwargs=dict(
-                start_skew_epoch=5000,
-                is_auto_encoder=False,
-                batch_size=32,
-                lr=1e-3,
-                skew_config=dict(
-                    method='vae_prob',
-                    power=0,
-                ),
-                skew_dataset=False,
-                priority_function_kwargs=dict(
-                    decoder_distribution='gaussian_identity_variance',
-                    sampling_method='importance_sampling',
-                    # sampling_method='true_prior_sampling',
-                    num_latents_to_sample=10,
-                ),
-                use_parallel_dataloading=False,
-            ),
-
-            save_period=25,
-        ),
-        region='us-west-2',
-
-        logger_variant=dict(
-            tensorboard=True,
-        ),
-
-        slurm_variant=dict(
-            timeout_min=48 * 60,
-            cpus_per_task=10,
-            gpus_per_node=1,
-        ),
-    )
-
-    search_space = {
-        'seedid': range(5),
-        'grill_variant.exploration_noise': [0.2, ],
-        'train_vae_variant.representation_size': [16,], #(3 * objects, 3 * colors)
-        'grill_variant.algo_kwargs.num_trains_per_train_loop':[4000, ],
-        'grill_variant.algo_kwargs.batch_size': [1024, ],
-    }
-    sweeper = hyp.DeterministicHyperparameterSweeper(
-        search_space, default_parameters=variant,
-    )
-
-    variants = []
-    for variant in sweeper.iterate_hyperparameters():
-        variants.append(variant)
-
-    run_variants(grill_her_td3_offpolicy_online_vae_full_experiment, variants, run_id=0)
diff --git a/experiments/ashvin/corl2019/robot/rig1.py b/experiments/ashvin/corl2019/robot/rig1.py
new file mode 100644
index 0000000..29598b9
--- /dev/null
+++ b/experiments/ashvin/corl2019/robot/rig1.py
@@ -0,0 +1,247 @@
+# from sawyer_control.envs.sawyer_reaching import SawyerReachXYZEnv
+from multiworld.envs.real_world.sawyer.sawyer_reaching import SawyerReachXYZEnv
+
+import railrl.misc.hyperparameter as hyp
+from experiments.murtaza.multiworld.skew_fit.reacher.generate_uniform_dataset import generate_uniform_dataset_reacher
+from multiworld.envs.mujoco.cameras import sawyer_init_camera_zoomed_in, sawyer_pusher_camera_upright_v2
+from railrl.launchers.launcher_util import run_experiment
+import railrl.torch.vae.vae_schedules as vae_schedules
+from railrl.torch.vae.conv_vae import imsize48_default_architecture, imsize48_default_architecture_with_more_hidden_layers
+from railrl.launchers.arglauncher import run_variants
+from railrl.torch.grill.cvae_experiments import (
+    grill_her_td3_offpolicy_online_vae_full_experiment,
+)
+# from multiworld.envs.pygame.multiobject_pygame_env import Multiobj2DEnv
+# from multiworld.envs.mujoco.sawyer_xyz.sawyer_push_multiobj_subset import SawyerMultiobjectEnv
+from railrl.torch.vae.conditional_conv_vae import DeltaCVAE
+from railrl.torch.vae.conditional_vae_trainer import DeltaCVAETrainer
+from railrl.data_management.online_conditional_vae_replay_buffer import \
+        OnlineConditionalVaeRelabelingBuffer
+
+x_var = 0.2
+x_low = -x_var
+x_high = x_var
+y_low = 0.5
+y_high = 0.7
+t = 0.05
+
+if __name__ == "__main__":
+    variant = dict(
+        double_algo=False,
+        online_vae_exploration=False,
+        imsize=48,
+        init_camera=sawyer_init_camera_zoomed_in,
+        env_class=SawyerReachXYZEnv,
+        env_kwargs=dict(
+            action_mode="position", 
+            max_speed = 0.05, 
+            camera="sawyer_head",
+            fixed_z=True,
+        ),
+
+        grill_variant=dict(
+            save_video=True,
+            custom_goal_sampler='replay_buffer',
+            online_vae_trainer_kwargs=dict(
+                beta=20,
+                lr=0,
+            ),
+            save_video_period=1,
+            qf_kwargs=dict(
+                hidden_sizes=[400, 300],
+            ),
+            policy_kwargs=dict(
+                hidden_sizes=[400, 300],
+            ),
+            vf_kwargs=dict(
+                hidden_sizes=[400, 300],
+            ),
+            max_path_length=100,
+            algo_kwargs=dict(
+                batch_size=128,
+                num_epochs=300,
+                num_eval_steps_per_epoch=1000,
+                num_expl_steps_per_train_loop=1000,
+                num_trains_per_train_loop=1000,
+                min_num_steps_before_training=0,
+                vae_training_schedule=vae_schedules.never_train,
+                oracle_data=False,
+                vae_save_period=25,
+                parallel_vae_train=False,
+                dataset_path=["/home/ashvin/data/pusher_pucks/puck_black.npy",
+                "/home/ashvin/data/pusher_pucks/puck_blue.npy",
+                "/home/ashvin/data/pusher_pucks/puck_blue1.npy",
+                "/home/ashvin/data/pusher_pucks/puck_gold.npy",
+                "/home/ashvin/data/pusher_pucks/puck_green.npy",
+                "/home/ashvin/data/pusher_pucks/puck_purple.npy",
+                "/home/ashvin/data/pusher_pucks/puck_purple1.npy",
+                "/home/ashvin/data/pusher_pucks/puck_red.npy",
+                "/home/ashvin/data/pusher_pucks/puck_red1.npy",
+                "/home/ashvin/data/pusher_pucks/puck_white.npy",
+                "/home/ashvin/data/pusher_pucks/bear.npy",
+                "/home/ashvin/data/pusher_pucks/cat.npy",
+                "/home/ashvin/data/pusher_pucks/dog.npy",
+                "/home/ashvin/data/pusher_pucks/jeans.npy",
+                "/home/ashvin/data/pusher_pucks/star.npy",
+                "/home/ashvin/data/pusher_pucks/towel_brown.npy",
+                "/home/ashvin/data/pusher_pucks/towel_purple.npy",
+                "/home/ashvin/data/pusher_pucks/towel_red.npy",
+                "/home/ashvin/data/pusher_pucks/lego_blue.npy",
+                "/home/ashvin/data/pusher_pucks/lego_red.npy",
+                "/home/ashvin/data/pusher_pucks/lego_green.npy",
+                "/home/ashvin/data/pusher_pucks/lego_yellow.npy",],
+                rl_offpolicy_num_training_steps=50000,
+            ),
+            td3_trainer_kwargs=dict(
+                discount=0.99,
+                # min_num_steps_before_training=4000,
+                reward_scale=1.0,
+                # render=False,
+                tau=1e-2,
+            ),
+            replay_buffer_class=OnlineConditionalVaeRelabelingBuffer,
+            replay_buffer_kwargs=dict(
+                start_skew_epoch=10,
+                max_size=int(100000),
+                fraction_goals_rollout_goals=0.2,
+                fraction_goals_env_goals=0.5,
+                exploration_rewards_type='None',
+                vae_priority_type='vae_prob',
+                priority_function_kwargs=dict(
+                    sampling_method='importance_sampling',
+                    decoder_distribution='gaussian_identity_variance',
+                    # decoder_distribution='bernoulli',
+                    num_latents_to_sample=10,
+                ),
+                power=-1,
+                relabeling_goal_sampling_mode='vae_prior',
+            ),
+            exploration_goal_sampling_mode='vae_prior',
+            evaluation_goal_sampling_mode='env',
+            #presampled_goals="/home/ashvin/data/pusher_pucks/arm_data_big.npy",
+            normalize=False,
+            render=False,
+            exploration_noise=0.2,
+            exploration_type='ou',
+            training_mode='train',
+            testing_mode='test',
+            reward_params=dict(
+                epsilon=0.05,
+            ),
+            observation_key='latent_observation',
+            desired_goal_key='latent_desired_goal',
+            vae_wrapped_env_kwargs=dict(
+                sample_from_true_prior=True,
+            ),
+            algorithm='ONLINE-VAE-SAC-BERNOULLI',
+            # vae_path="/home/ashvin/data/s3doodad/ashvin/corl2019/robot/test1/run32/id0/vae.pkl",
+        ),
+        train_vae_variant=dict(
+            latent_sizes=4,
+            beta=10,
+            beta_schedule_kwargs=dict(
+                x_values=(0, 1500),
+                y_values=(1, 50),
+            ),
+            num_epochs=1500,
+            dump_skew_debug_plots=False,
+            # decoder_activation='gaussian',
+            decoder_activation='sigmoid',
+            use_linear_dynamics=False,
+            generate_vae_dataset_kwargs=dict(
+                N=1000,
+                n_random_steps=200,
+                dataset_path=["/home/ashvin/data/pusher_pucks/puck_black.npy",
+                "/home/ashvin/data/pusher_pucks/puck_blue.npy",
+                "/home/ashvin/data/pusher_pucks/puck_blue1.npy",
+                "/home/ashvin/data/pusher_pucks/puck_gold.npy",
+                "/home/ashvin/data/pusher_pucks/puck_green.npy",
+                "/home/ashvin/data/pusher_pucks/puck_purple.npy",
+                "/home/ashvin/data/pusher_pucks/puck_purple1.npy",
+                "/home/ashvin/data/pusher_pucks/puck_red.npy",
+                "/home/ashvin/data/pusher_pucks/puck_red1.npy",
+                "/home/ashvin/data/pusher_pucks/puck_white.npy",
+                "/home/ashvin/data/pusher_pucks/bear.npy",
+                "/home/ashvin/data/pusher_pucks/cat.npy",
+                "/home/ashvin/data/pusher_pucks/dog.npy",
+                "/home/ashvin/data/pusher_pucks/jeans.npy",
+                "/home/ashvin/data/pusher_pucks/star.npy",
+                "/home/ashvin/data/pusher_pucks/towel_brown.npy",
+                "/home/ashvin/data/pusher_pucks/towel_purple.npy",
+                "/home/ashvin/data/pusher_pucks/towel_red.npy",
+                "/home/ashvin/data/pusher_pucks/lego_blue.npy",
+                "/home/ashvin/data/pusher_pucks/lego_red.npy",
+                "/home/ashvin/data/pusher_pucks/lego_green.npy",
+                "/home/ashvin/data/pusher_pucks/lego_yellow.npy",],
+                test_p=.9,
+                use_cached=False,
+                show=False,
+                oracle_dataset=False,
+                oracle_dataset_using_set_to_goal=False,
+                random_rollout_data=True,
+                random_rollout_data_set_to_goal=False,
+                conditional_vae_dataset=True,
+                save_trajectories=True,
+                save_file_prefix="delete",
+                save_directory="/home/ashvin/data/pusher_pucks/",
+                tag="ccrig1",
+
+                train_batch_loader_kwargs=dict(
+                    batch_size=128,
+                ),
+                test_batch_loader_kwargs=dict(
+                    batch_size=128,
+                ),
+            ),
+            # vae_trainer_class=DeltaCVAETrainer,
+            # vae_class=DeltaCVAE,
+            vae_kwargs=dict(
+                input_channels=3,
+                architecture=imsize48_default_architecture_with_more_hidden_layers,
+                decoder_distribution='gaussian_identity_variance',
+            ),
+            # TODO: why the redundancy?
+            algo_kwargs=dict(
+                start_skew_epoch=5000,
+                is_auto_encoder=False,
+                batch_size=128,
+                lr=1e-3,
+                skew_config=dict(
+                    method='vae_prob',
+                    power=0,
+                ),
+                skew_dataset=False,
+                priority_function_kwargs=dict(
+                    decoder_distribution='gaussian_identity_variance',
+                    sampling_method='importance_sampling',
+                    # sampling_method='true_prior_sampling',
+                    num_latents_to_sample=10,
+                ),
+                use_parallel_dataloading=False,
+            ),
+
+            save_period=10,
+        ),
+        region='us-west-2',
+
+        logger_variant=dict(
+            tensorboard=True,
+        ),
+    )
+
+    search_space = {
+        'seedid': range(1),
+        'grill_variant.exploration_noise': [0.3, ],
+        # 'train_vae_variant.latent_sizes': [(6, 6),], #(3 * objects, 3 * colors)
+        'train_vae_variant.representation_size': [12, ],
+        'grill_variant.algo_kwargs.num_trains_per_train_loop':[1000, ],
+    }
+    sweeper = hyp.DeterministicHyperparameterSweeper(
+        search_space, default_parameters=variant,
+    )
+
+    variants = []
+    for variant in sweeper.iterate_hyperparameters():
+        variants.append(variant)
+
+    run_variants(grill_her_td3_offpolicy_online_vae_full_experiment, variants, run_id=0)
diff --git a/experiments/ashvin/corl2019/robot/rig2.py b/experiments/ashvin/corl2019/robot/rig2.py
new file mode 100644
index 0000000..b8baeed
--- /dev/null
+++ b/experiments/ashvin/corl2019/robot/rig2.py
@@ -0,0 +1,248 @@
+# from sawyer_control.envs.sawyer_reaching import SawyerReachXYZEnv
+from multiworld.envs.real_world.sawyer.sawyer_reaching import SawyerReachXYZEnv
+
+import railrl.misc.hyperparameter as hyp
+from experiments.murtaza.multiworld.skew_fit.reacher.generate_uniform_dataset import generate_uniform_dataset_reacher
+from multiworld.envs.mujoco.cameras import sawyer_init_camera_zoomed_in, sawyer_pusher_camera_upright_v2
+from railrl.launchers.launcher_util import run_experiment
+import railrl.torch.vae.vae_schedules as vae_schedules
+from railrl.torch.vae.conv_vae import imsize48_default_architecture, imsize48_default_architecture_with_more_hidden_layers
+from railrl.launchers.arglauncher import run_variants
+from railrl.torch.grill.cvae_experiments import (
+    grill_her_td3_offpolicy_online_vae_full_experiment,
+)
+# from multiworld.envs.pygame.multiobject_pygame_env import Multiobj2DEnv
+# from multiworld.envs.mujoco.sawyer_xyz.sawyer_push_multiobj_subset import SawyerMultiobjectEnv
+from railrl.torch.vae.conditional_conv_vae import DeltaCVAE
+from railrl.torch.vae.conditional_vae_trainer import DeltaCVAETrainer
+from railrl.data_management.online_conditional_vae_replay_buffer import \
+        OnlineConditionalVaeRelabelingBuffer
+
+x_var = 0.2
+x_low = -x_var
+x_high = x_var
+y_low = 0.5
+y_high = 0.7
+t = 0.05
+
+if __name__ == "__main__":
+    variant = dict(
+        double_algo=False,
+        online_vae_exploration=False,
+        imsize=48,
+        init_camera=sawyer_init_camera_zoomed_in,
+        env_class=SawyerReachXYZEnv,
+        env_kwargs=dict(
+            action_mode="position", 
+            max_speed = 0.05, 
+            camera="sawyer_head",
+            fixed_z=True,
+        ),
+
+        grill_variant=dict(
+            save_video=True,
+            custom_goal_sampler='replay_buffer',
+            online_vae_trainer_kwargs=dict(
+                beta=20,
+                lr=0,
+            ),
+            save_video_period=1,
+            qf_kwargs=dict(
+                hidden_sizes=[400, 300],
+            ),
+            policy_kwargs=dict(
+                hidden_sizes=[400, 300],
+            ),
+            vf_kwargs=dict(
+                hidden_sizes=[400, 300],
+            ),
+            max_path_length=100,
+            algo_kwargs=dict(
+                batch_size=128,
+                num_epochs=300,
+                num_eval_steps_per_epoch=20000,
+                num_expl_steps_per_train_loop=1000,
+                num_trains_per_train_loop=1000,
+                min_num_steps_before_training=0,
+                vae_training_schedule=vae_schedules.never_train,
+                oracle_data=False,
+                vae_save_period=25,
+                parallel_vae_train=False,
+                dataset_path=["/home/ashvin/data/pusher_pucks/puck_black.npy",
+                "/home/ashvin/data/pusher_pucks/puck_blue.npy",
+                "/home/ashvin/data/pusher_pucks/puck_blue1.npy",
+                "/home/ashvin/data/pusher_pucks/puck_gold.npy",
+                "/home/ashvin/data/pusher_pucks/puck_green.npy",
+                "/home/ashvin/data/pusher_pucks/puck_purple.npy",
+                "/home/ashvin/data/pusher_pucks/puck_purple1.npy",
+                "/home/ashvin/data/pusher_pucks/puck_red.npy",
+                "/home/ashvin/data/pusher_pucks/puck_red1.npy",
+                "/home/ashvin/data/pusher_pucks/puck_white.npy",
+                "/home/ashvin/data/pusher_pucks/bear.npy",
+                "/home/ashvin/data/pusher_pucks/cat.npy",
+                "/home/ashvin/data/pusher_pucks/dog.npy",
+                "/home/ashvin/data/pusher_pucks/jeans.npy",
+                "/home/ashvin/data/pusher_pucks/star.npy",
+                "/home/ashvin/data/pusher_pucks/towel_brown.npy",
+                "/home/ashvin/data/pusher_pucks/towel_purple.npy",
+                "/home/ashvin/data/pusher_pucks/towel_red.npy",
+                "/home/ashvin/data/pusher_pucks/lego_blue.npy",
+                "/home/ashvin/data/pusher_pucks/lego_red.npy",
+                "/home/ashvin/data/pusher_pucks/lego_green.npy",
+                "/home/ashvin/data/pusher_pucks/lego_yellow.npy",],
+                rl_offpolicy_num_training_steps=50000,
+            ),
+            td3_trainer_kwargs=dict(
+                discount=0.99,
+                # min_num_steps_before_training=4000,
+                reward_scale=1.0,
+                # render=False,
+                tau=1e-2,
+            ),
+            # replay_buffer_class=OnlineConditionalVaeRelabelingBuffer,
+            replay_buffer_kwargs=dict(
+                start_skew_epoch=10,
+                max_size=int(100000),
+                fraction_goals_rollout_goals=0.2,
+                fraction_goals_env_goals=0.5,
+                exploration_rewards_type='None',
+                vae_priority_type='vae_prob',
+                priority_function_kwargs=dict(
+                    sampling_method='importance_sampling',
+                    decoder_distribution='gaussian_identity_variance',
+                    # decoder_distribution='bernoulli',
+                    num_latents_to_sample=10,
+                ),
+                power=-1,
+                relabeling_goal_sampling_mode='vae_prior',
+            ),
+            exploration_goal_sampling_mode='vae_prior',
+            evaluation_goal_sampling_mode='env',
+            #presampled_goals="/home/ashvin/data/pusher_pucks/arm_data_big.npy",
+            normalize=False,
+            render=False,
+            exploration_noise=0.2,
+            exploration_type='ou',
+            training_mode='train',
+            testing_mode='test',
+            reward_params=dict(
+                epsilon=0.05,
+            ),
+            observation_key='latent_observation',
+            desired_goal_key='latent_desired_goal',
+            vae_wrapped_env_kwargs=dict(
+                sample_from_true_prior=True,
+            ),
+            algorithm='ONLINE-VAE-SAC-BERNOULLI',
+            vae_path="/home/ashvin/data/s3doodad/ashvin/corl2019/robot/rig2/run0/id0/vae.pkl",
+        ),
+        train_vae_variant=dict(
+            latent_sizes=4,
+            beta=10,
+            beta_schedule_kwargs=dict(
+                x_values=(0, 1500),
+                y_values=(1, 50),
+            ),
+            num_epochs=1500,
+            dump_skew_debug_plots=False,
+            # decoder_activation='gaussian',
+            decoder_activation='sigmoid',
+            use_linear_dynamics=False,
+            generate_vae_dataset_kwargs=dict(
+                N=1000,
+                n_random_steps=200,
+                dataset_path=["/home/ashvin/data/pusher_pucks/puck_black.npy",
+                "/home/ashvin/data/pusher_pucks/puck_blue.npy",
+                "/home/ashvin/data/pusher_pucks/puck_blue1.npy",
+                "/home/ashvin/data/pusher_pucks/puck_gold.npy",
+                "/home/ashvin/data/pusher_pucks/puck_green.npy",
+                "/home/ashvin/data/pusher_pucks/puck_purple.npy",
+                "/home/ashvin/data/pusher_pucks/puck_purple1.npy",
+                "/home/ashvin/data/pusher_pucks/puck_red.npy",
+                "/home/ashvin/data/pusher_pucks/puck_red1.npy",
+                "/home/ashvin/data/pusher_pucks/puck_white.npy",
+                "/home/ashvin/data/pusher_pucks/bear.npy",
+                "/home/ashvin/data/pusher_pucks/cat.npy",
+                "/home/ashvin/data/pusher_pucks/dog.npy",
+                "/home/ashvin/data/pusher_pucks/jeans.npy",
+                "/home/ashvin/data/pusher_pucks/star.npy",
+                "/home/ashvin/data/pusher_pucks/towel_brown.npy",
+                "/home/ashvin/data/pusher_pucks/towel_purple.npy",
+                "/home/ashvin/data/pusher_pucks/towel_red.npy",
+                "/home/ashvin/data/pusher_pucks/lego_blue.npy",
+                "/home/ashvin/data/pusher_pucks/lego_red.npy",
+                "/home/ashvin/data/pusher_pucks/lego_green.npy",
+                "/home/ashvin/data/pusher_pucks/lego_yellow.npy",],
+                test_p=.9,
+                use_cached=False,
+                show=False,
+                oracle_dataset=False,
+                oracle_dataset_using_set_to_goal=False,
+                random_rollout_data=True,
+                random_rollout_data_set_to_goal=False,
+                conditional_vae_dataset=True,
+                save_trajectories=True,
+                save_file_prefix="delete",
+                save_directory="/home/ashvin/data/pusher_pucks/",
+                tag="ccrig1",
+
+                train_batch_loader_kwargs=dict(
+                    batch_size=128,
+                    num_workers=10,
+                ),
+                test_batch_loader_kwargs=dict(
+                    batch_size=128,
+                ),
+            ),
+            # vae_trainer_class=DeltaCVAETrainer,
+            # vae_class=DeltaCVAE,
+            vae_kwargs=dict(
+                input_channels=3,
+                architecture=imsize48_default_architecture_with_more_hidden_layers,
+                decoder_distribution='gaussian_identity_variance',
+            ),
+            # TODO: why the redundancy?
+            algo_kwargs=dict(
+                start_skew_epoch=5000,
+                is_auto_encoder=False,
+                batch_size=128,
+                lr=1e-3,
+                skew_config=dict(
+                    method='vae_prob',
+                    power=0,
+                ),
+                skew_dataset=False,
+                priority_function_kwargs=dict(
+                    decoder_distribution='gaussian_identity_variance',
+                    sampling_method='importance_sampling',
+                    # sampling_method='true_prior_sampling',
+                    num_latents_to_sample=10,
+                ),
+                use_parallel_dataloading=False,
+            ),
+
+            save_period=10,
+        ),
+        region='us-west-2',
+
+        logger_variant=dict(
+            tensorboard=True,
+        ),
+    )
+
+    search_space = {
+        'seedid': range(1),
+        'grill_variant.exploration_noise': [0.3, ],
+        # 'train_vae_variant.latent_sizes': [(6, 6),], #(3 * objects, 3 * colors)
+        'train_vae_variant.representation_size': [12, ],
+        'grill_variant.algo_kwargs.num_trains_per_train_loop':[1000, ],
+    }
+    sweeper = hyp.DeterministicHyperparameterSweeper(
+        search_space, default_parameters=variant,
+    )
+
+    variants = []
+    for variant in sweeper.iterate_hyperparameters():
+        variants.append(variant)
+
+    run_variants(grill_her_td3_offpolicy_online_vae_full_experiment, variants, run_id=3)
diff --git a/experiments/ashvin/corl2019/robot/test1.py b/experiments/ashvin/corl2019/robot/test1.py
new file mode 100644
index 0000000..2ab66d8
--- /dev/null
+++ b/experiments/ashvin/corl2019/robot/test1.py
@@ -0,0 +1,246 @@
+# from sawyer_control.envs.sawyer_reaching import SawyerReachXYZEnv
+from multiworld.envs.real_world.sawyer.sawyer_reaching import SawyerReachXYZEnv
+
+import railrl.misc.hyperparameter as hyp
+from experiments.murtaza.multiworld.skew_fit.reacher.generate_uniform_dataset import generate_uniform_dataset_reacher
+from multiworld.envs.mujoco.cameras import sawyer_init_camera_zoomed_in, sawyer_pusher_camera_upright_v2
+from railrl.launchers.launcher_util import run_experiment
+import railrl.torch.vae.vae_schedules as vae_schedules
+from railrl.torch.vae.conv_vae import imsize48_default_architecture, imsize48_default_architecture_with_more_hidden_layers
+from railrl.launchers.arglauncher import run_variants
+from railrl.torch.grill.cvae_experiments import (
+    grill_her_td3_offpolicy_online_vae_full_experiment,
+)
+# from multiworld.envs.pygame.multiobject_pygame_env import Multiobj2DEnv
+# from multiworld.envs.mujoco.sawyer_xyz.sawyer_push_multiobj_subset import SawyerMultiobjectEnv
+from railrl.torch.vae.conditional_conv_vae import DeltaCVAE
+from railrl.torch.vae.conditional_vae_trainer import DeltaCVAETrainer
+from railrl.data_management.online_conditional_vae_replay_buffer import \
+        OnlineConditionalVaeRelabelingBuffer
+
+x_var = 0.2
+x_low = -x_var
+x_high = x_var
+y_low = 0.5
+y_high = 0.7
+t = 0.05
+
+if __name__ == "__main__":
+    variant = dict(
+        double_algo=False,
+        online_vae_exploration=False,
+        imsize=48,
+        init_camera=sawyer_init_camera_zoomed_in,
+        env_class=SawyerReachXYZEnv,
+        env_kwargs=dict(
+            action_mode="position", 
+            max_speed = 0.05, 
+            camera="sawyer_head",
+            fixed_z=True,
+        ),
+
+        grill_variant=dict(
+            save_video=True,
+            custom_goal_sampler='replay_buffer',
+            online_vae_trainer_kwargs=dict(
+                beta=20,
+                lr=0,
+            ),
+            save_video_period=1,
+            qf_kwargs=dict(
+                hidden_sizes=[400, 300],
+            ),
+            policy_kwargs=dict(
+                hidden_sizes=[400, 300],
+            ),
+            vf_kwargs=dict(
+                hidden_sizes=[400, 300],
+            ),
+            max_path_length=100,
+            algo_kwargs=dict(
+                batch_size=128,
+                num_epochs=300,
+                num_eval_steps_per_epoch=20000,
+                num_expl_steps_per_train_loop=1000,
+                num_trains_per_train_loop=1000,
+                min_num_steps_before_training=0,
+                vae_training_schedule=vae_schedules.never_train,
+                oracle_data=False,
+                vae_save_period=25,
+                parallel_vae_train=False,
+                dataset_path=["/home/ashvin/data/pusher_pucks/puck_black.npy",
+                "/home/ashvin/data/pusher_pucks/puck_blue.npy",
+                "/home/ashvin/data/pusher_pucks/puck_blue1.npy",
+                "/home/ashvin/data/pusher_pucks/puck_gold.npy",
+                "/home/ashvin/data/pusher_pucks/puck_green.npy",
+                "/home/ashvin/data/pusher_pucks/puck_purple.npy",
+                "/home/ashvin/data/pusher_pucks/puck_purple1.npy",
+                "/home/ashvin/data/pusher_pucks/puck_red.npy",
+                "/home/ashvin/data/pusher_pucks/puck_red1.npy",
+                "/home/ashvin/data/pusher_pucks/puck_white.npy",
+                "/home/ashvin/data/pusher_pucks/bear.npy",
+                "/home/ashvin/data/pusher_pucks/cat.npy",
+                "/home/ashvin/data/pusher_pucks/dog.npy",
+                "/home/ashvin/data/pusher_pucks/jeans.npy",
+                "/home/ashvin/data/pusher_pucks/star.npy",
+                "/home/ashvin/data/pusher_pucks/towel_brown.npy",
+                "/home/ashvin/data/pusher_pucks/towel_purple.npy",
+                "/home/ashvin/data/pusher_pucks/towel_red.npy",
+                "/home/ashvin/data/pusher_pucks/lego_blue.npy",
+                "/home/ashvin/data/pusher_pucks/lego_red.npy",
+                "/home/ashvin/data/pusher_pucks/lego_green.npy",
+                "/home/ashvin/data/pusher_pucks/lego_yellow.npy",],
+                rl_offpolicy_num_training_steps=50000,
+            ),
+            td3_trainer_kwargs=dict(
+                discount=0.99,
+                # min_num_steps_before_training=4000,
+                reward_scale=1.0,
+                # render=False,
+                tau=1e-2,
+            ),
+            replay_buffer_class=OnlineConditionalVaeRelabelingBuffer,
+            replay_buffer_kwargs=dict(
+                start_skew_epoch=10,
+                max_size=int(100000),
+                fraction_goals_rollout_goals=0.2,
+                fraction_goals_env_goals=0.5,
+                exploration_rewards_type='None',
+                vae_priority_type='vae_prob',
+                priority_function_kwargs=dict(
+                    sampling_method='importance_sampling',
+                    decoder_distribution='gaussian_identity_variance',
+                    # decoder_distribution='bernoulli',
+                    num_latents_to_sample=10,
+                ),
+                power=-1,
+                relabeling_goal_sampling_mode='vae_prior',
+            ),
+            exploration_goal_sampling_mode='vae_prior',
+            evaluation_goal_sampling_mode='env',
+            #presampled_goals="/home/ashvin/data/pusher_pucks/arm_data_big.npy",
+            normalize=False,
+            render=False,
+            exploration_noise=0.2,
+            exploration_type='ou',
+            training_mode='train',
+            testing_mode='test',
+            reward_params=dict(
+                epsilon=0.05,
+            ),
+            observation_key='latent_observation',
+            desired_goal_key='latent_desired_goal',
+            vae_wrapped_env_kwargs=dict(
+                sample_from_true_prior=True,
+            ),
+            algorithm='ONLINE-VAE-SAC-BERNOULLI',
+            vae_path="/home/ashvin/data/s3doodad/ashvin/corl2019/robot/test1/run32/id0/vae.pkl",
+        ),
+        train_vae_variant=dict(
+            latent_sizes=4,
+            beta=10,
+            beta_schedule_kwargs=dict(
+                x_values=(0, 1500),
+                y_values=(1, 50),
+            ),
+            num_epochs=1500,
+            dump_skew_debug_plots=False,
+            # decoder_activation='gaussian',
+            decoder_activation='sigmoid',
+            use_linear_dynamics=False,
+            generate_vae_dataset_kwargs=dict(
+                N=0,
+                n_random_steps=200,
+                dataset_path=["/home/ashvin/data/pusher_pucks/puck_black.npy",
+                "/home/ashvin/data/pusher_pucks/puck_blue.npy",
+                "/home/ashvin/data/pusher_pucks/puck_blue1.npy",
+                "/home/ashvin/data/pusher_pucks/puck_gold.npy",
+                "/home/ashvin/data/pusher_pucks/puck_green.npy",
+                "/home/ashvin/data/pusher_pucks/puck_purple.npy",
+                "/home/ashvin/data/pusher_pucks/puck_purple1.npy",
+                "/home/ashvin/data/pusher_pucks/puck_red.npy",
+                "/home/ashvin/data/pusher_pucks/puck_red1.npy",
+                "/home/ashvin/data/pusher_pucks/puck_white.npy",
+                "/home/ashvin/data/pusher_pucks/bear.npy",
+                "/home/ashvin/data/pusher_pucks/cat.npy",
+                "/home/ashvin/data/pusher_pucks/dog.npy",
+                "/home/ashvin/data/pusher_pucks/jeans.npy",
+                "/home/ashvin/data/pusher_pucks/star.npy",
+                "/home/ashvin/data/pusher_pucks/towel_brown.npy",
+                "/home/ashvin/data/pusher_pucks/towel_purple.npy",
+                "/home/ashvin/data/pusher_pucks/towel_red.npy",
+                "/home/ashvin/data/pusher_pucks/lego_blue.npy",
+                "/home/ashvin/data/pusher_pucks/lego_red.npy",
+                "/home/ashvin/data/pusher_pucks/lego_green.npy",
+                "/home/ashvin/data/pusher_pucks/lego_yellow.npy",],
+                test_p=.9,
+                use_cached=False,
+                show=False,
+                oracle_dataset=False,
+                oracle_dataset_using_set_to_goal=False,
+                random_rollout_data=True,
+                random_rollout_data_set_to_goal=False,
+                conditional_vae_dataset=True,
+                save_trajectories=True,
+                save_file_prefix="delete",
+                save_directory="/home/ashvin/data/pusher_pucks/",
+                tag="ccrig1",
+
+                train_batch_loader_kwargs=dict(
+                    batch_size=128,
+                ),
+                test_batch_loader_kwargs=dict(
+                    batch_size=128,
+                ),
+            ),
+            vae_trainer_class=DeltaCVAETrainer,
+            vae_class=DeltaCVAE,
+            vae_kwargs=dict(
+                input_channels=3,
+                architecture=imsize48_default_architecture_with_more_hidden_layers,
+                decoder_distribution='gaussian_identity_variance',
+            ),
+            # TODO: why the redundancy?
+            algo_kwargs=dict(
+                start_skew_epoch=5000,
+                is_auto_encoder=False,
+                batch_size=128,
+                lr=1e-3,
+                skew_config=dict(
+                    method='vae_prob',
+                    power=0,
+                ),
+                skew_dataset=False,
+                priority_function_kwargs=dict(
+                    decoder_distribution='gaussian_identity_variance',
+                    sampling_method='importance_sampling',
+                    # sampling_method='true_prior_sampling',
+                    num_latents_to_sample=10,
+                ),
+                use_parallel_dataloading=False,
+            ),
+
+            save_period=10,
+        ),
+        region='us-west-2',
+
+        logger_variant=dict(
+            tensorboard=True,
+        ),
+    )
+
+    search_space = {
+        'seedid': range(1),
+        'grill_variant.exploration_noise': [0.3, ],
+        'train_vae_variant.latent_sizes': [(6, 6),], #(3 * objects, 3 * colors)
+        'grill_variant.algo_kwargs.num_trains_per_train_loop':[1000, ],
+    }
+    sweeper = hyp.DeterministicHyperparameterSweeper(
+        search_space, default_parameters=variant,
+    )
+
+    variants = []
+    for variant in sweeper.iterate_hyperparameters():
+        variants.append(variant)
+
+    run_variants(grill_her_td3_offpolicy_online_vae_full_experiment, variants, run_id=55)
diff --git a/experiments/ashvin/iros2019/door_spacemouse_demos.py b/experiments/ashvin/iros2019/door_spacemouse_demos.py
deleted file mode 100644
index 16504ed..0000000
--- a/experiments/ashvin/iros2019/door_spacemouse_demos.py
+++ /dev/null
@@ -1,61 +0,0 @@
-from railrl.demos.collect_demo import collect_demos_fixed
-from railrl.demos.spacemouse.input_server import SpaceMouseExpert
-
-from multiworld.core.image_env import ImageEnv
-# from multiworld.envs.mujoco.cameras import sawyer_pusher_camera_upright_v2
-
-# from multiworld.envs.mujoco.sawyer_xyz.sawyer_push_multiobj import SawyerMultiobjectEnv
-# from multiworld.envs.pygame.point2d import Point2DWallEnv
-
-import gym
-import numpy as np
-
-# from sawyer_control.envs.sawyer_reaching import SawyerReachXYZEnv
-from multiworld.envs.real_world.sawyer.sawyer_reaching import SawyerReachXYZEnv
-
-import time
-import rospy
-
-# from sawyer_control.envs.sawyer_insertion_refined_USB_sparse_RLonly import SawyerHumanControlEnv
-
-if __name__ == '__main__':
-    scale = 0.1
-    expert = SpaceMouseExpert(
-        xyz_dims=3,
-        xyz_remap=[0, 1, 2],
-        xyz_scale=[-scale, -scale, scale],
-    )
-
-    # env = gym.make("MountainCarContinuous-v0")
-    # env = SawyerHumanControlEnv(action_mode='joint_space_impd', position_action_scale=1, max_speed=0.015)
-    env = SawyerReachXYZEnv(action_mode="position", max_speed = 0.05, camera="sawyer_head")
-
-    env = ImageEnv(env,
-        recompute_reward=False,
-        transpose=True,
-        image_length=450000,
-        reward_type="image_distance",
-        # init_camera=sawyer_pusher_camera_upright_v2,
-    )
-
-    # env.reset()
-
-    collect_demos_fixed(env, expert, "demo.npy", 2, horizon=1000, pause=0.05)
-
-    # o = None
-    # while True:
-    #     a, valid, reset, accept = expert.get_action(o)
-
-    #     if valid:
-    #         o, r, done, info = env.step(a)
-    #         time.sleep(0.05)
-
-    #     if reset or accept:
-    #         env.reset()
-
-    #     if rospy.is_shutdown():
-    #         break
-
-    #     time.sleep(0.01)
-
-    # exit()
\ No newline at end of file
diff --git a/experiments/ashvin/iros2019/spacemouse_robot.py b/experiments/ashvin/iros2019/spacemouse_robot.py
deleted file mode 100644
index a29127c..0000000
--- a/experiments/ashvin/iros2019/spacemouse_robot.py
+++ /dev/null
@@ -1,64 +0,0 @@
-from railrl.demos.collect_demo import collect_demos_fixed
-from railrl.demos.spacemouse.input_server import SpaceMouseExpert
-
-from multiworld.core.image_env import ImageEnv
-# from multiworld.envs.mujoco.cameras import sawyer_pusher_camera_upright_v2
-
-# from multiworld.envs.mujoco.sawyer_xyz.sawyer_push_multiobj import SawyerMultiobjectEnv
-# from multiworld.envs.pygame.point2d import Point2DWallEnv
-
-import gym
-import numpy as np
-
-# from sawyer_control.envs.sawyer_reaching import SawyerReachXYZEnv
-from multiworld.envs.real_world.sawyer.sawyer_reaching import SawyerReachXYZEnv
-
-import time
-import rospy
-
-# from sawyer_control.envs.sawyer_insertion_refined_USB_sparse_RLonly import SawyerHumanControlEnv
-
-if __name__ == '__main__':
-    scale = 0.1
-    expert = SpaceMouseExpert(
-        xyz_dims=3,
-        xyz_remap=[0, 1, 2],
-        xyz_scale=[-scale, -scale, scale],
-    )
-
-    # env = gym.make("MountainCarContinuous-v0")
-    # env = SawyerHumanControlEnv(action_mode='joint_space_impd', position_action_scale=1, max_speed=0.015)
-    env = SawyerReachXYZEnv(action_mode="position", max_speed = 0.05, camera="sawyer_head")
-
-    # env = SawyerMultiobjectEnv(
-    #     num_objects=1,
-    #     preload_obj_dict=[
-    #         dict(color2=(0.1, 0.1, 0.9)),
-    #     ],
-    # )
-    env = ImageEnv(env,
-        recompute_reward=False,
-        transpose=True,
-        image_length=3072000,
-        # init_camera=sawyer_pusher_camera_upright_v2,
-    )
-
-    # env.reset()
-
-    o = None
-    while True:
-        a, valid, reset, accept = expert.get_action(o)
-
-        if valid:
-            o, r, done, info = env.step(a)
-            time.sleep(0.05)
-
-        if reset or accept:
-            env.reset()
-
-        if rospy.is_shutdown():
-            break
-
-        time.sleep(0.01)
-
-    exit()
\ No newline at end of file
diff --git a/experiments/ashvin/rfeatures/sawyer/door/bc1.py b/experiments/ashvin/rfeatures/sawyer/door/bc1.py
deleted file mode 100644
index 7eb4365..0000000
--- a/experiments/ashvin/rfeatures/sawyer/door/bc1.py
+++ /dev/null
@@ -1,103 +0,0 @@
-import gym
-
-import railrl.torch.pytorch_util as ptu
-from railrl.data_management.obs_dict_replay_buffer import ObsDictRelabelingBuffer
-from railrl.exploration_strategies.base import \
-    PolicyWrappedWithExplorationStrategy
-from railrl.exploration_strategies.gaussian_and_epislon import \
-    GaussianAndEpislonStrategy
-from railrl.launchers.launcher_util import setup_logger
-from railrl.samplers.data_collector import GoalConditionedPathCollector
-from railrl.torch.her.her import HERTrainer
-from railrl.torch.networks import FlattenMlp, TanhMlpPolicy
-# from railrl.torch.td3.td3 import TD3
-from railrl.demos.td3_bc import TD3BCTrainer
-from railrl.torch.torch_rl_algorithm import TorchBatchRLAlgorithm
-# from multiworld.envs.mujoco.sawyer_xyz.sawyer_push_multiobj_subset import SawyerMultiobjectEnv
-# from multiworld.envs.mujoco.sawyer_xyz.sawyer_reach import SawyerReachXYZEnv
-
-from multiworld.core.image_env import ImageEnv
-from multiworld.envs.real_world.sawyer.sawyer_reaching import SawyerReachXYZEnv
-# from sawyer_control.envs.sawyer_reaching import SawyerReachXYZEnv
-
-from railrl.launchers.arglauncher import run_variants
-import railrl.misc.hyperparameter as hyp
-
-# from railrl.launchers.experiments.ashvin.rfeatures.rfeatures_trainer import TimePredictionTrainer
-from railrl.launchers.experiments.ashvin.rfeatures.rfeatures_rl import encoder_wrapped_td3bc_experiment
-
-if __name__ == "__main__":
-    variant = dict(
-        env_class=SawyerReachXYZEnv,
-        env_kwargs=dict(
-            action_mode="position", 
-            max_speed = 0.05, 
-            camera="sawyer_head"
-        ),
-        # algo_kwargs=dict(
-        #     num_epochs=3000,
-        #     max_path_length=20,
-        #     batch_size=128,
-        #     num_eval_steps_per_epoch=1000,
-        #     num_expl_steps_per_train_loop=1000,
-        #     num_trains_per_train_loop=1000,
-        #     min_num_steps_before_training=1000,
-        # ),
-        algo_kwargs=dict(
-            num_epochs=500,
-            max_path_length=100,
-            batch_size=128,
-            num_eval_steps_per_epoch=500,
-            num_expl_steps_per_train_loop=500,
-            num_trains_per_train_loop=500,
-            min_num_steps_before_training=0,
-        ),
-        model_kwargs=dict(
-            decoder_distribution='gaussian_identity_variance',
-            input_channels=3,
-            imsize=224,
-            architecture=dict(
-                hidden_sizes=[200, 200],
-            ),
-            delta_features=True,
-            pretrained_features=False,
-        ),
-        trainer_kwargs=dict(
-            discount=0.99,
-            demo_path="/home/anair/ros_ws/src/railrl-private/demo_v2_2_processed.npy",
-            add_demo_latents=False, # already done
-            bc_num_pretrain_steps=100,
-            rl_weight=0.0,
-            bc_weight=1.0,
-        ),
-        replay_buffer_kwargs=dict(
-            max_size=100000,
-            fraction_goals_rollout_goals=1.0,
-            fraction_goals_env_goals=0.0,
-        ),
-        qf_kwargs=dict(
-            hidden_sizes=[400, 300],
-        ),
-        policy_kwargs=dict(
-            hidden_sizes=[400, 300],
-        ),
-
-        save_video=True,
-        dump_video_kwargs=dict(
-            save_period=1,
-            # imsize=(3, 500, 300),
-        )
-    )
-
-    search_space = {
-        'seedid': range(1),
-    }
-    sweeper = hyp.DeterministicHyperparameterSweeper(
-        search_space, default_parameters=variant,
-    )
-
-    variants = []
-    for variant in sweeper.iterate_hyperparameters():
-        variants.append(variant)
-
-    run_variants(encoder_wrapped_td3bc_experiment, variants, run_id=2)
diff --git a/experiments/ashvin/rfeatures/sawyer/door/bc2.py b/experiments/ashvin/rfeatures/sawyer/door/bc2.py
deleted file mode 100644
index a43f465..0000000
--- a/experiments/ashvin/rfeatures/sawyer/door/bc2.py
+++ /dev/null
@@ -1,104 +0,0 @@
-import gym
-
-import railrl.torch.pytorch_util as ptu
-from railrl.data_management.obs_dict_replay_buffer import ObsDictRelabelingBuffer
-from railrl.exploration_strategies.base import \
-    PolicyWrappedWithExplorationStrategy
-from railrl.exploration_strategies.gaussian_and_epislon import \
-    GaussianAndEpislonStrategy
-from railrl.launchers.launcher_util import setup_logger
-from railrl.samplers.data_collector import GoalConditionedPathCollector
-from railrl.torch.her.her import HERTrainer
-from railrl.torch.networks import FlattenMlp, TanhMlpPolicy
-# from railrl.torch.td3.td3 import TD3
-from railrl.demos.td3_bc import TD3BCTrainer
-from railrl.torch.torch_rl_algorithm import TorchBatchRLAlgorithm
-# from multiworld.envs.mujoco.sawyer_xyz.sawyer_push_multiobj_subset import SawyerMultiobjectEnv
-# from multiworld.envs.mujoco.sawyer_xyz.sawyer_reach import SawyerReachXYZEnv
-
-from multiworld.core.image_env import ImageEnv
-from multiworld.envs.real_world.sawyer.sawyer_reaching import SawyerReachXYZEnv
-# from sawyer_control.envs.sawyer_reaching import SawyerReachXYZEnv
-
-from railrl.launchers.arglauncher import run_variants
-import railrl.misc.hyperparameter as hyp
-
-# from railrl.launchers.experiments.ashvin.rfeatures.rfeatures_trainer import TimePredictionTrainer
-from railrl.launchers.experiments.ashvin.rfeatures.rfeatures_rl import encoder_wrapped_td3bc_experiment
-
-if __name__ == "__main__":
-    variant = dict(
-        env_class=SawyerReachXYZEnv,
-        env_kwargs=dict(
-            action_mode="position", 
-            max_speed = 0.05, 
-            camera="sawyer_head"
-        ),
-        # algo_kwargs=dict(
-        #     num_epochs=3000,
-        #     max_path_length=20,
-        #     batch_size=128,
-        #     num_eval_steps_per_epoch=1000,
-        #     num_expl_steps_per_train_loop=1000,
-        #     num_trains_per_train_loop=1000,
-        #     min_num_steps_before_training=1000,
-        # ),
-        algo_kwargs=dict(
-            num_epochs=500,
-            max_path_length=100,
-            batch_size=128,
-            num_eval_steps_per_epoch=500,
-            num_expl_steps_per_train_loop=500,
-            num_trains_per_train_loop=500,
-            min_num_steps_before_training=0,
-        ),
-        model_kwargs=dict(
-            decoder_distribution='gaussian_identity_variance',
-            input_channels=3,
-            imsize=224,
-            architecture=dict(
-                hidden_sizes=[200, 200],
-            ),
-            delta_features=True,
-            pretrained_features=False,
-        ),
-        trainer_kwargs=dict(
-            discount=0.99,
-            demo_path="/home/lerrel/ros_ws/src/railrl-private/demo_v3_processed.npy",
-            add_demo_latents=False, # already done
-            bc_num_pretrain_steps=1000,
-            rl_weight=0.0,
-            bc_weight=1.0,
-            weight_decay=0.01,
-        ),
-        replay_buffer_kwargs=dict(
-            max_size=100000,
-            fraction_goals_rollout_goals=1.0,
-            fraction_goals_env_goals=0.0,
-        ),
-        qf_kwargs=dict(
-            hidden_sizes=[400, 300],
-        ),
-        policy_kwargs=dict(
-            hidden_sizes=[400, 300],
-        ),
-
-        save_video=True,
-        dump_video_kwargs=dict(
-            save_period=1,
-            # imsize=(3, 500, 300),
-        )
-    )
-
-    search_space = {
-        'seedid': range(1),
-    }
-    sweeper = hyp.DeterministicHyperparameterSweeper(
-        search_space, default_parameters=variant,
-    )
-
-    variants = []
-    for variant in sweeper.iterate_hyperparameters():
-        variants.append(variant)
-
-    run_variants(encoder_wrapped_td3bc_experiment, variants, run_id=3)
diff --git a/experiments/ashvin/rfeatures/sawyer/door/bc3.py b/experiments/ashvin/rfeatures/sawyer/door/bc3.py
deleted file mode 100644
index fe50195..0000000
--- a/experiments/ashvin/rfeatures/sawyer/door/bc3.py
+++ /dev/null
@@ -1,106 +0,0 @@
-import gym
-
-import railrl.torch.pytorch_util as ptu
-from railrl.data_management.obs_dict_replay_buffer import ObsDictRelabelingBuffer
-from railrl.exploration_strategies.base import \
-    PolicyWrappedWithExplorationStrategy
-from railrl.exploration_strategies.gaussian_and_epislon import \
-    GaussianAndEpislonStrategy
-from railrl.launchers.launcher_util import setup_logger
-from railrl.samplers.data_collector import GoalConditionedPathCollector
-from railrl.torch.her.her import HERTrainer
-from railrl.torch.networks import FlattenMlp, TanhMlpPolicy
-# from railrl.torch.td3.td3 import TD3
-from railrl.demos.td3_bc import TD3BCTrainer
-from railrl.torch.torch_rl_algorithm import TorchBatchRLAlgorithm
-# from multiworld.envs.mujoco.sawyer_xyz.sawyer_push_multiobj_subset import SawyerMultiobjectEnv
-# from multiworld.envs.mujoco.sawyer_xyz.sawyer_reach import SawyerReachXYZEnv
-
-from multiworld.core.image_env import ImageEnv
-from multiworld.envs.real_world.sawyer.sawyer_reaching import SawyerReachXYZEnv
-# from sawyer_control.envs.sawyer_reaching import SawyerReachXYZEnv
-
-from railrl.launchers.arglauncher import run_variants
-import railrl.misc.hyperparameter as hyp
-
-# from railrl.launchers.experiments.ashvin.rfeatures.rfeatures_trainer import TimePredictionTrainer
-from railrl.launchers.experiments.ashvin.rfeatures.rfeatures_rl import encoder_wrapped_td3bc_experiment
-
-if __name__ == "__main__":
-    variant = dict(
-        env_class=SawyerReachXYZEnv,
-        env_kwargs=dict(
-            action_mode="position", 
-            max_speed = 0.05, 
-            camera="sawyer_head"
-        ),
-        # algo_kwargs=dict(
-        #     num_epochs=3000,
-        #     max_path_length=20,
-        #     batch_size=128,
-        #     num_eval_steps_per_epoch=1000,
-        #     num_expl_steps_per_train_loop=1000,
-        #     num_trains_per_train_loop=1000,
-        #     min_num_steps_before_training=1000,
-        # ),
-        algo_kwargs=dict(
-            num_epochs=500,
-            max_path_length=100,
-            batch_size=128,
-            num_eval_steps_per_epoch=500,
-            num_expl_steps_per_train_loop=500,
-            num_trains_per_train_loop=500,
-            min_num_steps_before_training=0,
-        ),
-        model_kwargs=dict(
-            decoder_distribution='gaussian_identity_variance',
-            input_channels=3,
-            imsize=224,
-            architecture=dict(
-                hidden_sizes=[200, 200],
-            ),
-            delta_features=True,
-            pretrained_features=False,
-        ),
-        trainer_kwargs=dict(
-            discount=0.99,
-            demo_path="/home/lerrel/ros_ws/src/railrl-private/demo_v4_processed.npy",
-            add_demo_latents=False, # already done
-            bc_num_pretrain_steps=1000,
-            rl_weight=0.0,
-            bc_weight=1.0,
-            weight_decay=0.001,
-        ),
-        replay_buffer_kwargs=dict(
-            max_size=100000,
-            fraction_goals_rollout_goals=1.0,
-            fraction_goals_env_goals=0.0,
-        ),
-        qf_kwargs=dict(
-            hidden_sizes=[400, 300],
-        ),
-        policy_kwargs=dict(
-            hidden_sizes=[32, 32],
-        ),
-
-        save_video=True,
-        dump_video_kwargs=dict(
-            save_period=1,
-            # imsize=(3, 500, 300),
-        ),
-        desired_trajectory="demo_v4.npy",
-        snapshot_mode="all",
-    )
-
-    search_space = {
-        'seedid': range(1),
-    }
-    sweeper = hyp.DeterministicHyperparameterSweeper(
-        search_space, default_parameters=variant,
-    )
-
-    variants = []
-    for variant in sweeper.iterate_hyperparameters():
-        variants.append(variant)
-
-    run_variants(encoder_wrapped_td3bc_experiment, variants, run_id=1)
diff --git a/experiments/ashvin/rfeatures/sawyer/door/bc4.py b/experiments/ashvin/rfeatures/sawyer/door/bc4.py
deleted file mode 100644
index fbc0877..0000000
--- a/experiments/ashvin/rfeatures/sawyer/door/bc4.py
+++ /dev/null
@@ -1,105 +0,0 @@
-import gym
-
-import railrl.torch.pytorch_util as ptu
-from railrl.data_management.obs_dict_replay_buffer import ObsDictRelabelingBuffer
-from railrl.exploration_strategies.base import \
-    PolicyWrappedWithExplorationStrategy
-from railrl.exploration_strategies.gaussian_and_epislon import \
-    GaussianAndEpislonStrategy
-from railrl.launchers.launcher_util import setup_logger
-from railrl.samplers.data_collector import GoalConditionedPathCollector
-from railrl.torch.her.her import HERTrainer
-from railrl.torch.networks import FlattenMlp, TanhMlpPolicy
-# from railrl.torch.td3.td3 import TD3
-from railrl.demos.td3_bc import TD3BCTrainer
-from railrl.torch.torch_rl_algorithm import TorchBatchRLAlgorithm
-# from multiworld.envs.mujoco.sawyer_xyz.sawyer_push_multiobj_subset import SawyerMultiobjectEnv
-# from multiworld.envs.mujoco.sawyer_xyz.sawyer_reach import SawyerReachXYZEnv
-
-from multiworld.core.image_env import ImageEnv
-from multiworld.envs.real_world.sawyer.sawyer_reaching import SawyerReachXYZEnv
-# from sawyer_control.envs.sawyer_reaching import SawyerReachXYZEnv
-
-from railrl.launchers.arglauncher import run_variants
-import railrl.misc.hyperparameter as hyp
-
-# from railrl.launchers.experiments.ashvin.rfeatures.rfeatures_trainer import TimePredictionTrainer
-from railrl.launchers.experiments.ashvin.rfeatures.rfeatures_rl import encoder_wrapped_td3bc_experiment
-
-if __name__ == "__main__":
-    variant = dict(
-        env_class=SawyerReachXYZEnv,
-        env_kwargs=dict(
-            action_mode="position", 
-            max_speed = 0.05, 
-            camera="sawyer_head"
-        ),
-        # algo_kwargs=dict(
-        #     num_epochs=3000,
-        #     max_path_length=20,
-        #     batch_size=128,
-        #     num_eval_steps_per_epoch=1000,
-        #     num_expl_steps_per_train_loop=1000,
-        #     num_trains_per_train_loop=1000,
-        #     min_num_steps_before_training=1000,
-        # ),
-        algo_kwargs=dict(
-            num_epochs=500,
-            max_path_length=100,
-            batch_size=128,
-            num_eval_steps_per_epoch=500,
-            num_expl_steps_per_train_loop=500,
-            num_trains_per_train_loop=500,
-            min_num_steps_before_training=0,
-        ),
-        model_kwargs=dict(
-            decoder_distribution='gaussian_identity_variance',
-            input_channels=3,
-            imsize=224,
-            architecture=dict(
-                hidden_sizes=[200, 200],
-            ),
-            delta_features=True,
-            pretrained_features=False,
-        ),
-        trainer_kwargs=dict(
-            discount=0.99,
-            demo_path="/home/lerrel/ros_ws/src/railrl-private/demo_v4_processed.npy",
-            add_demo_latents=False, # already done
-            bc_num_pretrain_steps=1000,
-            rl_weight=0.0,
-            bc_weight=1.0,
-            weight_decay=0.01,
-        ),
-        replay_buffer_kwargs=dict(
-            max_size=100000,
-            fraction_goals_rollout_goals=1.0,
-            fraction_goals_env_goals=0.0,
-        ),
-        qf_kwargs=dict(
-            hidden_sizes=[400, 300],
-        ),
-        policy_kwargs=dict(
-            hidden_sizes=[400, 300],
-        ),
-
-        save_video=True,
-        dump_video_kwargs=dict(
-            save_period=1,
-            # imsize=(3, 500, 300),
-        ),
-        desired_trajectory="demo_v4_cream.npy",
-    )
-
-    search_space = {
-        'seedid': range(1),
-    }
-    sweeper = hyp.DeterministicHyperparameterSweeper(
-        search_space, default_parameters=variant,
-    )
-
-    variants = []
-    for variant in sweeper.iterate_hyperparameters():
-        variants.append(variant)
-
-    run_variants(encoder_wrapped_td3bc_experiment, variants, run_id=1)
diff --git a/experiments/ashvin/rfeatures/sawyer/door/rl1.py b/experiments/ashvin/rfeatures/sawyer/door/rl1.py
deleted file mode 100644
index 678c3c4..0000000
--- a/experiments/ashvin/rfeatures/sawyer/door/rl1.py
+++ /dev/null
@@ -1,197 +0,0 @@
-"""
-This should results in an average return of ~3000 by the end of training.
-
-Usually hits 3000 around epoch 80-100. Within a see, the performance will be
-a bit noisy from one epoch to the next (occasionally dips dow to ~2000).
-
-Note that one epoch = 5k steps, so 200 epochs = 1 million steps.
-"""
-import gym
-
-import railrl.torch.pytorch_util as ptu
-from railrl.data_management.obs_dict_replay_buffer import ObsDictRelabelingBuffer
-from railrl.exploration_strategies.base import \
-    PolicyWrappedWithExplorationStrategy
-from railrl.exploration_strategies.gaussian_and_epislon import \
-    GaussianAndEpislonStrategy
-from railrl.launchers.launcher_util import setup_logger
-from railrl.samplers.data_collector import GoalConditionedPathCollector
-from railrl.torch.her.her import HERTrainer
-from railrl.torch.networks import FlattenMlp, TanhMlpPolicy
-from railrl.torch.td3.td3 import TD3
-from railrl.torch.torch_rl_algorithm import TorchBatchRLAlgorithm
-# from multiworld.envs.mujoco.sawyer_xyz.sawyer_push_multiobj_subset import SawyerMultiobjectEnv
-# from multiworld.envs.mujoco.sawyer_xyz.sawyer_reach import SawyerReachXYZEnv
-
-from multiworld.core.image_env import ImageEnv
-from multiworld.envs.real_world.sawyer.sawyer_reaching import SawyerReachXYZEnv
-
-from railrl.launchers.launcher_util import run_experiment
-# import railrl.util.hyperparameter as hyp
-from railrl.launchers.experiments.ashvin.rfeatures.encoder_wrapped_env import EncoderWrappedEnv
-from railrl.misc.asset_loader import load_local_or_remote_file
-
-import torch
-
-from railrl.launchers.experiments.ashvin.rfeatures.rfeatures_model import TimestepPredictionModel
-from railrl.launchers.experiments.ashvin.rfeatures.rfeatures_trainer import TimePredictionTrainer
-
-def experiment(variant):
-    import ipdb; ipdb.set_trace()
-    
-    model_class = variant.get('model_class', TimestepPredictionModel)
-    model = model_class(
-        representation_size,
-        decoder_output_activation=decoder_activation,
-        output_classes=output_classes,
-        **variant['model_kwargs'],
-    )
-    # model = torch.nn.DataParallel(model)
-
-    model_path = "/home/lerrel/data/s3doodad/facebook/models/rfeatures/multitask1/run2/id2/itr_4000.pt"
-    # model = load_local_or_remote_file(model_path)
-    model.load_state_dict(torch.load(model_path))
-    model.to(ptu.device)
-
-    env = variant['env_class'](**variant['env_kwargs'])
-    env = ImageEnv(env,
-        recompute_reward=False,
-        transpose=True,
-        image_length=450000,
-        reward_type="image_distance",
-        # init_camera=sawyer_pusher_camera_upright_v2,
-    )
-    env = EncoderWrappedEnv(env, model)
-
-    expl_env = env # variant['env_class'](**variant['env_kwargs'])
-    eval_env = env # variant['env_class'](**variant['env_kwargs'])
-
-    observation_key = 'state_observation'
-    desired_goal_key = 'state_desired_goal'
-    achieved_goal_key = desired_goal_key.replace("desired", "achieved")
-    es = GaussianAndEpislonStrategy(
-        action_space=expl_env.action_space,
-        max_sigma=.2,
-        min_sigma=.2,  # constant sigma
-        epsilon=.3,
-    )
-    obs_dim = expl_env.observation_space.spaces['observation'].low.size
-    goal_dim = expl_env.observation_space.spaces['desired_goal'].low.size
-    action_dim = expl_env.action_space.low.size
-    qf1 = FlattenMlp(
-        input_size=obs_dim + goal_dim + action_dim,
-        output_size=1,
-        **variant['qf_kwargs']
-    )
-    qf2 = FlattenMlp(
-        input_size=obs_dim + goal_dim + action_dim,
-        output_size=1,
-        **variant['qf_kwargs']
-    )
-    target_qf1 = FlattenMlp(
-        input_size=obs_dim + goal_dim + action_dim,
-        output_size=1,
-        **variant['qf_kwargs']
-    )
-    target_qf2 = FlattenMlp(
-        input_size=obs_dim + goal_dim + action_dim,
-        output_size=1,
-        **variant['qf_kwargs']
-    )
-    policy = TanhMlpPolicy(
-        input_size=obs_dim + goal_dim,
-        output_size=action_dim,
-        **variant['policy_kwargs']
-    )
-    target_policy = TanhMlpPolicy(
-        input_size=obs_dim + goal_dim,
-        output_size=action_dim,
-        **variant['policy_kwargs']
-    )
-    expl_policy = PolicyWrappedWithExplorationStrategy(
-        exploration_strategy=es,
-        policy=policy,
-    )
-    replay_buffer = ObsDictRelabelingBuffer(
-        env=eval_env,
-        observation_key=observation_key,
-        desired_goal_key=desired_goal_key,
-        achieved_goal_key=achieved_goal_key,
-        **variant['replay_buffer_kwargs']
-    )
-    trainer = TD3(
-        policy=policy,
-        qf1=qf1,
-        qf2=qf2,
-        target_qf1=target_qf1,
-        target_qf2=target_qf2,
-        target_policy=target_policy,
-        **variant['trainer_kwargs']
-    )
-    trainer = HERTrainer(trainer)
-    eval_path_collector = GoalConditionedPathCollector(
-        eval_env,
-        policy,
-        observation_key=observation_key,
-        desired_goal_key=desired_goal_key,
-    )
-    expl_path_collector = GoalConditionedPathCollector(
-        expl_env,
-        expl_policy,
-        observation_key=observation_key,
-        desired_goal_key=desired_goal_key,
-    )
-    algorithm = TorchBatchRLAlgorithm(
-        trainer=trainer,
-        exploration_env=expl_env,
-        evaluation_env=eval_env,
-        exploration_data_collector=expl_path_collector,
-        evaluation_data_collector=eval_path_collector,
-        replay_buffer=replay_buffer,
-        **variant['algo_kwargs']
-    )
-    algorithm.to(ptu.device)
-    algorithm.train()
-
-
-if __name__ == "__main__":
-    x_var=0.2
-    x_low = -x_var
-    x_high = x_var
-    y_low = 0.5
-    y_high = 0.7
-    t = 0.05
-    variant = dict(
-        env_class=SawyerReachXYZEnv,
-        env_kwargs=dict(
-            action_mode="position", 
-            max_speed = 0.05, 
-            camera="sawyer_head"
-        ),
-        algo_kwargs=dict(
-            num_epochs=3000,
-            max_path_length=20,
-            batch_size=128,
-            num_eval_steps_per_epoch=1000,
-            num_expl_steps_per_train_loop=1000,
-            num_trains_per_train_loop=1000,
-            min_num_steps_before_training=1000,
-        ),
-        trainer_kwargs=dict(
-            discount=0.99,
-        ),
-        replay_buffer_kwargs=dict(
-            max_size=100000,
-            fraction_goals_rollout_goals=0.2,
-            fraction_goals_env_goals=0.0,
-        ),
-        qf_kwargs=dict(
-            hidden_sizes=[400, 300],
-        ),
-        policy_kwargs=dict(
-            hidden_sizes=[400, 300],
-        ),
-    )
-    import ipdb; ipdb.set_trace()
-    setup_logger('her-td3-pusher-0', variant=variant)
-    experiment(variant)
diff --git a/experiments/ashvin/rfeatures/sawyer/door/rl2.py b/experiments/ashvin/rfeatures/sawyer/door/rl2.py
deleted file mode 100644
index ff3cfc5..0000000
--- a/experiments/ashvin/rfeatures/sawyer/door/rl2.py
+++ /dev/null
@@ -1,259 +0,0 @@
-"""
-This should results in an average return of ~3000 by the end of training.
-
-Usually hits 3000 around epoch 80-100. Within a see, the performance will be
-a bit noisy from one epoch to the next (occasionally dips dow to ~2000).
-
-Note that one epoch = 5k steps, so 200 epochs = 1 million steps.
-"""
-import gym
-
-import railrl.torch.pytorch_util as ptu
-from railrl.data_management.obs_dict_replay_buffer import ObsDictRelabelingBuffer
-from railrl.exploration_strategies.base import \
-    PolicyWrappedWithExplorationStrategy
-from railrl.exploration_strategies.gaussian_and_epislon import \
-    GaussianAndEpislonStrategy
-from railrl.launchers.launcher_util import setup_logger
-from railrl.samplers.data_collector import GoalConditionedPathCollector
-from railrl.torch.her.her import HERTrainer
-from railrl.torch.networks import FlattenMlp, TanhMlpPolicy
-from railrl.torch.td3.td3 import TD3
-from railrl.torch.torch_rl_algorithm import TorchBatchRLAlgorithm
-# from multiworld.envs.mujoco.sawyer_xyz.sawyer_push_multiobj_subset import SawyerMultiobjectEnv
-# from multiworld.envs.mujoco.sawyer_xyz.sawyer_reach import SawyerReachXYZEnv
-
-from multiworld.core.image_env import ImageEnv
-from multiworld.envs.real_world.sawyer.sawyer_reaching import SawyerReachXYZEnv
-# from sawyer_control.envs.sawyer_reaching import SawyerReachXYZEnv
-
-from railrl.launchers.launcher_util import run_experiment
-# import railrl.util.hyperparameter as hyp
-from railrl.launchers.experiments.ashvin.rfeatures.encoder_wrapped_env import EncoderWrappedEnv
-from railrl.misc.asset_loader import load_local_or_remote_file
-
-import torch
-
-from railrl.launchers.experiments.ashvin.rfeatures.rfeatures_model import TimestepPredictionModel
-import numpy as np
-
-from railrl.torch.grill.video_gen import VideoSaveFunction
-
-from railrl.launchers.arglauncher import run_variants
-import railrl.misc.hyperparameter as hyp
-
-# from railrl.launchers.experiments.ashvin.rfeatures.rfeatures_trainer import TimePredictionTrainer
-
-def experiment(variant):
-    representation_size = 128
-    output_classes = 20
-
-    model_class = variant.get('model_class', TimestepPredictionModel)
-    model = model_class(
-        representation_size,
-        # decoder_output_activation=decoder_activation,
-        output_classes=output_classes,
-        **variant['model_kwargs'],
-    )
-    # model = torch.nn.DataParallel(model)
-
-    model_path = "/home/lerrel/data/s3doodad/facebook/models/rfeatures/multitask1/run2/id2/itr_4000.pt"
-    # model = load_local_or_remote_file(model_path)
-    state_dict = torch.load(model_path)
-    model.load_state_dict(state_dict)
-    model.to(ptu.device)
-
-    demos = np.load("demo_v2_1.npy", allow_pickle=True)
-    traj = demos[0]
-    goal_image = traj["observations"][-1]["image_observation"].reshape(1, 3, 500, 300)
-    goal_image = goal_image[:, ::-1, :, :].copy() # flip bgr
-    goal_latent = model.encoder(ptu.from_numpy(goal_image)).detach().cpu().numpy()
-    reward_params = dict(
-        goal_latent=goal_latent,
-    )
-
-    env = variant['env_class'](**variant['env_kwargs'])
-    env = ImageEnv(env,
-        recompute_reward=False,
-        transpose=True,
-        image_length=450000,
-        reward_type="image_distance",
-        # init_camera=sawyer_pusher_camera_upright_v2,
-    )
-    env = EncoderWrappedEnv(env, model, reward_params)
-
-    expl_env = env # variant['env_class'](**variant['env_kwargs'])
-    eval_env = env # variant['env_class'](**variant['env_kwargs'])
-
-    observation_key = 'latent_observation'
-    desired_goal_key = 'latent_observation'
-    achieved_goal_key = desired_goal_key.replace("desired", "achieved")
-    es = GaussianAndEpislonStrategy(
-        action_space=expl_env.action_space,
-        max_sigma=.2,
-        min_sigma=.2,  # constant sigma
-        epsilon=.3,
-    )
-    obs_dim = expl_env.observation_space.spaces['observation'].low.size
-    goal_dim = expl_env.observation_space.spaces['desired_goal'].low.size
-    action_dim = expl_env.action_space.low.size
-    qf1 = FlattenMlp(
-        input_size=obs_dim + goal_dim + action_dim,
-        output_size=1,
-        **variant['qf_kwargs']
-    )
-    qf2 = FlattenMlp(
-        input_size=obs_dim + goal_dim + action_dim,
-        output_size=1,
-        **variant['qf_kwargs']
-    )
-    target_qf1 = FlattenMlp(
-        input_size=obs_dim + goal_dim + action_dim,
-        output_size=1,
-        **variant['qf_kwargs']
-    )
-    target_qf2 = FlattenMlp(
-        input_size=obs_dim + goal_dim + action_dim,
-        output_size=1,
-        **variant['qf_kwargs']
-    )
-    policy = TanhMlpPolicy(
-        input_size=obs_dim + goal_dim,
-        output_size=action_dim,
-        **variant['policy_kwargs']
-    )
-    target_policy = TanhMlpPolicy(
-        input_size=obs_dim + goal_dim,
-        output_size=action_dim,
-        **variant['policy_kwargs']
-    )
-    expl_policy = PolicyWrappedWithExplorationStrategy(
-        exploration_strategy=es,
-        policy=policy,
-    )
-    replay_buffer = ObsDictRelabelingBuffer(
-        env=eval_env,
-        observation_key=observation_key,
-        desired_goal_key=desired_goal_key,
-        achieved_goal_key=achieved_goal_key,
-        **variant['replay_buffer_kwargs']
-    )
-    trainer = TD3(
-        policy=policy,
-        qf1=qf1,
-        qf2=qf2,
-        target_qf1=target_qf1,
-        target_qf2=target_qf2,
-        target_policy=target_policy,
-        **variant['trainer_kwargs']
-    )
-    trainer = HERTrainer(trainer)
-    eval_path_collector = GoalConditionedPathCollector(
-        eval_env,
-        policy,
-        observation_key=observation_key,
-        desired_goal_key=desired_goal_key,
-    )
-    expl_path_collector = GoalConditionedPathCollector(
-        expl_env,
-        expl_policy,
-        observation_key=observation_key,
-        desired_goal_key=desired_goal_key,
-    )
-    algorithm = TorchBatchRLAlgorithm(
-        trainer=trainer,
-        exploration_env=expl_env,
-        evaluation_env=eval_env,
-        exploration_data_collector=expl_path_collector,
-        evaluation_data_collector=eval_path_collector,
-        replay_buffer=replay_buffer,
-        **variant['algo_kwargs']
-    )
-
-    if variant.get("save_video", True):
-        video_func = VideoSaveFunction(
-            env,
-            **variant["dump_video_kwargs"],
-        )
-        algorithm.post_train_funcs.append(video_func)
-
-    algorithm.to(ptu.device)
-    algorithm.train()
-
-
-if __name__ == "__main__":
-    x_var=0.2
-    x_low = -x_var
-    x_high = x_var
-    y_low = 0.5
-    y_high = 0.7
-    t = 0.05
-    variant = dict(
-        env_class=SawyerReachXYZEnv,
-        env_kwargs=dict(
-            action_mode="position", 
-            max_speed = 0.05, 
-            camera="sawyer_head"
-        ),
-        # algo_kwargs=dict(
-        #     num_epochs=3000,
-        #     max_path_length=20,
-        #     batch_size=128,
-        #     num_eval_steps_per_epoch=1000,
-        #     num_expl_steps_per_train_loop=1000,
-        #     num_trains_per_train_loop=1000,
-        #     min_num_steps_before_training=1000,
-        # ),
-        algo_kwargs=dict(
-            num_epochs=3000,
-            max_path_length=10,
-            batch_size=5,
-            num_eval_steps_per_epoch=10,
-            num_expl_steps_per_train_loop=10,
-            num_trains_per_train_loop=10,
-            min_num_steps_before_training=10,
-        ),
-        model_kwargs=dict(
-            decoder_distribution='gaussian_identity_variance',
-            input_channels=3,
-            imsize=224,
-            architecture=dict(
-                hidden_sizes=[200, 200],
-            ),
-            delta_features=True,
-            pretrained_features=False,
-        ),
-        trainer_kwargs=dict(
-            discount=0.99,
-        ),
-        replay_buffer_kwargs=dict(
-            max_size=100000,
-            fraction_goals_rollout_goals=1.0,
-            fraction_goals_env_goals=0.0,
-        ),
-        qf_kwargs=dict(
-            hidden_sizes=[400, 300],
-        ),
-        policy_kwargs=dict(
-            hidden_sizes=[400, 300],
-        ),
-
-        save_video=True,
-        dump_video_kwargs=dict(
-            save_period=1,
-            # imsize=(3, 500, 300),
-        )
-    )
-
-    search_space = {
-        'seedid': range(1),
-    }
-    sweeper = hyp.DeterministicHyperparameterSweeper(
-        search_space, default_parameters=variant,
-    )
-
-    variants = []
-    for variant in sweeper.iterate_hyperparameters():
-        variants.append(variant)
-
-    run_variants(experiment, variants, run_id=0)
diff --git a/experiments/ashvin/rfeatures/sawyer/door/td3bc1.py b/experiments/ashvin/rfeatures/sawyer/door/td3bc1.py
deleted file mode 100644
index 6483403..0000000
--- a/experiments/ashvin/rfeatures/sawyer/door/td3bc1.py
+++ /dev/null
@@ -1,106 +0,0 @@
-import gym
-
-import railrl.torch.pytorch_util as ptu
-from railrl.data_management.obs_dict_replay_buffer import ObsDictRelabelingBuffer
-from railrl.exploration_strategies.base import \
-    PolicyWrappedWithExplorationStrategy
-from railrl.exploration_strategies.gaussian_and_epislon import \
-    GaussianAndEpislonStrategy
-from railrl.launchers.launcher_util import setup_logger
-from railrl.samplers.data_collector import GoalConditionedPathCollector
-from railrl.torch.her.her import HERTrainer
-from railrl.torch.networks import FlattenMlp, TanhMlpPolicy
-# from railrl.torch.td3.td3 import TD3
-from railrl.demos.td3_bc import TD3BCTrainer
-from railrl.torch.torch_rl_algorithm import TorchBatchRLAlgorithm
-# from multiworld.envs.mujoco.sawyer_xyz.sawyer_push_multiobj_subset import SawyerMultiobjectEnv
-# from multiworld.envs.mujoco.sawyer_xyz.sawyer_reach import SawyerReachXYZEnv
-
-from multiworld.core.image_env import ImageEnv
-from multiworld.envs.real_world.sawyer.sawyer_reaching import SawyerReachXYZEnv
-# from sawyer_control.envs.sawyer_reaching import SawyerReachXYZEnv
-
-from railrl.launchers.arglauncher import run_variants
-import railrl.misc.hyperparameter as hyp
-
-# from railrl.launchers.experiments.ashvin.rfeatures.rfeatures_trainer import TimePredictionTrainer
-from railrl.launchers.experiments.ashvin.rfeatures.rfeatures_rl import encoder_wrapped_td3bc_experiment
-
-if __name__ == "__main__":
-    x_var=0.2
-    x_low = -x_var
-    x_high = x_var
-    y_low = 0.5
-    y_high = 0.7
-    t = 0.05
-    variant = dict(
-        env_class=SawyerReachXYZEnv,
-        env_kwargs=dict(
-            action_mode="position", 
-            max_speed = 0.05, 
-            camera="sawyer_head"
-        ),
-        # algo_kwargs=dict(
-        #     num_epochs=3000,
-        #     max_path_length=20,
-        #     batch_size=128,
-        #     num_eval_steps_per_epoch=1000,
-        #     num_expl_steps_per_train_loop=1000,
-        #     num_trains_per_train_loop=1000,
-        #     min_num_steps_before_training=1000,
-        # ),
-        algo_kwargs=dict(
-            num_epochs=3000,
-            max_path_length=10,
-            batch_size=5,
-            num_eval_steps_per_epoch=10,
-            num_expl_steps_per_train_loop=10,
-            num_trains_per_train_loop=10,
-            min_num_steps_before_training=10,
-        ),
-        model_kwargs=dict(
-            decoder_distribution='gaussian_identity_variance',
-            input_channels=3,
-            imsize=224,
-            architecture=dict(
-                hidden_sizes=[200, 200],
-            ),
-            delta_features=True,
-            pretrained_features=False,
-        ),
-        trainer_kwargs=dict(
-            discount=0.99,
-            demo_path="/home/lerrel/ros_ws/src/railrl-private/demo_v2_2.npy",
-            add_demo_latents=True,
-        ),
-        replay_buffer_kwargs=dict(
-            max_size=100000,
-            fraction_goals_rollout_goals=1.0,
-            fraction_goals_env_goals=0.0,
-        ),
-        qf_kwargs=dict(
-            hidden_sizes=[400, 300],
-        ),
-        policy_kwargs=dict(
-            hidden_sizes=[400, 300],
-        ),
-
-        save_video=True,
-        dump_video_kwargs=dict(
-            save_period=1,
-            # imsize=(3, 500, 300),
-        )
-    )
-
-    search_space = {
-        'seedid': range(1),
-    }
-    sweeper = hyp.DeterministicHyperparameterSweeper(
-        search_space, default_parameters=variant,
-    )
-
-    variants = []
-    for variant in sweeper.iterate_hyperparameters():
-        variants.append(variant)
-
-    run_variants(encoder_wrapped_td3bc_experiment, variants, run_id=0)
diff --git a/experiments/ashvin/rfeatures/sawyer/door/td3bc2.py b/experiments/ashvin/rfeatures/sawyer/door/td3bc2.py
deleted file mode 100644
index 12f4fc5..0000000
--- a/experiments/ashvin/rfeatures/sawyer/door/td3bc2.py
+++ /dev/null
@@ -1,104 +0,0 @@
-import gym
-
-import railrl.torch.pytorch_util as ptu
-from railrl.data_management.obs_dict_replay_buffer import ObsDictRelabelingBuffer
-from railrl.exploration_strategies.base import \
-    PolicyWrappedWithExplorationStrategy
-from railrl.exploration_strategies.gaussian_and_epislon import \
-    GaussianAndEpislonStrategy
-from railrl.launchers.launcher_util import setup_logger
-from railrl.samplers.data_collector import GoalConditionedPathCollector
-from railrl.torch.her.her import HERTrainer
-from railrl.torch.networks import FlattenMlp, TanhMlpPolicy
-# from railrl.torch.td3.td3 import TD3
-from railrl.demos.td3_bc import TD3BCTrainer
-from railrl.torch.torch_rl_algorithm import TorchBatchRLAlgorithm
-# from multiworld.envs.mujoco.sawyer_xyz.sawyer_push_multiobj_subset import SawyerMultiobjectEnv
-# from multiworld.envs.mujoco.sawyer_xyz.sawyer_reach import SawyerReachXYZEnv
-
-from multiworld.core.image_env import ImageEnv
-from multiworld.envs.real_world.sawyer.sawyer_reaching import SawyerReachXYZEnv
-# from sawyer_control.envs.sawyer_reaching import SawyerReachXYZEnv
-
-from railrl.launchers.arglauncher import run_variants
-import railrl.misc.hyperparameter as hyp
-
-# from railrl.launchers.experiments.ashvin.rfeatures.rfeatures_trainer import TimePredictionTrainer
-from railrl.launchers.experiments.ashvin.rfeatures.rfeatures_rl import encoder_wrapped_td3bc_experiment
-
-if __name__ == "__main__":
-    variant = dict(
-        env_class=SawyerReachXYZEnv,
-        env_kwargs=dict(
-            action_mode="position", 
-            max_speed = 0.05, 
-            camera="sawyer_head"
-        ),
-        # algo_kwargs=dict(
-        #     num_epochs=3000,
-        #     max_path_length=20,
-        #     batch_size=128,
-        #     num_eval_steps_per_epoch=1000,
-        #     num_expl_steps_per_train_loop=1000,
-        #     num_trains_per_train_loop=1000,
-        #     min_num_steps_before_training=1000,
-        # ),
-        algo_kwargs=dict(
-            num_epochs=500,
-            max_path_length=100,
-            batch_size=128,
-            num_eval_steps_per_epoch=500,
-            num_expl_steps_per_train_loop=500,
-            num_trains_per_train_loop=500,
-            min_num_steps_before_training=500,
-        ),
-        model_kwargs=dict(
-            decoder_distribution='gaussian_identity_variance',
-            input_channels=3,
-            imsize=224,
-            architecture=dict(
-                hidden_sizes=[200, 200],
-            ),
-            delta_features=True,
-            pretrained_features=False,
-        ),
-        trainer_kwargs=dict(
-            discount=0.99,
-            demo_path="/home/lerrel/ros_ws/src/railrl-private/demo_v3_processed.npy",
-            add_demo_latents=False, # already done
-            bc_num_pretrain_steps=1000,
-            rl_weight=0.1,
-            bc_weight=1.0,
-            weight_decay=0.01,
-        ),
-        replay_buffer_kwargs=dict(
-            max_size=100000,
-            fraction_goals_rollout_goals=1.0,
-            fraction_goals_env_goals=0.0,
-        ),
-        qf_kwargs=dict(
-            hidden_sizes=[400, 300],
-        ),
-        policy_kwargs=dict(
-            hidden_sizes=[400, 300],
-        ),
-
-        save_video=True,
-        dump_video_kwargs=dict(
-            save_period=1,
-            # imsize=(3, 500, 300),
-        )
-    )
-
-    search_space = {
-        'seedid': range(1),
-    }
-    sweeper = hyp.DeterministicHyperparameterSweeper(
-        search_space, default_parameters=variant,
-    )
-
-    variants = []
-    for variant in sweeper.iterate_hyperparameters():
-        variants.append(variant)
-
-    run_variants(encoder_wrapped_td3bc_experiment, variants, run_id=4)
diff --git a/experiments/ashvin/rfeatures/sawyer/door/td3bc3.py b/experiments/ashvin/rfeatures/sawyer/door/td3bc3.py
deleted file mode 100644
index db626bf..0000000
--- a/experiments/ashvin/rfeatures/sawyer/door/td3bc3.py
+++ /dev/null
@@ -1,106 +0,0 @@
-import gym
-
-import railrl.torch.pytorch_util as ptu
-from railrl.data_management.obs_dict_replay_buffer import ObsDictRelabelingBuffer
-from railrl.exploration_strategies.base import \
-    PolicyWrappedWithExplorationStrategy
-from railrl.exploration_strategies.gaussian_and_epislon import \
-    GaussianAndEpislonStrategy
-from railrl.launchers.launcher_util import setup_logger
-from railrl.samplers.data_collector import GoalConditionedPathCollector
-from railrl.torch.her.her import HERTrainer
-from railrl.torch.networks import FlattenMlp, TanhMlpPolicy
-# from railrl.torch.td3.td3 import TD3
-from railrl.demos.td3_bc import TD3BCTrainer
-from railrl.torch.torch_rl_algorithm import TorchBatchRLAlgorithm
-# from multiworld.envs.mujoco.sawyer_xyz.sawyer_push_multiobj_subset import SawyerMultiobjectEnv
-# from multiworld.envs.mujoco.sawyer_xyz.sawyer_reach import SawyerReachXYZEnv
-
-from multiworld.core.image_env import ImageEnv
-from multiworld.envs.real_world.sawyer.sawyer_reaching import SawyerReachXYZEnv
-# from sawyer_control.envs.sawyer_reaching import SawyerReachXYZEnv
-
-from railrl.launchers.arglauncher import run_variants
-import railrl.misc.hyperparameter as hyp
-
-# from railrl.launchers.experiments.ashvin.rfeatures.rfeatures_trainer import TimePredictionTrainer
-from railrl.launchers.experiments.ashvin.rfeatures.rfeatures_rl import encoder_wrapped_td3bc_experiment
-
-if __name__ == "__main__":
-    variant = dict(
-        env_class=SawyerReachXYZEnv,
-        env_kwargs=dict(
-            action_mode="position", 
-            max_speed = 0.05, 
-            camera="sawyer_head"
-        ),
-        # algo_kwargs=dict(
-        #     num_epochs=3000,
-        #     max_path_length=20,
-        #     batch_size=128,
-        #     num_eval_steps_per_epoch=1000,
-        #     num_expl_steps_per_train_loop=1000,
-        #     num_trains_per_train_loop=1000,
-        #     min_num_steps_before_training=1000,
-        # ),
-        algo_kwargs=dict(
-            num_epochs=500,
-            max_path_length=100,
-            batch_size=128,
-            num_eval_steps_per_epoch=500,
-            num_expl_steps_per_train_loop=500,
-            num_trains_per_train_loop=500,
-            min_num_steps_before_training=500,
-        ),
-        model_kwargs=dict(
-            decoder_distribution='gaussian_identity_variance',
-            input_channels=3,
-            imsize=224,
-            architecture=dict(
-                hidden_sizes=[200, 200],
-            ),
-            delta_features=True,
-            pretrained_features=False,
-        ),
-        trainer_kwargs=dict(
-            discount=0.99,
-            demo_path="/home/lerrel/ros_ws/src/railrl-private/demo_v4_processed.npy",
-            add_demo_latents=False, # already done
-            bc_num_pretrain_steps=1000,
-            rl_weight=0.1,
-            bc_weight=1.0,
-            weight_decay=0.01,
-        ),
-        replay_buffer_kwargs=dict(
-            max_size=100000,
-            fraction_goals_rollout_goals=1.0,
-            fraction_goals_env_goals=0.0,
-        ),
-        qf_kwargs=dict(
-            hidden_sizes=[400, 300],
-        ),
-        policy_kwargs=dict(
-            hidden_sizes=[400, 300],
-        ),
-
-        save_video=True,
-        dump_video_kwargs=dict(
-            save_period=1,
-            # imsize=(3, 500, 300),
-        ),
-        desired_trajectory="demo_v4.npy",
-        snapshot_mode="all",
-    )
-
-    search_space = {
-        'seedid': range(1),
-    }
-    sweeper = hyp.DeterministicHyperparameterSweeper(
-        search_space, default_parameters=variant,
-    )
-
-    variants = []
-    for variant in sweeper.iterate_hyperparameters():
-        variants.append(variant)
-
-    run_variants(encoder_wrapped_td3bc_experiment, variants, run_id=2)
diff --git a/experiments/ashvin/rfeatures/sawyer/door/td3bc4.py b/experiments/ashvin/rfeatures/sawyer/door/td3bc4.py
deleted file mode 100644
index ef3f262..0000000
--- a/experiments/ashvin/rfeatures/sawyer/door/td3bc4.py
+++ /dev/null
@@ -1,106 +0,0 @@
-import gym
-
-import railrl.torch.pytorch_util as ptu
-from railrl.data_management.obs_dict_replay_buffer import ObsDictRelabelingBuffer
-from railrl.exploration_strategies.base import \
-    PolicyWrappedWithExplorationStrategy
-from railrl.exploration_strategies.gaussian_and_epislon import \
-    GaussianAndEpislonStrategy
-from railrl.launchers.launcher_util import setup_logger
-from railrl.samplers.data_collector import GoalConditionedPathCollector
-from railrl.torch.her.her import HERTrainer
-from railrl.torch.networks import FlattenMlp, TanhMlpPolicy
-# from railrl.torch.td3.td3 import TD3
-from railrl.demos.td3_bc import TD3BCTrainer
-from railrl.torch.torch_rl_algorithm import TorchBatchRLAlgorithm
-# from multiworld.envs.mujoco.sawyer_xyz.sawyer_push_multiobj_subset import SawyerMultiobjectEnv
-# from multiworld.envs.mujoco.sawyer_xyz.sawyer_reach import SawyerReachXYZEnv
-
-from multiworld.core.image_env import ImageEnv
-from multiworld.envs.real_world.sawyer.sawyer_reaching import SawyerReachXYZEnv
-# from sawyer_control.envs.sawyer_reaching import SawyerReachXYZEnv
-
-from railrl.launchers.arglauncher import run_variants
-import railrl.misc.hyperparameter as hyp
-
-# from railrl.launchers.experiments.ashvin.rfeatures.rfeatures_trainer import TimePredictionTrainer
-from railrl.launchers.experiments.ashvin.rfeatures.rfeatures_rl import encoder_wrapped_td3bc_experiment
-
-if __name__ == "__main__":
-    variant = dict(
-        env_class=SawyerReachXYZEnv,
-        env_kwargs=dict(
-            action_mode="position", 
-            max_speed = 0.05, 
-            camera="sawyer_head"
-        ),
-        # algo_kwargs=dict(
-        #     num_epochs=3000,
-        #     max_path_length=20,
-        #     batch_size=128,
-        #     num_eval_steps_per_epoch=1000,
-        #     num_expl_steps_per_train_loop=1000,
-        #     num_trains_per_train_loop=1000,
-        #     min_num_steps_before_training=1000,
-        # ),
-        algo_kwargs=dict(
-            num_epochs=500,
-            max_path_length=100,
-            batch_size=128,
-            num_eval_steps_per_epoch=500,
-            num_expl_steps_per_train_loop=500,
-            num_trains_per_train_loop=500,
-            min_num_steps_before_training=500,
-        ),
-        model_kwargs=dict(
-            decoder_distribution='gaussian_identity_variance',
-            input_channels=3,
-            imsize=224,
-            architecture=dict(
-                hidden_sizes=[200, 200],
-            ),
-            delta_features=True,
-            pretrained_features=False,
-        ),
-        trainer_kwargs=dict(
-            discount=0.99,
-            demo_path="/home/lerrel/ros_ws/src/railrl-private/demo_v4_processed.npy",
-            add_demo_latents=False, # already done
-            bc_num_pretrain_steps=1000,
-            rl_weight=0.1,
-            bc_weight=1.0,
-            weight_decay=0.01,
-        ),
-        replay_buffer_kwargs=dict(
-            max_size=100000,
-            fraction_goals_rollout_goals=1.0,
-            fraction_goals_env_goals=0.0,
-        ),
-        qf_kwargs=dict(
-            hidden_sizes=[400, 300],
-        ),
-        policy_kwargs=dict(
-            hidden_sizes=[400, 300],
-        ),
-
-        save_video=True,
-        dump_video_kwargs=dict(
-            save_period=1,
-            # imsize=(3, 500, 300),
-        ),
-        desired_trajectory="demo_v4_cream.npy",
-        snapshot_mode="all",
-    )
-
-    search_space = {
-        'seedid': range(1),
-    }
-    sweeper = hyp.DeterministicHyperparameterSweeper(
-        search_space, default_parameters=variant,
-    )
-
-    variants = []
-    for variant in sweeper.iterate_hyperparameters():
-        variants.append(variant)
-
-    run_variants(encoder_wrapped_td3bc_experiment, variants, run_id=1)
diff --git a/experiments/ashvin/rfeatures/sawyer/door/td3bc5.py b/experiments/ashvin/rfeatures/sawyer/door/td3bc5.py
deleted file mode 100644
index c67d4e2..0000000
--- a/experiments/ashvin/rfeatures/sawyer/door/td3bc5.py
+++ /dev/null
@@ -1,110 +0,0 @@
-import gym
-
-import railrl.torch.pytorch_util as ptu
-from railrl.data_management.obs_dict_replay_buffer import ObsDictRelabelingBuffer
-from railrl.exploration_strategies.base import \
-    PolicyWrappedWithExplorationStrategy
-from railrl.exploration_strategies.gaussian_and_epislon import \
-    GaussianAndEpislonStrategy
-from railrl.launchers.launcher_util import setup_logger
-from railrl.samplers.data_collector import GoalConditionedPathCollector
-from railrl.torch.her.her import HERTrainer
-from railrl.torch.networks import FlattenMlp, TanhMlpPolicy
-# from railrl.torch.td3.td3 import TD3
-from railrl.demos.td3_bc import TD3BCTrainer
-from railrl.torch.torch_rl_algorithm import TorchBatchRLAlgorithm
-# from multiworld.envs.mujoco.sawyer_xyz.sawyer_push_multiobj_subset import SawyerMultiobjectEnv
-# from multiworld.envs.mujoco.sawyer_xyz.sawyer_reach import SawyerReachXYZEnv
-
-from multiworld.core.image_env import ImageEnv
-from multiworld.envs.real_world.sawyer.sawyer_reaching import SawyerReachXYZEnv
-# from sawyer_control.envs.sawyer_reaching import SawyerReachXYZEnv
-
-from railrl.launchers.arglauncher import run_variants
-import railrl.misc.hyperparameter as hyp
-
-# from railrl.launchers.experiments.ashvin.rfeatures.rfeatures_trainer import TimePredictionTrainer
-from railrl.launchers.experiments.ashvin.rfeatures.rfeatures_rl import encoder_wrapped_td3bc_experiment
-
-if __name__ == "__main__":
-    variant = dict(
-        env_class=SawyerReachXYZEnv,
-        env_kwargs=dict(
-            action_mode="position", 
-            max_speed = 0.05, 
-            camera="sawyer_head"
-        ),
-        # algo_kwargs=dict(
-        #     num_epochs=3000,
-        #     max_path_length=20,
-        #     batch_size=128,
-        #     num_eval_steps_per_epoch=1000,
-        #     num_expl_steps_per_train_loop=1000,
-        #     num_trains_per_train_loop=1000,
-        #     min_num_steps_before_training=1000,
-        # ),
-        algo_kwargs=dict(
-            num_epochs=500,
-            max_path_length=100,
-            batch_size=128,
-            num_eval_steps_per_epoch=500,
-            num_expl_steps_per_train_loop=500,
-            num_trains_per_train_loop=500,
-            min_num_steps_before_training=500,
-        ),
-        model_kwargs=dict(
-            decoder_distribution='gaussian_identity_variance',
-            input_channels=3,
-            imsize=224,
-            architecture=dict(
-                hidden_sizes=[200, 200],
-            ),
-            delta_features=True,
-            pretrained_features=False,
-        ),
-        trainer_kwargs=dict(
-            discount=0.99,
-            demo_path="/home/lerrel/ros_ws/src/railrl-private/demo_v4_processed.npy",
-            add_demo_latents=False, # already done
-            bc_num_pretrain_steps=1000,
-            rl_weight=0.1,
-            bc_weight=1.0,
-            weight_decay=0.001,
-        ),
-        replay_buffer_kwargs=dict(
-            max_size=100000,
-            fraction_goals_rollout_goals=1.0,
-            fraction_goals_env_goals=0.0,
-        ),
-        qf_kwargs=dict(
-            hidden_sizes=[400, 300],
-        ),
-        policy_kwargs=dict(
-            hidden_sizes=[32, 32],
-        ),
-
-        save_video=True,
-        dump_video_kwargs=dict(
-            save_period=1,
-            # imsize=(3, 500, 300),
-        ),
-        desired_trajectory="demo_v4.npy",
-        snapshot_mode="all",
-        
-        logger_variant=dict(
-            tensorboard=True,
-        ),
-    )
-
-    search_space = {
-        'seedid': range(1),
-    }
-    sweeper = hyp.DeterministicHyperparameterSweeper(
-        search_space, default_parameters=variant,
-    )
-
-    variants = []
-    for variant in sweeper.iterate_hyperparameters():
-        variants.append(variant)
-
-    run_variants(encoder_wrapped_td3bc_experiment, variants, run_id=1)
diff --git a/experiments/ashvin/rfeatures/sawyer/door2/bc1.py b/experiments/ashvin/rfeatures/sawyer/door2/bc1.py
deleted file mode 100644
index 46a19ba..0000000
--- a/experiments/ashvin/rfeatures/sawyer/door2/bc1.py
+++ /dev/null
@@ -1,112 +0,0 @@
-import gym
-
-import railrl.torch.pytorch_util as ptu
-from railrl.data_management.obs_dict_replay_buffer import ObsDictRelabelingBuffer
-from railrl.exploration_strategies.base import \
-    PolicyWrappedWithExplorationStrategy
-from railrl.exploration_strategies.gaussian_and_epislon import \
-    GaussianAndEpislonStrategy
-from railrl.launchers.launcher_util import setup_logger
-from railrl.samplers.data_collector import GoalConditionedPathCollector
-from railrl.torch.her.her import HERTrainer
-from railrl.torch.networks import FlattenMlp, TanhMlpPolicy
-# from railrl.torch.td3.td3 import TD3
-from railrl.demos.td3_bc import TD3BCTrainer
-from railrl.torch.torch_rl_algorithm import TorchBatchRLAlgorithm
-# from multiworld.envs.mujoco.sawyer_xyz.sawyer_push_multiobj_subset import SawyerMultiobjectEnv
-# from multiworld.envs.mujoco.sawyer_xyz.sawyer_reach import SawyerReachXYZEnv
-
-from multiworld.core.image_env import ImageEnv
-from multiworld.envs.real_world.sawyer.sawyer_reaching import SawyerReachXYZEnv
-# from sawyer_control.envs.sawyer_reaching import SawyerReachXYZEnv
-
-from railrl.launchers.arglauncher import run_variants
-import railrl.misc.hyperparameter as hyp
-
-# from railrl.launchers.experiments.ashvin.rfeatures.rfeatures_trainer import TimePredictionTrainer
-from railrl.launchers.experiments.ashvin.rfeatures.rfeatures_rl import encoder_wrapped_td3bc_experiment
-
-if __name__ == "__main__":
-    variant = dict(
-        env_class=SawyerReachXYZEnv,
-        env_kwargs=dict(
-            action_mode="position", 
-            max_speed = 0.05, 
-            camera="sawyer_head"
-        ),
-        # algo_kwargs=dict(
-        #     num_epochs=3000,
-        #     max_path_length=20,
-        #     batch_size=128,
-        #     num_eval_steps_per_epoch=1000,
-        #     num_expl_steps_per_train_loop=1000,
-        #     num_trains_per_train_loop=1000,
-        #     min_num_steps_before_training=1000,
-        # ),
-        algo_kwargs=dict(
-            num_epochs=500,
-            max_path_length=100,
-            batch_size=128,
-            num_eval_steps_per_epoch=500,
-            num_expl_steps_per_train_loop=500,
-            num_trains_per_train_loop=500,
-            min_num_steps_before_training=0,
-        ),
-        model_kwargs=dict(
-            decoder_distribution='gaussian_identity_variance',
-            input_channels=3,
-            imsize=224,
-            architecture=dict(
-                hidden_sizes=[200, 200],
-            ),
-            delta_features=True,
-            pretrained_features=False,
-        ),
-        trainer_kwargs=dict(
-            discount=0.99,
-            demo_path="/home/anair/ros_ws/src/railrl-private/demos/door_demos_10_2/processed_demos.pkl",
-            # demo_path="/home/anair/ros_ws/src/railrl-private/demos/door_demos_10_2/processed_demos_imagenet.pkl",
-            add_demo_latents=False, # already done
-            bc_num_pretrain_steps=1000,
-            rl_weight=0.0,
-            bc_weight=1.0,
-            weight_decay=0.01,
-        ),
-        replay_buffer_kwargs=dict(
-            max_size=100000,
-            fraction_goals_rollout_goals=1.0,
-            fraction_goals_env_goals=0.0,
-        ),
-        qf_kwargs=dict(
-            hidden_sizes=[400, 300],
-        ),
-        policy_kwargs=dict(
-            hidden_sizes=[32, 32],
-        ),
-
-        save_video=True,
-        dump_video_kwargs=dict(
-            save_period=1,
-            # imsize=(3, 500, 300),
-        ),
-        desired_trajectory="/home/anair/ros_ws/src/railrl-private/demos/door_demos_10_2/demo_0.pkl",
-
-        logger_variant=dict(
-            tensorboard=True,
-        ),
-        model_path="/home/anair/data/s3doodad/facebook/models/rfeatures/multitask1/run2/id2/itr_4000.pt",
-        # model_path ="/home/anair/data/s3doodad/facebook/models/rfeatures/multitask1/run2/id0/itr_0.pt" # imagenet
-    )
-
-    search_space = {
-        'seedid': range(1),
-    }
-    sweeper = hyp.DeterministicHyperparameterSweeper(
-        search_space, default_parameters=variant,
-    )
-
-    variants = []
-    for variant in sweeper.iterate_hyperparameters():
-        variants.append(variant)
-
-    run_variants(encoder_wrapped_td3bc_experiment, variants, run_id=14)
diff --git a/experiments/ashvin/rfeatures/sawyer/door2/bc1_imagenet.py b/experiments/ashvin/rfeatures/sawyer/door2/bc1_imagenet.py
deleted file mode 100644
index 0faf9ad..0000000
--- a/experiments/ashvin/rfeatures/sawyer/door2/bc1_imagenet.py
+++ /dev/null
@@ -1,112 +0,0 @@
-import gym
-
-import railrl.torch.pytorch_util as ptu
-from railrl.data_management.obs_dict_replay_buffer import ObsDictRelabelingBuffer
-from railrl.exploration_strategies.base import \
-    PolicyWrappedWithExplorationStrategy
-from railrl.exploration_strategies.gaussian_and_epislon import \
-    GaussianAndEpislonStrategy
-from railrl.launchers.launcher_util import setup_logger
-from railrl.samplers.data_collector import GoalConditionedPathCollector
-from railrl.torch.her.her import HERTrainer
-from railrl.torch.networks import FlattenMlp, TanhMlpPolicy
-# from railrl.torch.td3.td3 import TD3
-from railrl.demos.td3_bc import TD3BCTrainer
-from railrl.torch.torch_rl_algorithm import TorchBatchRLAlgorithm
-# from multiworld.envs.mujoco.sawyer_xyz.sawyer_push_multiobj_subset import SawyerMultiobjectEnv
-# from multiworld.envs.mujoco.sawyer_xyz.sawyer_reach import SawyerReachXYZEnv
-
-from multiworld.core.image_env import ImageEnv
-from multiworld.envs.real_world.sawyer.sawyer_reaching import SawyerReachXYZEnv
-# from sawyer_control.envs.sawyer_reaching import SawyerReachXYZEnv
-
-from railrl.launchers.arglauncher import run_variants
-import railrl.misc.hyperparameter as hyp
-
-# from railrl.launchers.experiments.ashvin.rfeatures.rfeatures_trainer import TimePredictionTrainer
-from railrl.launchers.experiments.ashvin.rfeatures.rfeatures_rl import encoder_wrapped_td3bc_experiment
-
-if __name__ == "__main__":
-    variant = dict(
-        env_class=SawyerReachXYZEnv,
-        env_kwargs=dict(
-            action_mode="position", 
-            max_speed = 0.05, 
-            camera="sawyer_head"
-        ),
-        # algo_kwargs=dict(
-        #     num_epochs=3000,
-        #     max_path_length=20,
-        #     batch_size=128,
-        #     num_eval_steps_per_epoch=1000,
-        #     num_expl_steps_per_train_loop=1000,
-        #     num_trains_per_train_loop=1000,
-        #     min_num_steps_before_training=1000,
-        # ),
-        algo_kwargs=dict(
-            num_epochs=500,
-            max_path_length=100,
-            batch_size=128,
-            num_eval_steps_per_epoch=500,
-            num_expl_steps_per_train_loop=500,
-            num_trains_per_train_loop=500,
-            min_num_steps_before_training=0,
-        ),
-        model_kwargs=dict(
-            decoder_distribution='gaussian_identity_variance',
-            input_channels=3,
-            imsize=224,
-            architecture=dict(
-                hidden_sizes=[200, 200],
-            ),
-            delta_features=True,
-            pretrained_features=False,
-        ),
-        trainer_kwargs=dict(
-            discount=0.99,
-            # demo_path="/home/anair/ros_ws/src/railrl-private/demos/door_demos_10_2/processed_demos.pkl",
-            demo_path="/home/anair/ros_ws/src/railrl-private/demos/door_demos_10_2/processed_demos_imagenet.pkl",
-            add_demo_latents=False, # already done
-            bc_num_pretrain_steps=1000,
-            rl_weight=0.0,
-            bc_weight=1.0,
-            weight_decay=0.01,
-        ),
-        replay_buffer_kwargs=dict(
-            max_size=100000,
-            fraction_goals_rollout_goals=1.0,
-            fraction_goals_env_goals=0.0,
-        ),
-        qf_kwargs=dict(
-            hidden_sizes=[400, 300],
-        ),
-        policy_kwargs=dict(
-            hidden_sizes=[32, 32],
-        ),
-
-        save_video=True,
-        dump_video_kwargs=dict(
-            save_period=1,
-            # imsize=(3, 500, 300),
-        ),
-        desired_trajectory="/home/anair/ros_ws/src/railrl-private/demos/door_demos_10_2/demo_0.pkl",
-
-        logger_variant=dict(
-            tensorboard=True,
-        ),
-        # model_path="/home/anair/data/s3doodad/facebook/models/rfeatures/multitask1/run2/id2/itr_4000.pt",
-        model_path ="/home/anair/data/s3doodad/facebook/models/rfeatures/multitask1/run2/id0/itr_0.pt" # imagenet
-    )
-
-    search_space = {
-        'seedid': range(1),
-    }
-    sweeper = hyp.DeterministicHyperparameterSweeper(
-        search_space, default_parameters=variant,
-    )
-
-    variants = []
-    for variant in sweeper.iterate_hyperparameters():
-        variants.append(variant)
-
-    run_variants(encoder_wrapped_td3bc_experiment, variants, run_id=14)
diff --git a/experiments/ashvin/rfeatures/sawyer/door2/bc1_imagenet_new.py b/experiments/ashvin/rfeatures/sawyer/door2/bc1_imagenet_new.py
deleted file mode 100644
index 3e81837..0000000
--- a/experiments/ashvin/rfeatures/sawyer/door2/bc1_imagenet_new.py
+++ /dev/null
@@ -1,110 +0,0 @@
-import gym
-
-import railrl.torch.pytorch_util as ptu
-from railrl.data_management.obs_dict_replay_buffer import ObsDictRelabelingBuffer
-from railrl.exploration_strategies.base import \
-    PolicyWrappedWithExplorationStrategy
-from railrl.exploration_strategies.gaussian_and_epislon import \
-    GaussianAndEpislonStrategy
-from railrl.launchers.launcher_util import setup_logger
-from railrl.samplers.data_collector import GoalConditionedPathCollector
-from railrl.torch.her.her import HERTrainer
-from railrl.torch.networks import FlattenMlp, TanhMlpPolicy
-# from railrl.torch.td3.td3 import TD3
-from railrl.demos.td3_bc import TD3BCTrainer
-from railrl.torch.torch_rl_algorithm import TorchBatchRLAlgorithm
-# from multiworld.envs.mujoco.sawyer_xyz.sawyer_push_multiobj_subset import SawyerMultiobjectEnv
-# from multiworld.envs.mujoco.sawyer_xyz.sawyer_reach import SawyerReachXYZEnv
-
-from multiworld.core.image_env import ImageEnv
-from multiworld.envs.real_world.sawyer.sawyer_reaching import SawyerReachXYZEnv
-# from sawyer_control.envs.sawyer_reaching import SawyerReachXYZEnv
-
-from railrl.launchers.arglauncher import run_variants
-import railrl.misc.hyperparameter as hyp
-
-# from railrl.launchers.experiments.ashvin.rfeatures.rfeatures_trainer import TimePredictionTrainer
-from railrl.launchers.experiments.ashvin.rfeatures.rfeatures_rl import encoder_wrapped_td3bc_experiment
-
-if __name__ == "__main__":
-    variant = dict(
-        env_class=SawyerReachXYZEnv,
-        env_kwargs=dict(
-            action_mode="position", 
-            max_speed = 0.05, 
-            camera="sawyer_head"
-        ),
-        # algo_kwargs=dict(
-        #     num_epochs=3000,
-        #     max_path_length=20,
-        #     batch_size=128,
-        #     num_eval_steps_per_epoch=1000,
-        #     num_expl_steps_per_train_loop=1000,
-        #     num_trains_per_train_loop=1000,
-        #     min_num_steps_before_training=1000,
-        # ),
-        algo_kwargs=dict(
-            num_epochs=500,
-            max_path_length=100,
-            batch_size=128,
-            num_eval_steps_per_epoch=500,
-            num_expl_steps_per_train_loop=500,
-            num_trains_per_train_loop=500,
-            min_num_steps_before_training=0,
-        ),
-        model_kwargs=dict(
-            decoder_distribution='gaussian_identity_variance',
-            input_channels=3,
-            imsize=224,
-            architecture=dict(
-                hidden_sizes=[200, 200],
-            ),
-            delta_features=True,
-            pretrained_features=False,
-        ),
-        trainer_kwargs=dict(
-            discount=0.99,
-            demo_path="/home/anair/ros_ws/src/railrl-private/demos/door_demos_new_setup/processed_demos_imagenet.pkl",
-            add_demo_latents=False, # already done
-            bc_num_pretrain_steps=1000,
-            rl_weight=0.0,
-            bc_weight=1.0,
-            weight_decay=0.01,
-        ),
-        replay_buffer_kwargs=dict(
-            max_size=100000,
-            fraction_goals_rollout_goals=1.0,
-            fraction_goals_env_goals=0.0,
-        ),
-        qf_kwargs=dict(
-            hidden_sizes=[400, 300],
-        ),
-        policy_kwargs=dict(
-            hidden_sizes=[32, 32],
-        ),
-
-        save_video=True,
-        dump_video_kwargs=dict(
-            save_period=1,
-            # imsize=(3, 500, 300),
-        ),
-        desired_trajectory="/home/anair/ros_ws/src/railrl-private/demos/door_demos_new_setup/demo_0.pkl",
-
-        logger_variant=dict(
-            tensorboard=True,
-        ),
-        model_path ="/home/anair/data/s3doodad/facebook/models/rfeatures/multitask1/run2/id0/itr_0.pt" # imagenet
-    )
-
-    search_space = {
-        'seedid': range(1),
-    }
-    sweeper = hyp.DeterministicHyperparameterSweeper(
-        search_space, default_parameters=variant,
-    )
-
-    variants = []
-    for variant in sweeper.iterate_hyperparameters():
-        variants.append(variant)
-
-    run_variants(encoder_wrapped_td3bc_experiment, variants, run_id=2)
diff --git a/experiments/ashvin/rfeatures/sawyer/door2/bc1_new.py b/experiments/ashvin/rfeatures/sawyer/door2/bc1_new.py
deleted file mode 100644
index 8878d67..0000000
--- a/experiments/ashvin/rfeatures/sawyer/door2/bc1_new.py
+++ /dev/null
@@ -1,111 +0,0 @@
-import gym
-
-import railrl.torch.pytorch_util as ptu
-from railrl.data_management.obs_dict_replay_buffer import ObsDictRelabelingBuffer
-from railrl.exploration_strategies.base import \
-    PolicyWrappedWithExplorationStrategy
-from railrl.exploration_strategies.gaussian_and_epislon import \
-    GaussianAndEpislonStrategy
-from railrl.launchers.launcher_util import setup_logger
-from railrl.samplers.data_collector import GoalConditionedPathCollector
-from railrl.torch.her.her import HERTrainer
-from railrl.torch.networks import FlattenMlp, TanhMlpPolicy
-# from railrl.torch.td3.td3 import TD3
-from railrl.demos.td3_bc import TD3BCTrainer
-from railrl.torch.torch_rl_algorithm import TorchBatchRLAlgorithm
-# from multiworld.envs.mujoco.sawyer_xyz.sawyer_push_multiobj_subset import SawyerMultiobjectEnv
-# from multiworld.envs.mujoco.sawyer_xyz.sawyer_reach import SawyerReachXYZEnv
-
-from multiworld.core.image_env import ImageEnv
-from multiworld.envs.real_world.sawyer.sawyer_reaching import SawyerReachXYZEnv
-# from sawyer_control.envs.sawyer_reaching import SawyerReachXYZEnv
-
-from railrl.launchers.arglauncher import run_variants
-import railrl.misc.hyperparameter as hyp
-
-# from railrl.launchers.experiments.ashvin.rfeatures.rfeatures_trainer import TimePredictionTrainer
-from railrl.launchers.experiments.ashvin.rfeatures.rfeatures_rl import encoder_wrapped_td3bc_experiment
-
-if __name__ == "__main__":
-    variant = dict(
-        env_class=SawyerReachXYZEnv,
-        env_kwargs=dict(
-            action_mode="position", 
-            max_speed = 0.05, 
-            camera="sawyer_head"
-        ),
-        # algo_kwargs=dict(
-        #     num_epochs=3000,
-        #     max_path_length=20,
-        #     batch_size=128,
-        #     num_eval_steps_per_epoch=1000,
-        #     num_expl_steps_per_train_loop=1000,
-        #     num_trains_per_train_loop=1000,
-        #     min_num_steps_before_training=1000,
-        # ),
-        algo_kwargs=dict(
-            num_epochs=500,
-            max_path_length=100,
-            batch_size=128,
-            num_eval_steps_per_epoch=500,
-            num_expl_steps_per_train_loop=500,
-            num_trains_per_train_loop=500,
-            min_num_steps_before_training=0,
-        ),
-        model_kwargs=dict(
-            decoder_distribution='gaussian_identity_variance',
-            input_channels=3,
-            imsize=224,
-            architecture=dict(
-                hidden_sizes=[200, 200],
-            ),
-            delta_features=True,
-            pretrained_features=False,
-        ),
-        trainer_kwargs=dict(
-            discount=0.99,
-            demo_path="/home/anair/ros_ws/src/railrl-private/demos/door_demos_new_setup/processed_demos.pkl",
-            # demo_path="/home/anair/ros_ws/src/railrl-private/demos/door_demos_10_2/processed_demos_imagenet.pkl",
-            add_demo_latents=False, # already done
-            bc_num_pretrain_steps=1000,
-            rl_weight=0.0,
-            bc_weight=1.0,
-            weight_decay=0.01,
-        ),
-        replay_buffer_kwargs=dict(
-            max_size=100000,
-            fraction_goals_rollout_goals=1.0,
-            fraction_goals_env_goals=0.0,
-        ),
-        qf_kwargs=dict(
-            hidden_sizes=[400, 300],
-        ),
-        policy_kwargs=dict(
-            hidden_sizes=[32, 32],
-        ),
-
-        save_video=True,
-        dump_video_kwargs=dict(
-            save_period=1,
-            # imsize=(3, 500, 300),
-        ),
-        desired_trajectory="/home/anair/ros_ws/src/railrl-private/demos/door_demos_new_setup/demo_0.pkl",
-
-        logger_variant=dict(
-            tensorboard=True,
-        ),
-        model_path="/home/anair/data/s3doodad/facebook/models/rfeatures/multitask1/run2/id2/itr_4000.pt",
-    )
-
-    search_space = {
-        'seedid': range(1),
-    }
-    sweeper = hyp.DeterministicHyperparameterSweeper(
-        search_space, default_parameters=variant,
-    )
-
-    variants = []
-    for variant in sweeper.iterate_hyperparameters():
-        variants.append(variant)
-
-    run_variants(encoder_wrapped_td3bc_experiment, variants, run_id=3)
diff --git a/experiments/ashvin/rfeatures/sawyer/door2/bc_test1.py b/experiments/ashvin/rfeatures/sawyer/door2/bc_test1.py
deleted file mode 100644
index 2da6b6e..0000000
--- a/experiments/ashvin/rfeatures/sawyer/door2/bc_test1.py
+++ /dev/null
@@ -1,134 +0,0 @@
-import gym
-
-import railrl.torch.pytorch_util as ptu
-from railrl.data_management.obs_dict_replay_buffer import ObsDictRelabelingBuffer
-from railrl.exploration_strategies.base import \
-    PolicyWrappedWithExplorationStrategy
-from railrl.exploration_strategies.gaussian_and_epislon import \
-    GaussianAndEpislonStrategy
-from railrl.launchers.launcher_util import setup_logger
-from railrl.samplers.data_collector import GoalConditionedPathCollector
-from railrl.torch.her.her import HERTrainer
-from railrl.torch.networks import FlattenMlp, TanhMlpPolicy
-# from railrl.torch.td3.td3 import TD3
-from railrl.demos.td3_bc import TD3BCTrainer
-from railrl.torch.torch_rl_algorithm import TorchBatchRLAlgorithm
-# from multiworld.envs.mujoco.sawyer_xyz.sawyer_push_multiobj_subset import SawyerMultiobjectEnv
-# from multiworld.envs.mujoco.sawyer_xyz.sawyer_reach import SawyerReachXYZEnv
-
-from multiworld.core.image_env import ImageEnv
-from multiworld.envs.real_world.sawyer.sawyer_reaching import SawyerReachXYZEnv
-# from sawyer_control.envs.sawyer_reaching import SawyerReachXYZEnv
-
-from railrl.launchers.arglauncher import run_variants
-import railrl.misc.hyperparameter as hyp
-
-# from railrl.launchers.experiments.ashvin.rfeatures.rfeatures_trainer import TimePredictionTrainer
-from railrl.launchers.experiments.ashvin.rfeatures.rfeatures_rl import encoder_wrapped_td3bc_experiment
-
-if __name__ == "__main__":
-    colors = ["grey", "beige", "green", "brownhatch"]
-    # colors = ["grey"]
-    # demo_path = ["/home/anair/ros_ws/src/railrl-private/demos/door_demos_v3/processed_demos_%s_latent_distance__use_goal_from_trajectory_jitter2.pkl" % color for color in colors]
-    demo_path = ["/home/anair/ros_ws/src/railrl-private/demos/door_demos_v3/processed_demos_%s_latent_distance_use_initial_use_initial_from_trajectory_use_goal_from_trajectory_jitter2.pkl" % color for color in colors]
-    variant = dict(
-        env_class=SawyerReachXYZEnv,
-        env_kwargs=dict(
-            action_mode="position", 
-            max_speed = 0.05, 
-            camera="sawyer_head"
-        ),
-        # algo_kwargs=dict(
-        #     num_epochs=3000,
-        #     max_path_length=20,
-        #     batch_size=128,
-        #     num_eval_steps_per_epoch=1000,
-        #     num_expl_steps_per_train_loop=1000,
-        #     num_trains_per_train_loop=1000,
-        #     min_num_steps_before_training=1000,
-        # ),
-        algo_kwargs=dict(
-            num_epochs=500,
-            max_path_length=100,
-            batch_size=128,
-            num_eval_steps_per_epoch=500,
-            num_expl_steps_per_train_loop=500,
-            num_trains_per_train_loop=500,
-            min_num_steps_before_training=0,
-        ),
-        # config_params = dict(
-        #     initial_type="",
-        #     goal_type="",
-        #     use_initial=False
-        # ),
-        config_params = dict(
-            initial_type="use_initial_from_trajectory",
-            goal_type="",
-            use_initial=True
-        ),
-        reward_params_type="latent_distance",
-        model_kwargs=dict(
-            decoder_distribution='gaussian_identity_variance',
-            input_channels=3,
-            imsize=224,
-            architecture=dict(
-                hidden_sizes=[200, 200],
-            ),
-            delta_features=True,
-            pretrained_features=False,
-        ),
-        trainer_kwargs=dict(
-            discount=0.99,
-            demo_path=demo_path,
-            # demo_path="/home/anair/ros_ws/src/railrl-private/demos/door_demos_10_2/processed_demos_imagenet.pkl",
-            add_demo_latents=False, # already done
-            bc_num_pretrain_steps=10000,
-            rl_weight=0.0,
-            bc_weight=1.0,
-            weight_decay=0.001,
-        ),
-        replay_buffer_kwargs=dict(
-            max_size=1000000,
-            fraction_goals_rollout_goals=1.0,
-            fraction_goals_env_goals=0.0,
-        ),
-        qf_kwargs=dict(
-            hidden_sizes=[400, 300],
-        ),
-        policy_kwargs=dict(
-            hidden_sizes=[128, 128],
-        ),
-
-        save_video=True,
-        save_period=1,
-        dump_video_kwargs=dict(
-            imwidth=500,
-            imheight=300,
-            num_imgs=1,
-            dump_pickle=True,
-            exploration_goal_image_key="image_observation",
-            evaluation_goal_image_key="image_observation",
-            rows=1,
-            columns=5,
-            unnormalize=False,
-        ),
-        desired_trajectory="/home/anair/ros_ws/src/railrl-private/demos/door_demos_v3/demo_v3_grey_0.pkl",
-
-        logger_variant=dict(
-            tensorboard=True,
-        ),
-        model_path="/home/anair/data/s3doodad/facebook/models/rfeatures/multitask1/run2/id2/itr_4000.pt",
-    )
-
-    search_space = {
-        'seedid': range(1),
-    }
-    sweeper = hyp.DeterministicHyperparameterSweeper(
-        search_space, default_parameters=variant,
-    )
-
-    variants = []
-    for variant in sweeper.iterate_hyperparameters():
-        variants.append(variant)
-
-    run_variants(encoder_wrapped_td3bc_experiment, variants, run_id=1)
diff --git a/experiments/ashvin/rfeatures/sawyer/door2/bc_v3_1.py b/experiments/ashvin/rfeatures/sawyer/door2/bc_v3_1.py
deleted file mode 100644
index 0768269..0000000
--- a/experiments/ashvin/rfeatures/sawyer/door2/bc_v3_1.py
+++ /dev/null
@@ -1,111 +0,0 @@
-import gym
-
-import railrl.torch.pytorch_util as ptu
-from railrl.data_management.obs_dict_replay_buffer import ObsDictRelabelingBuffer
-from railrl.exploration_strategies.base import \
-    PolicyWrappedWithExplorationStrategy
-from railrl.exploration_strategies.gaussian_and_epislon import \
-    GaussianAndEpislonStrategy
-from railrl.launchers.launcher_util import setup_logger
-from railrl.samplers.data_collector import GoalConditionedPathCollector
-from railrl.torch.her.her import HERTrainer
-from railrl.torch.networks import FlattenMlp, TanhMlpPolicy
-# from railrl.torch.td3.td3 import TD3
-from railrl.demos.td3_bc import TD3BCTrainer
-from railrl.torch.torch_rl_algorithm import TorchBatchRLAlgorithm
-# from multiworld.envs.mujoco.sawyer_xyz.sawyer_push_multiobj_subset import SawyerMultiobjectEnv
-# from multiworld.envs.mujoco.sawyer_xyz.sawyer_reach import SawyerReachXYZEnv
-
-from multiworld.core.image_env import ImageEnv
-from multiworld.envs.real_world.sawyer.sawyer_reaching import SawyerReachXYZEnv
-# from sawyer_control.envs.sawyer_reaching import SawyerReachXYZEnv
-
-from railrl.launchers.arglauncher import run_variants
-import railrl.misc.hyperparameter as hyp
-
-# from railrl.launchers.experiments.ashvin.rfeatures.rfeatures_trainer import TimePredictionTrainer
-from railrl.launchers.experiments.ashvin.rfeatures.rfeatures_rl import encoder_wrapped_td3bc_experiment
-
-if __name__ == "__main__":
-    variant = dict(
-        env_class=SawyerReachXYZEnv,
-        env_kwargs=dict(
-            action_mode="position", 
-            max_speed = 0.05, 
-            camera="sawyer_head"
-        ),
-        # algo_kwargs=dict(
-        #     num_epochs=3000,
-        #     max_path_length=20,
-        #     batch_size=128,
-        #     num_eval_steps_per_epoch=1000,
-        #     num_expl_steps_per_train_loop=1000,
-        #     num_trains_per_train_loop=1000,
-        #     min_num_steps_before_training=1000,
-        # ),
-        algo_kwargs=dict(
-            num_epochs=500,
-            max_path_length=100,
-            batch_size=128,
-            num_eval_steps_per_epoch=500,
-            num_expl_steps_per_train_loop=500,
-            num_trains_per_train_loop=500,
-            min_num_steps_before_training=0,
-        ),
-        model_kwargs=dict(
-            decoder_distribution='gaussian_identity_variance',
-            input_channels=3,
-            imsize=224,
-            architecture=dict(
-                hidden_sizes=[200, 200],
-            ),
-            delta_features=True,
-            pretrained_features=False,
-        ),
-        trainer_kwargs=dict(
-            discount=0.99,
-            demo_path="/home/anair/ros_ws/src/railrl-private/demos/door_demos_v3/processed_demos.pkl",
-            # demo_path="/home/anair/ros_ws/src/railrl-private/demos/door_demos_10_2/processed_demos_imagenet.pkl",
-            add_demo_latents=False, # already done
-            bc_num_pretrain_steps=1000,
-            rl_weight=0.0,
-            bc_weight=1.0,
-            weight_decay=0.01,
-        ),
-        replay_buffer_kwargs=dict(
-            max_size=100000,
-            fraction_goals_rollout_goals=1.0,
-            fraction_goals_env_goals=0.0,
-        ),
-        qf_kwargs=dict(
-            hidden_sizes=[400, 300],
-        ),
-        policy_kwargs=dict(
-            hidden_sizes=[32, 32],
-        ),
-
-        save_video=True,
-        dump_video_kwargs=dict(
-            save_period=1,
-            # imsize=(3, 500, 300),
-        ),
-        desired_trajectory="/home/anair/ros_ws/src/railrl-private/demos/door_demos_v3/demo_v3_0.pkl",
-
-        logger_variant=dict(
-            tensorboard=True,
-        ),
-        model_path="/home/anair/data/s3doodad/facebook/models/rfeatures/multitask1/run2/id2/itr_4000.pt",
-    )
-
-    search_space = {
-        'seedid': range(1),
-    }
-    sweeper = hyp.DeterministicHyperparameterSweeper(
-        search_space, default_parameters=variant,
-    )
-
-    variants = []
-    for variant in sweeper.iterate_hyperparameters():
-        variants.append(variant)
-
-    run_variants(encoder_wrapped_td3bc_experiment, variants, run_id=2)
diff --git a/experiments/ashvin/rfeatures/sawyer/door2/bc_v3_2.py b/experiments/ashvin/rfeatures/sawyer/door2/bc_v3_2.py
deleted file mode 100644
index 16c5500..0000000
--- a/experiments/ashvin/rfeatures/sawyer/door2/bc_v3_2.py
+++ /dev/null
@@ -1,111 +0,0 @@
-import gym
-
-import railrl.torch.pytorch_util as ptu
-from railrl.data_management.obs_dict_replay_buffer import ObsDictRelabelingBuffer
-from railrl.exploration_strategies.base import \
-    PolicyWrappedWithExplorationStrategy
-from railrl.exploration_strategies.gaussian_and_epislon import \
-    GaussianAndEpislonStrategy
-from railrl.launchers.launcher_util import setup_logger
-from railrl.samplers.data_collector import GoalConditionedPathCollector
-from railrl.torch.her.her import HERTrainer
-from railrl.torch.networks import FlattenMlp, TanhMlpPolicy
-# from railrl.torch.td3.td3 import TD3
-from railrl.demos.td3_bc import TD3BCTrainer
-from railrl.torch.torch_rl_algorithm import TorchBatchRLAlgorithm
-# from multiworld.envs.mujoco.sawyer_xyz.sawyer_push_multiobj_subset import SawyerMultiobjectEnv
-# from multiworld.envs.mujoco.sawyer_xyz.sawyer_reach import SawyerReachXYZEnv
-
-from multiworld.core.image_env import ImageEnv
-from multiworld.envs.real_world.sawyer.sawyer_reaching import SawyerReachXYZEnv
-# from sawyer_control.envs.sawyer_reaching import SawyerReachXYZEnv
-
-from railrl.launchers.arglauncher import run_variants
-import railrl.misc.hyperparameter as hyp
-
-# from railrl.launchers.experiments.ashvin.rfeatures.rfeatures_trainer import TimePredictionTrainer
-from railrl.launchers.experiments.ashvin.rfeatures.rfeatures_rl import encoder_wrapped_td3bc_experiment
-
-if __name__ == "__main__":
-    variant = dict(
-        env_class=SawyerReachXYZEnv,
-        env_kwargs=dict(
-            action_mode="position", 
-            max_speed = 0.05, 
-            camera="sawyer_head"
-        ),
-        # algo_kwargs=dict(
-        #     num_epochs=3000,
-        #     max_path_length=20,
-        #     batch_size=128,
-        #     num_eval_steps_per_epoch=1000,
-        #     num_expl_steps_per_train_loop=1000,
-        #     num_trains_per_train_loop=1000,
-        #     min_num_steps_before_training=1000,
-        # ),
-        algo_kwargs=dict(
-            num_epochs=500,
-            max_path_length=100,
-            batch_size=128,
-            num_eval_steps_per_epoch=500,
-            num_expl_steps_per_train_loop=500,
-            num_trains_per_train_loop=500,
-            min_num_steps_before_training=0,
-        ),
-        model_kwargs=dict(
-            decoder_distribution='gaussian_identity_variance',
-            input_channels=3,
-            imsize=224,
-            architecture=dict(
-                hidden_sizes=[200, 200],
-            ),
-            delta_features=True,
-            pretrained_features=False,
-        ),
-        trainer_kwargs=dict(
-            discount=0.99,
-            demo_path="/home/anair/ros_ws/src/railrl-private/demos/door_demos_v3/processed_demos.pkl",
-            # demo_path="/home/anair/ros_ws/src/railrl-private/demos/door_demos_10_2/processed_demos_imagenet.pkl",
-            add_demo_latents=False, # already done
-            bc_num_pretrain_steps=1000,
-            rl_weight=0.0,
-            bc_weight=1.0,
-            weight_decay=0.01,
-        ),
-        replay_buffer_kwargs=dict(
-            max_size=100000,
-            fraction_goals_rollout_goals=1.0,
-            fraction_goals_env_goals=0.0,
-        ),
-        qf_kwargs=dict(
-            hidden_sizes=[400, 300],
-        ),
-        policy_kwargs=dict(
-            hidden_sizes=[32, 32],
-        ),
-
-        save_video=True,
-        dump_video_kwargs=dict(
-            save_period=1,
-            # imsize=(3, 500, 300),
-        ),
-        desired_trajectory="/home/anair/ros_ws/src/railrl-private/demos/door_demos_v3/demo_v3_0.pkl",
-
-        logger_variant=dict(
-            tensorboard=True,
-        ),
-        model_path="/home/anair/data/s3doodad/facebook/models/rfeatures/multitask1/run2/id2/itr_4000.pt",
-    )
-
-    search_space = {
-        'seedid': range(1),
-    }
-    sweeper = hyp.DeterministicHyperparameterSweeper(
-        search_space, default_parameters=variant,
-    )
-
-    variants = []
-    for variant in sweeper.iterate_hyperparameters():
-        variants.append(variant)
-
-    run_variants(encoder_wrapped_td3bc_experiment, variants, run_id=3)
diff --git a/experiments/ashvin/rfeatures/sawyer/door2/bc_v3_color1.py b/experiments/ashvin/rfeatures/sawyer/door2/bc_v3_color1.py
deleted file mode 100644
index 799434c..0000000
--- a/experiments/ashvin/rfeatures/sawyer/door2/bc_v3_color1.py
+++ /dev/null
@@ -1,111 +0,0 @@
-import gym
-
-import railrl.torch.pytorch_util as ptu
-from railrl.data_management.obs_dict_replay_buffer import ObsDictRelabelingBuffer
-from railrl.exploration_strategies.base import \
-    PolicyWrappedWithExplorationStrategy
-from railrl.exploration_strategies.gaussian_and_epislon import \
-    GaussianAndEpislonStrategy
-from railrl.launchers.launcher_util import setup_logger
-from railrl.samplers.data_collector import GoalConditionedPathCollector
-from railrl.torch.her.her import HERTrainer
-from railrl.torch.networks import FlattenMlp, TanhMlpPolicy
-# from railrl.torch.td3.td3 import TD3
-from railrl.demos.td3_bc import TD3BCTrainer
-from railrl.torch.torch_rl_algorithm import TorchBatchRLAlgorithm
-# from multiworld.envs.mujoco.sawyer_xyz.sawyer_push_multiobj_subset import SawyerMultiobjectEnv
-# from multiworld.envs.mujoco.sawyer_xyz.sawyer_reach import SawyerReachXYZEnv
-
-from multiworld.core.image_env import ImageEnv
-from multiworld.envs.real_world.sawyer.sawyer_reaching import SawyerReachXYZEnv
-# from sawyer_control.envs.sawyer_reaching import SawyerReachXYZEnv
-
-from railrl.launchers.arglauncher import run_variants
-import railrl.misc.hyperparameter as hyp
-
-# from railrl.launchers.experiments.ashvin.rfeatures.rfeatures_trainer import TimePredictionTrainer
-from railrl.launchers.experiments.ashvin.rfeatures.rfeatures_rl import encoder_wrapped_td3bc_experiment
-
-if __name__ == "__main__":
-    variant = dict(
-        env_class=SawyerReachXYZEnv,
-        env_kwargs=dict(
-            action_mode="position", 
-            max_speed = 0.05, 
-            camera="sawyer_head"
-        ),
-        # algo_kwargs=dict(
-        #     num_epochs=3000,
-        #     max_path_length=20,
-        #     batch_size=128,
-        #     num_eval_steps_per_epoch=1000,
-        #     num_expl_steps_per_train_loop=1000,
-        #     num_trains_per_train_loop=1000,
-        #     min_num_steps_before_training=1000,
-        # ),
-        algo_kwargs=dict(
-            num_epochs=500,
-            max_path_length=100,
-            batch_size=128,
-            num_eval_steps_per_epoch=500,
-            num_expl_steps_per_train_loop=500,
-            num_trains_per_train_loop=500,
-            min_num_steps_before_training=0,
-        ),
-        model_kwargs=dict(
-            decoder_distribution='gaussian_identity_variance',
-            input_channels=3,
-            imsize=224,
-            architecture=dict(
-                hidden_sizes=[200, 200],
-            ),
-            delta_features=True,
-            pretrained_features=False,
-        ),
-        trainer_kwargs=dict(
-            discount=0.99,
-            demo_path="/home/anair/ros_ws/src/railrl-private/demos/door_demos_v3/processed_demos_color1.pkl",
-            # demo_path="/home/anair/ros_ws/src/railrl-private/demos/door_demos_10_2/processed_demos_imagenet.pkl",
-            add_demo_latents=False, # already done
-            bc_num_pretrain_steps=1000,
-            rl_weight=0.0,
-            bc_weight=1.0,
-            weight_decay=0.001,
-        ),
-        replay_buffer_kwargs=dict(
-            max_size=100000,
-            fraction_goals_rollout_goals=1.0,
-            fraction_goals_env_goals=0.0,
-        ),
-        qf_kwargs=dict(
-            hidden_sizes=[400, 300],
-        ),
-        policy_kwargs=dict(
-            hidden_sizes=[128, 128],
-        ),
-
-        save_video=True,
-        dump_video_kwargs=dict(
-            save_period=1,
-            # imsize=(3, 500, 300),
-        ),
-        desired_trajectory="/home/anair/ros_ws/src/railrl-private/demos/door_demos_v3/demo_v3_0.pkl",
-
-        logger_variant=dict(
-            tensorboard=True,
-        ),
-        model_path="/home/anair/data/s3doodad/facebook/models/rfeatures/multitask1/run2/id2/itr_4000.pt",
-    )
-
-    search_space = {
-        'seedid': range(1),
-    }
-    sweeper = hyp.DeterministicHyperparameterSweeper(
-        search_space, default_parameters=variant,
-    )
-
-    variants = []
-    for variant in sweeper.iterate_hyperparameters():
-        variants.append(variant)
-
-    run_variants(encoder_wrapped_td3bc_experiment, variants, run_id=7)
diff --git a/experiments/ashvin/rfeatures/sawyer/door2/bc_v3_color2.py b/experiments/ashvin/rfeatures/sawyer/door2/bc_v3_color2.py
deleted file mode 100644
index 6a811af..0000000
--- a/experiments/ashvin/rfeatures/sawyer/door2/bc_v3_color2.py
+++ /dev/null
@@ -1,111 +0,0 @@
-import gym
-
-import railrl.torch.pytorch_util as ptu
-from railrl.data_management.obs_dict_replay_buffer import ObsDictRelabelingBuffer
-from railrl.exploration_strategies.base import \
-    PolicyWrappedWithExplorationStrategy
-from railrl.exploration_strategies.gaussian_and_epislon import \
-    GaussianAndEpislonStrategy
-from railrl.launchers.launcher_util import setup_logger
-from railrl.samplers.data_collector import GoalConditionedPathCollector
-from railrl.torch.her.her import HERTrainer
-from railrl.torch.networks import FlattenMlp, TanhMlpPolicy
-# from railrl.torch.td3.td3 import TD3
-from railrl.demos.td3_bc import TD3BCTrainer
-from railrl.torch.torch_rl_algorithm import TorchBatchRLAlgorithm
-# from multiworld.envs.mujoco.sawyer_xyz.sawyer_push_multiobj_subset import SawyerMultiobjectEnv
-# from multiworld.envs.mujoco.sawyer_xyz.sawyer_reach import SawyerReachXYZEnv
-
-from multiworld.core.image_env import ImageEnv
-from multiworld.envs.real_world.sawyer.sawyer_reaching import SawyerReachXYZEnv
-# from sawyer_control.envs.sawyer_reaching import SawyerReachXYZEnv
-
-from railrl.launchers.arglauncher import run_variants
-import railrl.misc.hyperparameter as hyp
-
-# from railrl.launchers.experiments.ashvin.rfeatures.rfeatures_trainer import TimePredictionTrainer
-from railrl.launchers.experiments.ashvin.rfeatures.rfeatures_rl import encoder_wrapped_td3bc_experiment
-
-if __name__ == "__main__":
-    variant = dict(
-        env_class=SawyerReachXYZEnv,
-        env_kwargs=dict(
-            action_mode="position", 
-            max_speed = 0.05, 
-            camera="sawyer_head"
-        ),
-        # algo_kwargs=dict(
-        #     num_epochs=3000,
-        #     max_path_length=20,
-        #     batch_size=128,
-        #     num_eval_steps_per_epoch=1000,
-        #     num_expl_steps_per_train_loop=1000,
-        #     num_trains_per_train_loop=1000,
-        #     min_num_steps_before_training=1000,
-        # ),
-        algo_kwargs=dict(
-            num_epochs=500,
-            max_path_length=100,
-            batch_size=128,
-            num_eval_steps_per_epoch=500,
-            num_expl_steps_per_train_loop=500,
-            num_trains_per_train_loop=500,
-            min_num_steps_before_training=0,
-        ),
-        model_kwargs=dict(
-            decoder_distribution='gaussian_identity_variance',
-            input_channels=3,
-            imsize=224,
-            architecture=dict(
-                hidden_sizes=[200, 200],
-            ),
-            delta_features=True,
-            pretrained_features=False,
-        ),
-        trainer_kwargs=dict(
-            discount=0.99,
-            demo_path="/home/anair/ros_ws/src/railrl-private/demos/door_demos_v3/processed_demos_color1.pkl",
-            # demo_path="/home/anair/ros_ws/src/railrl-private/demos/door_demos_10_2/processed_demos_imagenet.pkl",
-            add_demo_latents=False, # already done
-            bc_num_pretrain_steps=1000,
-            rl_weight=0.0,
-            bc_weight=1.0,
-            weight_decay=0.001,
-        ),
-        replay_buffer_kwargs=dict(
-            max_size=100000,
-            fraction_goals_rollout_goals=1.0,
-            fraction_goals_env_goals=0.0,
-        ),
-        qf_kwargs=dict(
-            hidden_sizes=[400, 300],
-        ),
-        policy_kwargs=dict(
-            hidden_sizes=[128, 128],
-        ),
-
-        save_video=True,
-        dump_video_kwargs=dict(
-            save_period=1,
-            # imsize=(3, 500, 300),
-        ),
-        desired_trajectory="/home/anair/ros_ws/src/railrl-private/demos/door_demos_v3/demo_v3_0.pkl",
-
-        logger_variant=dict(
-            tensorboard=True,
-        ),
-        model_path="/home/anair/data/s3doodad/facebook/models/rfeatures/multitask1/run2/id2/itr_4000.pt",
-    )
-
-    search_space = {
-        'seedid': range(1),
-    }
-    sweeper = hyp.DeterministicHyperparameterSweeper(
-        search_space, default_parameters=variant,
-    )
-
-    variants = []
-    for variant in sweeper.iterate_hyperparameters():
-        variants.append(variant)
-
-    run_variants(encoder_wrapped_td3bc_experiment, variants, run_id=1)
diff --git a/experiments/ashvin/rfeatures/sawyer/door2/bc_v3_imagenet1.py b/experiments/ashvin/rfeatures/sawyer/door2/bc_v3_imagenet1.py
deleted file mode 100644
index f74ec98..0000000
--- a/experiments/ashvin/rfeatures/sawyer/door2/bc_v3_imagenet1.py
+++ /dev/null
@@ -1,111 +0,0 @@
-import gym
-
-import railrl.torch.pytorch_util as ptu
-from railrl.data_management.obs_dict_replay_buffer import ObsDictRelabelingBuffer
-from railrl.exploration_strategies.base import \
-    PolicyWrappedWithExplorationStrategy
-from railrl.exploration_strategies.gaussian_and_epislon import \
-    GaussianAndEpislonStrategy
-from railrl.launchers.launcher_util import setup_logger
-from railrl.samplers.data_collector import GoalConditionedPathCollector
-from railrl.torch.her.her import HERTrainer
-from railrl.torch.networks import FlattenMlp, TanhMlpPolicy
-# from railrl.torch.td3.td3 import TD3
-from railrl.demos.td3_bc import TD3BCTrainer
-from railrl.torch.torch_rl_algorithm import TorchBatchRLAlgorithm
-# from multiworld.envs.mujoco.sawyer_xyz.sawyer_push_multiobj_subset import SawyerMultiobjectEnv
-# from multiworld.envs.mujoco.sawyer_xyz.sawyer_reach import SawyerReachXYZEnv
-
-from multiworld.core.image_env import ImageEnv
-from multiworld.envs.real_world.sawyer.sawyer_reaching import SawyerReachXYZEnv
-# from sawyer_control.envs.sawyer_reaching import SawyerReachXYZEnv
-
-from railrl.launchers.arglauncher import run_variants
-import railrl.misc.hyperparameter as hyp
-
-# from railrl.launchers.experiments.ashvin.rfeatures.rfeatures_trainer import TimePredictionTrainer
-from railrl.launchers.experiments.ashvin.rfeatures.rfeatures_rl import encoder_wrapped_td3bc_experiment
-
-if __name__ == "__main__":
-    variant = dict(
-        env_class=SawyerReachXYZEnv,
-        env_kwargs=dict(
-            action_mode="position", 
-            max_speed = 0.05, 
-            camera="sawyer_head"
-        ),
-        # algo_kwargs=dict(
-        #     num_epochs=3000,
-        #     max_path_length=20,
-        #     batch_size=128,
-        #     num_eval_steps_per_epoch=1000,
-        #     num_expl_steps_per_train_loop=1000,
-        #     num_trains_per_train_loop=1000,
-        #     min_num_steps_before_training=1000,
-        # ),
-        algo_kwargs=dict(
-            num_epochs=500,
-            max_path_length=100,
-            batch_size=128,
-            num_eval_steps_per_epoch=500,
-            num_expl_steps_per_train_loop=500,
-            num_trains_per_train_loop=500,
-            min_num_steps_before_training=0,
-        ),
-        model_kwargs=dict(
-            decoder_distribution='gaussian_identity_variance',
-            input_channels=3,
-            imsize=224,
-            architecture=dict(
-                hidden_sizes=[200, 200],
-            ),
-            delta_features=True,
-            pretrained_features=False,
-        ),
-        trainer_kwargs=dict(
-            discount=0.99,
-            # demo_path="/home/anair/ros_ws/src/railrl-private/demos/door_demos_v3/processed_demos.pkl",
-            demo_path="/home/anair/ros_ws/src/railrl-private/demos/door_demos_v3/processed_demos_imagenet.pkl",
-            add_demo_latents=False, # already done
-            bc_num_pretrain_steps=1000,
-            rl_weight=0.0,
-            bc_weight=1.0,
-            weight_decay=0.01,
-        ),
-        replay_buffer_kwargs=dict(
-            max_size=100000,
-            fraction_goals_rollout_goals=1.0,
-            fraction_goals_env_goals=0.0,
-        ),
-        qf_kwargs=dict(
-            hidden_sizes=[400, 300],
-        ),
-        policy_kwargs=dict(
-            hidden_sizes=[32, 32],
-        ),
-
-        save_video=True,
-        dump_video_kwargs=dict(
-            save_period=1,
-            # imsize=(3, 500, 300),
-        ),
-        desired_trajectory="/home/anair/ros_ws/src/railrl-private/demos/door_demos_v3/demo_v3_0.pkl",
-
-        logger_variant=dict(
-            tensorboard=True,
-        ),
-        model_path="/home/anair/data/s3doodad/facebook/models/rfeatures/multitask1/run2/id0/itr_0.pt",
-    )
-
-    search_space = {
-        'seedid': range(1),
-    }
-    sweeper = hyp.DeterministicHyperparameterSweeper(
-        search_space, default_parameters=variant,
-    )
-
-    variants = []
-    for variant in sweeper.iterate_hyperparameters():
-        variants.append(variant)
-
-    run_variants(encoder_wrapped_td3bc_experiment, variants, run_id=2)
diff --git a/experiments/ashvin/rfeatures/sawyer/door2/bc_v3_imagenet_color1.py b/experiments/ashvin/rfeatures/sawyer/door2/bc_v3_imagenet_color1.py
deleted file mode 100644
index e1a3f00..0000000
--- a/experiments/ashvin/rfeatures/sawyer/door2/bc_v3_imagenet_color1.py
+++ /dev/null
@@ -1,112 +0,0 @@
-import gym
-
-import railrl.torch.pytorch_util as ptu
-from railrl.data_management.obs_dict_replay_buffer import ObsDictRelabelingBuffer
-from railrl.exploration_strategies.base import \
-    PolicyWrappedWithExplorationStrategy
-from railrl.exploration_strategies.gaussian_and_epislon import \
-    GaussianAndEpislonStrategy
-from railrl.launchers.launcher_util import setup_logger
-from railrl.samplers.data_collector import GoalConditionedPathCollector
-from railrl.torch.her.her import HERTrainer
-from railrl.torch.networks import FlattenMlp, TanhMlpPolicy
-# from railrl.torch.td3.td3 import TD3
-from railrl.demos.td3_bc import TD3BCTrainer
-from railrl.torch.torch_rl_algorithm import TorchBatchRLAlgorithm
-# from multiworld.envs.mujoco.sawyer_xyz.sawyer_push_multiobj_subset import SawyerMultiobjectEnv
-# from multiworld.envs.mujoco.sawyer_xyz.sawyer_reach import SawyerReachXYZEnv
-
-from multiworld.core.image_env import ImageEnv
-from multiworld.envs.real_world.sawyer.sawyer_reaching import SawyerReachXYZEnv
-# from sawyer_control.envs.sawyer_reaching import SawyerReachXYZEnv
-
-from railrl.launchers.arglauncher import run_variants
-import railrl.misc.hyperparameter as hyp
-
-# from railrl.launchers.experiments.ashvin.rfeatures.rfeatures_trainer import TimePredictionTrainer
-from railrl.launchers.experiments.ashvin.rfeatures.rfeatures_rl import encoder_wrapped_td3bc_experiment
-
-if __name__ == "__main__":
-    variant = dict(
-        env_class=SawyerReachXYZEnv,
-        env_kwargs=dict(
-            action_mode="position", 
-            max_speed = 0.05, 
-            camera="sawyer_head"
-        ),
-        # algo_kwargs=dict(
-        #     num_epochs=3000,
-        #     max_path_length=20,
-        #     batch_size=128,
-        #     num_eval_steps_per_epoch=1000,
-        #     num_expl_steps_per_train_loop=1000,
-        #     num_trains_per_train_loop=1000,
-        #     min_num_steps_before_training=1000,
-        # ),
-        algo_kwargs=dict(
-            num_epochs=500,
-            max_path_length=100,
-            batch_size=128,
-            num_eval_steps_per_epoch=500,
-            num_expl_steps_per_train_loop=500,
-            num_trains_per_train_loop=500,
-            min_num_steps_before_training=0,
-        ),
-        model_kwargs=dict(
-            decoder_distribution='gaussian_identity_variance',
-            input_channels=3,
-            imsize=224,
-            architecture=dict(
-                hidden_sizes=[200, 200],
-            ),
-            delta_features=True,
-            pretrained_features=False,
-        ),
-        trainer_kwargs=dict(
-            discount=0.99,
-            # demo_path="/home/anair/ros_ws/src/railrl-private/demos/door_demos_v3/processed_demos_color1.pkl",
-            demo_path="/home/anair/ros_ws/src/railrl-private/demos/door_demos_v3/processed_demos_imagenet_color1.pkl",
-            add_demo_latents=False, # already done
-            bc_num_pretrain_steps=1000,
-            rl_weight=0.0,
-            bc_weight=1.0,
-            weight_decay=0.01,
-        ),
-        replay_buffer_kwargs=dict(
-            max_size=100000,
-            fraction_goals_rollout_goals=1.0,
-            fraction_goals_env_goals=0.0,
-        ),
-        qf_kwargs=dict(
-            hidden_sizes=[400, 300],
-        ),
-        policy_kwargs=dict(
-            hidden_sizes=[32, 32],
-        ),
-
-        save_video=True,
-        dump_video_kwargs=dict(
-            save_period=1,
-            # imsize=(3, 500, 300),
-        ),
-        desired_trajectory="/home/anair/ros_ws/src/railrl-private/demos/door_demos_v3/demo_v3_0.pkl",
-
-        logger_variant=dict(
-            tensorboard=True,
-        ),
-        # model_path="/home/anair/data/s3doodad/facebook/models/rfeatures/multitask1/run2/id2/itr_4000.pt",
-        model_path="/home/anair/data/s3doodad/facebook/models/rfeatures/multitask1/run2/id0/itr_0.pt",
-    )
-
-    search_space = {
-        'seedid': range(1),
-    }
-    sweeper = hyp.DeterministicHyperparameterSweeper(
-        search_space, default_parameters=variant,
-    )
-
-    variants = []
-    for variant in sweeper.iterate_hyperparameters():
-        variants.append(variant)
-
-    run_variants(encoder_wrapped_td3bc_experiment, variants, run_id=3)
diff --git a/experiments/ashvin/rfeatures/sawyer/door2/bc_v3_imagenet_jittered2.py b/experiments/ashvin/rfeatures/sawyer/door2/bc_v3_imagenet_jittered2.py
deleted file mode 100644
index bc45faf..0000000
--- a/experiments/ashvin/rfeatures/sawyer/door2/bc_v3_imagenet_jittered2.py
+++ /dev/null
@@ -1,112 +0,0 @@
-import gym
-
-import railrl.torch.pytorch_util as ptu
-from railrl.data_management.obs_dict_replay_buffer import ObsDictRelabelingBuffer
-from railrl.exploration_strategies.base import \
-    PolicyWrappedWithExplorationStrategy
-from railrl.exploration_strategies.gaussian_and_epislon import \
-    GaussianAndEpislonStrategy
-from railrl.launchers.launcher_util import setup_logger
-from railrl.samplers.data_collector import GoalConditionedPathCollector
-from railrl.torch.her.her import HERTrainer
-from railrl.torch.networks import FlattenMlp, TanhMlpPolicy
-# from railrl.torch.td3.td3 import TD3
-from railrl.demos.td3_bc import TD3BCTrainer
-from railrl.torch.torch_rl_algorithm import TorchBatchRLAlgorithm
-# from multiworld.envs.mujoco.sawyer_xyz.sawyer_push_multiobj_subset import SawyerMultiobjectEnv
-# from multiworld.envs.mujoco.sawyer_xyz.sawyer_reach import SawyerReachXYZEnv
-
-from multiworld.core.image_env import ImageEnv
-from multiworld.envs.real_world.sawyer.sawyer_reaching import SawyerReachXYZEnv
-# from sawyer_control.envs.sawyer_reaching import SawyerReachXYZEnv
-
-from railrl.launchers.arglauncher import run_variants
-import railrl.misc.hyperparameter as hyp
-
-# from railrl.launchers.experiments.ashvin.rfeatures.rfeatures_trainer import TimePredictionTrainer
-from railrl.launchers.experiments.ashvin.rfeatures.rfeatures_rl import encoder_wrapped_td3bc_experiment
-
-if __name__ == "__main__":
-    variant = dict(
-        env_class=SawyerReachXYZEnv,
-        env_kwargs=dict(
-            action_mode="position", 
-            max_speed = 0.05, 
-            camera="sawyer_head"
-        ),
-        # algo_kwargs=dict(
-        #     num_epochs=3000,
-        #     max_path_length=20,
-        #     batch_size=128,
-        #     num_eval_steps_per_epoch=1000,
-        #     num_expl_steps_per_train_loop=1000,
-        #     num_trains_per_train_loop=1000,
-        #     min_num_steps_before_training=1000,
-        # ),
-        algo_kwargs=dict(
-            num_epochs=500,
-            max_path_length=100,
-            batch_size=128,
-            num_eval_steps_per_epoch=500,
-            num_expl_steps_per_train_loop=500,
-            num_trains_per_train_loop=500,
-            min_num_steps_before_training=0,
-        ),
-        model_kwargs=dict(
-            decoder_distribution='gaussian_identity_variance',
-            input_channels=3,
-            imsize=224,
-            architecture=dict(
-                hidden_sizes=[200, 200],
-            ),
-            delta_features=True,
-            pretrained_features=False,
-        ),
-        trainer_kwargs=dict(
-            discount=0.99,
-            demo_path="/home/anair/ros_ws/src/railrl-private/demos/door_demos_v3/processed_demos_jitter2.pkl",
-            # demo_path="/home/anair/ros_ws/src/railrl-private/demos/door_demos_10_2/processed_demos_imagenet.pkl",
-            add_demo_latents=False, # already done
-            bc_num_pretrain_steps=10000,
-            rl_weight=0.0,
-            bc_weight=10.0,
-            weight_decay=0.01,
-        ),
-        replay_buffer_kwargs=dict(
-            max_size=100000,
-            fraction_goals_rollout_goals=1.0,
-            fraction_goals_env_goals=0.0,
-        ),
-        qf_kwargs=dict(
-            hidden_sizes=[400, 300],
-        ),
-        policy_kwargs=dict(
-            hidden_sizes=[32, 32],
-        ),
-
-        save_video=True,
-        dump_video_kwargs=dict(
-            save_period=1,
-            # imsize=(3, 500, 300),
-        ),
-        desired_trajectory="/home/anair/ros_ws/src/railrl-private/demos/door_demos_v3/demo_v3_0.pkl",
-
-        logger_variant=dict(
-            tensorboard=True,
-        ),
-        # model_path="/home/anair/data/s3doodad/facebook/models/rfeatures/multitask1/run2/id2/itr_4000.pt",
-        model_path="/home/anair/data/s3doodad/facebook/models/rfeatures/multitask1/run2/id0/itr_0.pt",
-    )
-
-    search_space = {
-        'seedid': range(1),
-    }
-    sweeper = hyp.DeterministicHyperparameterSweeper(
-        search_space, default_parameters=variant,
-    )
-
-    variants = []
-    for variant in sweeper.iterate_hyperparameters():
-        variants.append(variant)
-
-    run_variants(encoder_wrapped_td3bc_experiment, variants, run_id=2)
diff --git a/experiments/ashvin/rfeatures/sawyer/door2/bc_v3_jittered1.py b/experiments/ashvin/rfeatures/sawyer/door2/bc_v3_jittered1.py
deleted file mode 100644
index 4a92328..0000000
--- a/experiments/ashvin/rfeatures/sawyer/door2/bc_v3_jittered1.py
+++ /dev/null
@@ -1,111 +0,0 @@
-import gym
-
-import railrl.torch.pytorch_util as ptu
-from railrl.data_management.obs_dict_replay_buffer import ObsDictRelabelingBuffer
-from railrl.exploration_strategies.base import \
-    PolicyWrappedWithExplorationStrategy
-from railrl.exploration_strategies.gaussian_and_epislon import \
-    GaussianAndEpislonStrategy
-from railrl.launchers.launcher_util import setup_logger
-from railrl.samplers.data_collector import GoalConditionedPathCollector
-from railrl.torch.her.her import HERTrainer
-from railrl.torch.networks import FlattenMlp, TanhMlpPolicy
-# from railrl.torch.td3.td3 import TD3
-from railrl.demos.td3_bc import TD3BCTrainer
-from railrl.torch.torch_rl_algorithm import TorchBatchRLAlgorithm
-# from multiworld.envs.mujoco.sawyer_xyz.sawyer_push_multiobj_subset import SawyerMultiobjectEnv
-# from multiworld.envs.mujoco.sawyer_xyz.sawyer_reach import SawyerReachXYZEnv
-
-from multiworld.core.image_env import ImageEnv
-from multiworld.envs.real_world.sawyer.sawyer_reaching import SawyerReachXYZEnv
-# from sawyer_control.envs.sawyer_reaching import SawyerReachXYZEnv
-
-from railrl.launchers.arglauncher import run_variants
-import railrl.misc.hyperparameter as hyp
-
-# from railrl.launchers.experiments.ashvin.rfeatures.rfeatures_trainer import TimePredictionTrainer
-from railrl.launchers.experiments.ashvin.rfeatures.rfeatures_rl import encoder_wrapped_td3bc_experiment
-
-if __name__ == "__main__":
-    variant = dict(
-        env_class=SawyerReachXYZEnv,
-        env_kwargs=dict(
-            action_mode="position", 
-            max_speed = 0.05, 
-            camera="sawyer_head"
-        ),
-        # algo_kwargs=dict(
-        #     num_epochs=3000,
-        #     max_path_length=20,
-        #     batch_size=128,
-        #     num_eval_steps_per_epoch=1000,
-        #     num_expl_steps_per_train_loop=1000,
-        #     num_trains_per_train_loop=1000,
-        #     min_num_steps_before_training=1000,
-        # ),
-        algo_kwargs=dict(
-            num_epochs=500,
-            max_path_length=100,
-            batch_size=128,
-            num_eval_steps_per_epoch=500,
-            num_expl_steps_per_train_loop=500,
-            num_trains_per_train_loop=500,
-            min_num_steps_before_training=0,
-        ),
-        model_kwargs=dict(
-            decoder_distribution='gaussian_identity_variance',
-            input_channels=3,
-            imsize=224,
-            architecture=dict(
-                hidden_sizes=[200, 200],
-            ),
-            delta_features=True,
-            pretrained_features=False,
-        ),
-        trainer_kwargs=dict(
-            discount=0.99,
-            demo_path="/home/anair/ros_ws/src/railrl-private/demos/door_demos_v3/processed_demos_jittered1.pkl",
-            # demo_path="/home/anair/ros_ws/src/railrl-private/demos/door_demos_10_2/processed_demos_imagenet.pkl",
-            add_demo_latents=False, # already done
-            bc_num_pretrain_steps=1000,
-            rl_weight=0.0,
-            bc_weight=1.0,
-            weight_decay=0.001,
-        ),
-        replay_buffer_kwargs=dict(
-            max_size=100000,
-            fraction_goals_rollout_goals=1.0,
-            fraction_goals_env_goals=0.0,
-        ),
-        qf_kwargs=dict(
-            hidden_sizes=[400, 300],
-        ),
-        policy_kwargs=dict(
-            hidden_sizes=[128, 128],
-        ),
-
-        save_video=True,
-        dump_video_kwargs=dict(
-            save_period=1,
-            # imsize=(3, 500, 300),
-        ),
-        desired_trajectory="/home/anair/ros_ws/src/railrl-private/demos/door_demos_v3/demo_v3_0.pkl",
-
-        logger_variant=dict(
-            tensorboard=True,
-        ),
-        model_path="/home/anair/data/s3doodad/facebook/models/rfeatures/multitask1/run2/id2/itr_4000.pt",
-    )
-
-    search_space = {
-        'seedid': range(1),
-    }
-    sweeper = hyp.DeterministicHyperparameterSweeper(
-        search_space, default_parameters=variant,
-    )
-
-    variants = []
-    for variant in sweeper.iterate_hyperparameters():
-        variants.append(variant)
-
-    run_variants(encoder_wrapped_td3bc_experiment, variants, run_id=1)
diff --git a/experiments/ashvin/rfeatures/sawyer/door2/bc_v3_jittered2.py b/experiments/ashvin/rfeatures/sawyer/door2/bc_v3_jittered2.py
deleted file mode 100644
index 114422d..0000000
--- a/experiments/ashvin/rfeatures/sawyer/door2/bc_v3_jittered2.py
+++ /dev/null
@@ -1,111 +0,0 @@
-import gym
-
-import railrl.torch.pytorch_util as ptu
-from railrl.data_management.obs_dict_replay_buffer import ObsDictRelabelingBuffer
-from railrl.exploration_strategies.base import \
-    PolicyWrappedWithExplorationStrategy
-from railrl.exploration_strategies.gaussian_and_epislon import \
-    GaussianAndEpislonStrategy
-from railrl.launchers.launcher_util import setup_logger
-from railrl.samplers.data_collector import GoalConditionedPathCollector
-from railrl.torch.her.her import HERTrainer
-from railrl.torch.networks import FlattenMlp, TanhMlpPolicy
-# from railrl.torch.td3.td3 import TD3
-from railrl.demos.td3_bc import TD3BCTrainer
-from railrl.torch.torch_rl_algorithm import TorchBatchRLAlgorithm
-# from multiworld.envs.mujoco.sawyer_xyz.sawyer_push_multiobj_subset import SawyerMultiobjectEnv
-# from multiworld.envs.mujoco.sawyer_xyz.sawyer_reach import SawyerReachXYZEnv
-
-from multiworld.core.image_env import ImageEnv
-from multiworld.envs.real_world.sawyer.sawyer_reaching import SawyerReachXYZEnv
-# from sawyer_control.envs.sawyer_reaching import SawyerReachXYZEnv
-
-from railrl.launchers.arglauncher import run_variants
-import railrl.misc.hyperparameter as hyp
-
-# from railrl.launchers.experiments.ashvin.rfeatures.rfeatures_trainer import TimePredictionTrainer
-from railrl.launchers.experiments.ashvin.rfeatures.rfeatures_rl import encoder_wrapped_td3bc_experiment
-
-if __name__ == "__main__":
-    variant = dict(
-        env_class=SawyerReachXYZEnv,
-        env_kwargs=dict(
-            action_mode="position", 
-            max_speed = 0.05, 
-            camera="sawyer_head"
-        ),
-        # algo_kwargs=dict(
-        #     num_epochs=3000,
-        #     max_path_length=20,
-        #     batch_size=128,
-        #     num_eval_steps_per_epoch=1000,
-        #     num_expl_steps_per_train_loop=1000,
-        #     num_trains_per_train_loop=1000,
-        #     min_num_steps_before_training=1000,
-        # ),
-        algo_kwargs=dict(
-            num_epochs=500,
-            max_path_length=100,
-            batch_size=128,
-            num_eval_steps_per_epoch=500,
-            num_expl_steps_per_train_loop=500,
-            num_trains_per_train_loop=500,
-            min_num_steps_before_training=0,
-        ),
-        model_kwargs=dict(
-            decoder_distribution='gaussian_identity_variance',
-            input_channels=3,
-            imsize=224,
-            architecture=dict(
-                hidden_sizes=[200, 200],
-            ),
-            delta_features=True,
-            pretrained_features=False,
-        ),
-        trainer_kwargs=dict(
-            discount=0.99,
-            demo_path="/home/anair/ros_ws/src/railrl-private/demos/door_demos_v3/processed_demos_jitter2.pkl",
-            # demo_path="/home/anair/ros_ws/src/railrl-private/demos/door_demos_10_2/processed_demos_imagenet.pkl",
-            add_demo_latents=False, # already done
-            bc_num_pretrain_steps=1000,
-            rl_weight=0.0,
-            bc_weight=1.0,
-            weight_decay=0.01,
-        ),
-        replay_buffer_kwargs=dict(
-            max_size=100000,
-            fraction_goals_rollout_goals=1.0,
-            fraction_goals_env_goals=0.0,
-        ),
-        qf_kwargs=dict(
-            hidden_sizes=[400, 300],
-        ),
-        policy_kwargs=dict(
-            hidden_sizes=[32, 32],
-        ),
-
-        save_video=True,
-        dump_video_kwargs=dict(
-            save_period=1,
-            # imsize=(3, 500, 300),
-        ),
-        desired_trajectory="/home/anair/ros_ws/src/railrl-private/demos/door_demos_v3/demo_v3_0.pkl",
-
-        logger_variant=dict(
-            tensorboard=True,
-        ),
-        model_path="/home/anair/data/s3doodad/facebook/models/rfeatures/multitask1/run2/id2/itr_4000.pt",
-    )
-
-    search_space = {
-        'seedid': range(1),
-    }
-    sweeper = hyp.DeterministicHyperparameterSweeper(
-        search_space, default_parameters=variant,
-    )
-
-    variants = []
-    for variant in sweeper.iterate_hyperparameters():
-        variants.append(variant)
-
-    run_variants(encoder_wrapped_td3bc_experiment, variants, run_id=1)
diff --git a/experiments/ashvin/rfeatures/sawyer/door2/bc_v3_varied1.py b/experiments/ashvin/rfeatures/sawyer/door2/bc_v3_varied1.py
deleted file mode 100644
index 0146d50..0000000
--- a/experiments/ashvin/rfeatures/sawyer/door2/bc_v3_varied1.py
+++ /dev/null
@@ -1,133 +0,0 @@
-import gym
-
-import railrl.torch.pytorch_util as ptu
-from railrl.data_management.obs_dict_replay_buffer import ObsDictRelabelingBuffer
-from railrl.exploration_strategies.base import \
-    PolicyWrappedWithExplorationStrategy
-from railrl.exploration_strategies.gaussian_and_epislon import \
-    GaussianAndEpislonStrategy
-from railrl.launchers.launcher_util import setup_logger
-from railrl.samplers.data_collector import GoalConditionedPathCollector
-from railrl.torch.her.her import HERTrainer
-from railrl.torch.networks import FlattenMlp, TanhMlpPolicy
-# from railrl.torch.td3.td3 import TD3
-from railrl.demos.td3_bc import TD3BCTrainer
-from railrl.torch.torch_rl_algorithm import TorchBatchRLAlgorithm
-# from multiworld.envs.mujoco.sawyer_xyz.sawyer_push_multiobj_subset import SawyerMultiobjectEnv
-# from multiworld.envs.mujoco.sawyer_xyz.sawyer_reach import SawyerReachXYZEnv
-
-from multiworld.core.image_env import ImageEnv
-from multiworld.envs.real_world.sawyer.sawyer_reaching import SawyerReachXYZEnv
-# from sawyer_control.envs.sawyer_reaching import SawyerReachXYZEnv
-
-from railrl.launchers.arglauncher import run_variants
-import railrl.misc.hyperparameter as hyp
-
-# from railrl.launchers.experiments.ashvin.rfeatures.rfeatures_trainer import TimePredictionTrainer
-from railrl.launchers.experiments.ashvin.rfeatures.rfeatures_rl import encoder_wrapped_td3bc_experiment
-
-if __name__ == "__main__":
-    colors = ["grey", "beige", "green", "brownhatch"]
-    # colors = ["grey"]
-    # demo_path = ["/home/anair/ros_ws/src/railrl-private/demos/door_demos_v3/processed_demos_%s_latent_distance__use_goal_from_trajectory_jitter2.pkl" % color for color in colors]
-    demo_path = ["/home/anair/ros_ws/src/railrl-private/demos/door_demos_v3/processed_demos_%s_latent_distance_use_initial_use_initial_from_trajectory_use_goal_from_trajectory_jitter2.pkl" % color for color in colors]
-    variant = dict(
-        env_class=SawyerReachXYZEnv,
-        env_kwargs=dict(
-            action_mode="position", 
-            max_speed = 0.05, 
-            camera="sawyer_head"
-        ),
-        # algo_kwargs=dict(
-        #     num_epochs=3000,
-        #     max_path_length=20,
-        #     batch_size=128,
-        #     num_eval_steps_per_epoch=1000,
-        #     num_expl_steps_per_train_loop=1000,
-        #     num_trains_per_train_loop=1000,
-        #     min_num_steps_before_training=1000,
-        # ),
-        algo_kwargs=dict(
-            num_epochs=500,
-            max_path_length=100,
-            batch_size=128,
-            num_eval_steps_per_epoch=500,
-            num_expl_steps_per_train_loop=500,
-            num_trains_per_train_loop=500,
-            min_num_steps_before_training=0,
-        ),
-        # config_params = dict(
-        #     initial_type="",
-        #     goal_type="",
-        #     use_initial=False
-        # ),
-        config_params = dict(
-            initial_type="use_initial_from_trajectory",
-            goal_type="",
-            use_initial=True
-        ),
-        reward_params_type="latent_distance",
-        model_kwargs=dict(
-            decoder_distribution='gaussian_identity_variance',
-            input_channels=3,
-            imsize=224,
-            architecture=dict(
-                hidden_sizes=[200, 200],
-            ),
-            delta_features=True,
-            pretrained_features=False,
-        ),
-        trainer_kwargs=dict(
-            discount=0.99,
-            demo_path=demo_path,
-            # demo_path="/home/anair/ros_ws/src/railrl-private/demos/door_demos_10_2/processed_demos_imagenet.pkl",
-            add_demo_latents=False, # already done
-            bc_num_pretrain_steps=10000,
-            rl_weight=0.0,
-            bc_weight=1.0,
-            weight_decay=0.001,
-        ),
-        replay_buffer_kwargs=dict(
-            max_size=1000000,
-            fraction_goals_rollout_goals=1.0,
-            fraction_goals_env_goals=0.0,
-        ),
-        qf_kwargs=dict(
-            hidden_sizes=[400, 300],
-        ),
-        policy_kwargs=dict(
-            hidden_sizes=[128, 128],
-        ),
-
-        save_video=True,
-        save_period=1,
-        dump_video_kwargs=dict(
-            imwidth=500,
-            imheight=300,
-            num_imgs=1,
-            dump_pickle=True,
-            exploration_goal_image_key="image_observation",
-            evaluation_goal_image_key="image_observation",
-            rows=1,
-            columns=1,
-        ),
-        desired_trajectory="/home/anair/ros_ws/src/railrl-private/demos/door_demos_v3/demo_v3_grey_0.pkl",
-
-        logger_variant=dict(
-            tensorboard=True,
-        ),
-        model_path="/home/anair/data/s3doodad/facebook/models/rfeatures/multitask1/run2/id2/itr_4000.pt",
-    )
-
-    search_space = {
-        'seedid': range(1),
-    }
-    sweeper = hyp.DeterministicHyperparameterSweeper(
-        search_space, default_parameters=variant,
-    )
-
-    variants = []
-    for variant in sweeper.iterate_hyperparameters():
-        variants.append(variant)
-
-    run_variants(encoder_wrapped_td3bc_experiment, variants, run_id=29)
diff --git a/experiments/ashvin/rfeatures/sawyer/door2/bc_v3_varied_imagenet1.py b/experiments/ashvin/rfeatures/sawyer/door2/bc_v3_varied_imagenet1.py
deleted file mode 100644
index 3d4764f..0000000
--- a/experiments/ashvin/rfeatures/sawyer/door2/bc_v3_varied_imagenet1.py
+++ /dev/null
@@ -1,114 +0,0 @@
-import gym
-
-import railrl.torch.pytorch_util as ptu
-from railrl.data_management.obs_dict_replay_buffer import ObsDictRelabelingBuffer
-from railrl.exploration_strategies.base import \
-    PolicyWrappedWithExplorationStrategy
-from railrl.exploration_strategies.gaussian_and_epislon import \
-    GaussianAndEpislonStrategy
-from railrl.launchers.launcher_util import setup_logger
-from railrl.samplers.data_collector import GoalConditionedPathCollector
-from railrl.torch.her.her import HERTrainer
-from railrl.torch.networks import FlattenMlp, TanhMlpPolicy
-# from railrl.torch.td3.td3 import TD3
-from railrl.demos.td3_bc import TD3BCTrainer
-from railrl.torch.torch_rl_algorithm import TorchBatchRLAlgorithm
-# from multiworld.envs.mujoco.sawyer_xyz.sawyer_push_multiobj_subset import SawyerMultiobjectEnv
-# from multiworld.envs.mujoco.sawyer_xyz.sawyer_reach import SawyerReachXYZEnv
-
-from multiworld.core.image_env import ImageEnv
-from multiworld.envs.real_world.sawyer.sawyer_reaching import SawyerReachXYZEnv
-# from sawyer_control.envs.sawyer_reaching import SawyerReachXYZEnv
-
-from railrl.launchers.arglauncher import run_variants
-import railrl.misc.hyperparameter as hyp
-
-# from railrl.launchers.experiments.ashvin.rfeatures.rfeatures_trainer import TimePredictionTrainer
-from railrl.launchers.experiments.ashvin.rfeatures.rfeatures_rl import encoder_wrapped_td3bc_experiment
-
-if __name__ == "__main__":
-    demo_path = ["/home/anair/ros_ws/src/railrl-private/demos/door_demos_v3/processed_demos_%s_jitter2.pkl" % color for color in ["grey", "beige", "green", "brownhatch"]]
-
-    variant = dict(
-        env_class=SawyerReachXYZEnv,
-        env_kwargs=dict(
-            action_mode="position", 
-            max_speed = 0.05, 
-            camera="sawyer_head"
-        ),
-        # algo_kwargs=dict(
-        #     num_epochs=3000,
-        #     max_path_length=20,
-        #     batch_size=128,
-        #     num_eval_steps_per_epoch=1000,
-        #     num_expl_steps_per_train_loop=1000,
-        #     num_trains_per_train_loop=1000,
-        #     min_num_steps_before_training=1000,
-        # ),
-        algo_kwargs=dict(
-            num_epochs=500,
-            max_path_length=100,
-            batch_size=128,
-            num_eval_steps_per_epoch=500,
-            num_expl_steps_per_train_loop=500,
-            num_trains_per_train_loop=500,
-            min_num_steps_before_training=0,
-        ),
-        model_kwargs=dict(
-            decoder_distribution='gaussian_identity_variance',
-            input_channels=3,
-            imsize=224,
-            architecture=dict(
-                hidden_sizes=[200, 200],
-            ),
-            delta_features=True,
-            pretrained_features=False,
-        ),
-        trainer_kwargs=dict(
-            discount=0.99,
-            demo_path=demo_path,
-            # demo_path="/home/anair/ros_ws/src/railrl-private/demos/door_demos_10_2/processed_demos_imagenet.pkl",
-            add_demo_latents=False, # already done
-            bc_num_pretrain_steps=10000,
-            rl_weight=0.0,
-            bc_weight=1.0,
-            weight_decay=0.001,
-        ),
-        replay_buffer_kwargs=dict(
-            max_size=1000000,
-            fraction_goals_rollout_goals=1.0,
-            fraction_goals_env_goals=0.0,
-        ),
-        qf_kwargs=dict(
-            hidden_sizes=[400, 300],
-        ),
-        policy_kwargs=dict(
-            hidden_sizes=[128, 128],
-        ),
-
-        save_video=True,
-        dump_video_kwargs=dict(
-            save_period=1,
-            # imsize=(3, 500, 300),
-        ),
-        desired_trajectory="/home/anair/ros_ws/src/railrl-private/demos/door_demos_v3/demo_v3_beige_0.pkl",
-
-        logger_variant=dict(
-            tensorboard=True,
-        ),
-        # model_path="/home/anair/data/s3doodad/facebook/models/rfeatures/multitask1/run2/id2/itr_4000.pt",
-        model_path="/home/anair/data/s3doodad/facebook/models/rfeatures/multitask1/run2/id0/itr_0.pt",
-    )
-
-    search_space = {
-        'seedid': range(1),
-    }
-    sweeper = hyp.DeterministicHyperparameterSweeper(
-        search_space, default_parameters=variant,
-    )
-
-    variants = []
-    for variant in sweeper.iterate_hyperparameters():
-        variants.append(variant)
-
-    run_variants(encoder_wrapped_td3bc_experiment, variants, run_id=4)
diff --git a/experiments/ashvin/rfeatures/sawyer/door2/bc_v3_varied_imagenet2.py b/experiments/ashvin/rfeatures/sawyer/door2/bc_v3_varied_imagenet2.py
deleted file mode 100644
index 273bf42..0000000
--- a/experiments/ashvin/rfeatures/sawyer/door2/bc_v3_varied_imagenet2.py
+++ /dev/null
@@ -1,114 +0,0 @@
-import gym
-
-import railrl.torch.pytorch_util as ptu
-from railrl.data_management.obs_dict_replay_buffer import ObsDictRelabelingBuffer
-from railrl.exploration_strategies.base import \
-    PolicyWrappedWithExplorationStrategy
-from railrl.exploration_strategies.gaussian_and_epislon import \
-    GaussianAndEpislonStrategy
-from railrl.launchers.launcher_util import setup_logger
-from railrl.samplers.data_collector import GoalConditionedPathCollector
-from railrl.torch.her.her import HERTrainer
-from railrl.torch.networks import FlattenMlp, TanhMlpPolicy
-# from railrl.torch.td3.td3 import TD3
-from railrl.demos.td3_bc import TD3BCTrainer
-from railrl.torch.torch_rl_algorithm import TorchBatchRLAlgorithm
-# from multiworld.envs.mujoco.sawyer_xyz.sawyer_push_multiobj_subset import SawyerMultiobjectEnv
-# from multiworld.envs.mujoco.sawyer_xyz.sawyer_reach import SawyerReachXYZEnv
-
-from multiworld.core.image_env import ImageEnv
-from multiworld.envs.real_world.sawyer.sawyer_reaching import SawyerReachXYZEnv
-# from sawyer_control.envs.sawyer_reaching import SawyerReachXYZEnv
-
-from railrl.launchers.arglauncher import run_variants
-import railrl.misc.hyperparameter as hyp
-
-# from railrl.launchers.experiments.ashvin.rfeatures.rfeatures_trainer import TimePredictionTrainer
-from railrl.launchers.experiments.ashvin.rfeatures.rfeatures_rl import encoder_wrapped_td3bc_experiment
-
-if __name__ == "__main__":
-    demo_path = ["/home/anair/ros_ws/src/railrl-private/demos/door_demos_v3/processed_demos_%s_imagenet_jitter2.pkl" % color for color in ["grey", "beige", "green", "brownhatch"]]
-
-    variant = dict(
-        env_class=SawyerReachXYZEnv,
-        env_kwargs=dict(
-            action_mode="position", 
-            max_speed = 0.05, 
-            camera="sawyer_head"
-        ),
-        # algo_kwargs=dict(
-        #     num_epochs=3000,
-        #     max_path_length=20,
-        #     batch_size=128,
-        #     num_eval_steps_per_epoch=1000,
-        #     num_expl_steps_per_train_loop=1000,
-        #     num_trains_per_train_loop=1000,
-        #     min_num_steps_before_training=1000,
-        # ),
-        algo_kwargs=dict(
-            num_epochs=500,
-            max_path_length=100,
-            batch_size=128,
-            num_eval_steps_per_epoch=500,
-            num_expl_steps_per_train_loop=500,
-            num_trains_per_train_loop=500,
-            min_num_steps_before_training=0,
-        ),
-        model_kwargs=dict(
-            decoder_distribution='gaussian_identity_variance',
-            input_channels=3,
-            imsize=224,
-            architecture=dict(
-                hidden_sizes=[200, 200],
-            ),
-            delta_features=True,
-            pretrained_features=False,
-        ),
-        trainer_kwargs=dict(
-            discount=0.99,
-            demo_path=demo_path,
-            # demo_path="/home/anair/ros_ws/src/railrl-private/demos/door_demos_10_2/processed_demos_imagenet.pkl",
-            add_demo_latents=False, # already done
-            bc_num_pretrain_steps=10000,
-            rl_weight=0.0,
-            bc_weight=1.0,
-            weight_decay=0.001,
-        ),
-        replay_buffer_kwargs=dict(
-            max_size=1000000,
-            fraction_goals_rollout_goals=1.0,
-            fraction_goals_env_goals=0.0,
-        ),
-        qf_kwargs=dict(
-            hidden_sizes=[400, 300],
-        ),
-        policy_kwargs=dict(
-            hidden_sizes=[128, 128],
-        ),
-
-        save_video=True,
-        dump_video_kwargs=dict(
-            save_period=1,
-            # imsize=(3, 500, 300),
-        ),
-        desired_trajectory="/home/anair/ros_ws/src/railrl-private/demos/door_demos_v3/demo_v3_beige_0.pkl",
-
-        logger_variant=dict(
-            tensorboard=True,
-        ),
-        # model_path="/home/anair/data/s3doodad/facebook/models/rfeatures/multitask1/run2/id2/itr_4000.pt",
-        model_path="/home/anair/data/s3doodad/facebook/models/rfeatures/multitask1/run2/id0/itr_0.pt",
-    )
-
-    search_space = {
-        'seedid': range(1),
-    }
-    sweeper = hyp.DeterministicHyperparameterSweeper(
-        search_space, default_parameters=variant,
-    )
-
-    variants = []
-    for variant in sweeper.iterate_hyperparameters():
-        variants.append(variant)
-
-    run_variants(encoder_wrapped_td3bc_experiment, variants, run_id=2)
diff --git a/experiments/ashvin/rfeatures/sawyer/door2/td3bc1.py b/experiments/ashvin/rfeatures/sawyer/door2/td3bc1.py
deleted file mode 100644
index 5cd5510..0000000
--- a/experiments/ashvin/rfeatures/sawyer/door2/td3bc1.py
+++ /dev/null
@@ -1,137 +0,0 @@
-import gym
-
-import railrl.torch.pytorch_util as ptu
-from railrl.data_management.obs_dict_replay_buffer import ObsDictRelabelingBuffer
-from railrl.exploration_strategies.base import \
-    PolicyWrappedWithExplorationStrategy
-from railrl.exploration_strategies.gaussian_and_epislon import \
-    GaussianAndEpislonStrategy
-from railrl.launchers.launcher_util import setup_logger
-from railrl.samplers.data_collector import GoalConditionedPathCollector
-from railrl.torch.her.her import HERTrainer
-from railrl.torch.networks import FlattenMlp, TanhMlpPolicy
-# from railrl.torch.td3.td3 import TD3
-from railrl.demos.td3_bc import TD3BCTrainer
-from railrl.torch.torch_rl_algorithm import TorchBatchRLAlgorithm
-# from multiworld.envs.mujoco.sawyer_xyz.sawyer_push_multiobj_subset import SawyerMultiobjectEnv
-# from multiworld.envs.mujoco.sawyer_xyz.sawyer_reach import SawyerReachXYZEnv
-
-from multiworld.core.image_env import ImageEnv
-from multiworld.envs.real_world.sawyer.sawyer_reaching import SawyerReachXYZEnv
-# from sawyer_control.envs.sawyer_reaching import SawyerReachXYZEnv
-
-from railrl.launchers.arglauncher import run_variants
-import railrl.misc.hyperparameter as hyp
-
-# from railrl.launchers.experiments.ashvin.rfeatures.rfeatures_trainer import TimePredictionTrainer
-from railrl.launchers.experiments.ashvin.rfeatures.rfeatures_rl import encoder_wrapped_td3bc_experiment
-
-if __name__ == "__main__":
-    colors = ["grey", "beige", "green", "brownhatch"]
-    # colors = ["grey"]
-    demo_path = ["/home/anair/ros_ws/src/railrl-private/demos/door_demos_v3/processed_demos_%s_latent_distance_jitter2.pkl" % color for color in colors]
-    demo_off_policy_path = ["/home/anair/data/s3doodad/ashvin/rfeatures/sawyer/door2/bc-v3-varied1/run%s/id0/video_0_env.p" % str(i) for i in [0, 1]]
-    demo_off_policy_path = demo_off_policy_path + [
-        "/home/anair/data/s3doodad/ashvin/rfeatures/sawyer/door2/td3bc1/run16/id0/video_0_env.p",
-        "/home/anair/data/s3doodad/ashvin/rfeatures/sawyer/door2/td3bc1/run16/id0/video_1_env.p",
-        "/home/anair/data/s3doodad/ashvin/rfeatures/sawyer/door2/td3bc1/run16/id0/video_0_vae.p",
-        "/home/anair/data/s3doodad/ashvin/rfeatures/sawyer/door2/td3bc1/run16/id0/video_1_vae.p",
-        "/home/anair/data/s3doodad/ashvin/rfeatures/sawyer/door2/td3bc1/run17/id0/video_0_env.p",
-        "/home/anair/data/s3doodad/ashvin/rfeatures/sawyer/door2/td3bc1/run17/id0/video_0_vae.p",
-        "/home/anair/data/s3doodad/ashvin/rfeatures/sawyer/door2/td3bc1/run19/id0/video_0_env.p",
-        "/home/anair/data/s3doodad/ashvin/rfeatures/sawyer/door2/td3bc1/run19/id0/video_0_vae.p"]
-    print(demo_off_policy_path)
-    variant = dict(
-        env_class=SawyerReachXYZEnv,
-        env_kwargs=dict(
-            action_mode="position", 
-            max_speed = 0.05, 
-            camera="sawyer_head"
-        ),
-        # algo_kwargs=dict(
-        #     num_epochs=3000,
-        #     max_path_length=20,
-        #     batch_size=128,
-        #     num_eval_steps_per_epoch=1000,
-        #     num_expl_steps_per_train_loop=1000,
-        #     num_trains_per_train_loop=1000,
-        #     min_num_steps_before_training=1000,
-        # ),
-        algo_kwargs=dict(
-            num_epochs=500,
-            max_path_length=100,
-            batch_size=128,
-            num_eval_steps_per_epoch=500,
-            num_expl_steps_per_train_loop=500,
-            num_trains_per_train_loop=500,
-            min_num_steps_before_training=0,
-        ),
-        config_params = dict(
-            initial_type="",
-            # initial_type="use_initial_from_trajectory",
-            # goal_type="use_goal_from_trajectory",
-            goal_type="",
-            use_initial=False
-        ),
-        reward_params_type="latent_distance",
-        model_kwargs=dict(
-            decoder_distribution='gaussian_identity_variance',
-            input_channels=3,
-            imsize=224,
-            architecture=dict(
-                hidden_sizes=[200, 200],
-            ),
-            delta_features=True,
-            pretrained_features=False,
-        ),
-        trainer_kwargs=dict(
-            discount=0.99,
-            demo_path=demo_path,
-            demo_off_policy_path=demo_off_policy_path,
-            # demo_path="/home/anair/ros_ws/src/railrl-private/demos/door_demos_10_2/processed_demos_imagenet.pkl",
-            add_demo_latents=False, # already done
-            bc_num_pretrain_steps=10000,
-            q_num_pretrain_steps=10000,
-            rl_weight=100.0,
-            # rl_weight=.0,
-            bc_weight=1.0,
-            reward_scale=0.000001,
-            weight_decay=0.001,
-        ),
-        replay_buffer_kwargs=dict(
-            max_size=1000000,
-            fraction_goals_rollout_goals=1.0,
-            fraction_goals_env_goals=0.0,
-        ),
-        qf_kwargs=dict(
-            hidden_sizes=[400, 300],
-        ),
-        policy_kwargs=dict(
-            hidden_sizes=[128, 128],
-        ),
-
-        save_video=True,
-        dump_video_kwargs=dict(
-            save_period=1,
-            # imsize=(3, 500, 300),
-        ),
-        desired_trajectory="/home/anair/ros_ws/src/railrl-private/demos/door_demos_v3/demo_v3_grey_0.pkl",
-
-        logger_variant=dict(
-            tensorboard=True,
-        ),
-        model_path="/home/anair/data/s3doodad/facebook/models/rfeatures/multitask1/run2/id2/itr_4000.pt",
-    )
-
-    search_space = {
-        'seedid': range(1),
-    }
-    sweeper = hyp.DeterministicHyperparameterSweeper(
-        search_space, default_parameters=variant,
-    )
-
-    variants = []
-    for variant in sweeper.iterate_hyperparameters():
-        variants.append(variant)
-
-    run_variants(encoder_wrapped_td3bc_experiment, variants, run_id=20)
diff --git a/experiments/ashvin/rfeatures/sawyer/door_spacemouse_demos.py b/experiments/ashvin/rfeatures/sawyer/door_spacemouse_demos.py
deleted file mode 100644
index 3891448..0000000
--- a/experiments/ashvin/rfeatures/sawyer/door_spacemouse_demos.py
+++ /dev/null
@@ -1,65 +0,0 @@
-from railrl.demos.collect_demo import collect_demos_fixed
-from railrl.demos.spacemouse.input_server import SpaceMouseExpert
-
-from multiworld.core.image_env import ImageEnv
-# from multiworld.envs.mujoco.cameras import sawyer_pusher_camera_upright_v2
-
-# from multiworld.envs.mujoco.sawyer_xyz.sawyer_push_multiobj import SawyerMultiobjectEnv
-# from multiworld.envs.pygame.point2d import Point2DWallEnv
-
-import gym
-import numpy as np
-
-# from sawyer_control.envs.sawyer_reaching import SawyerReachXYZEnv
-from multiworld.envs.real_world.sawyer.sawyer_reaching import SawyerReachXYZEnv
-
-import time
-import rospy
-
-# from sawyer_control.envs.sawyer_insertion_refined_USB_sparse_RLonly import SawyerHumanControlEnv
-
-if __name__ == '__main__':
-    scale = 1.0
-    expert = SpaceMouseExpert(
-        xyz_dims=3,
-        xyz_remap=[0, 1, 2],
-        xyz_scale=[-scale, -scale, scale],
-    )
-
-    # env = gym.make("MountainCarContinuous-v0")
-    # env = SawyerHumanControlEnv(action_mode='joint_space_impd', position_action_scale=1, max_speed=0.015)
-    env = SawyerReachXYZEnv(action_mode="position", max_speed = 0.05, camera="sawyer_head")
-
-    env = ImageEnv(env,
-        recompute_reward=False,
-        transpose=True,
-        image_length=450000,
-        reward_type="image_distance",
-        # init_camera=sawyer_pusher_camera_upright_v2,
-    )
-
-    # env.reset()
-
-    for i in range(10):
-        collect_demos_fixed(env, expert, "/home/anair/ros_ws/src/railrl-private/demos/demo_v3_brownhatch_%i.pkl" %i, 1, horizon=1000, pause=0.05)
-
-    # for i in range(10):
-    # collect_demos_fixed(env, expert, "demos/demo_v3.pkl", 1, horizon=1000, pause=0.05)
-
-    # o = None
-    # while True:
-    #     a, valid, reset, accept = expert.get_action(o)
-
-    #     if valid:
-    #         o, r, done, info = env.step(a)
-    #         time.sleep(0.05)
-
-    #     if reset or accept:
-    #         env.reset()
-
-    #     if rospy.is_shutdown():
-    #         break
-
-    #     time.sleep(0.01)
-
-    # exit()
\ No newline at end of file
diff --git a/experiments/ashvin/rfeatures/sawyer/reset.p b/experiments/ashvin/rfeatures/sawyer/reset.p
deleted file mode 100644
index e0f2609..0000000
--- a/experiments/ashvin/rfeatures/sawyer/reset.p
+++ /dev/null
@@ -1,80 +0,0 @@
-(dp0
-S'joints'
-p1
-(dp2
-S'right_j6'
-p3
-F-3.0337265625
-sS'right_j5'
-p4
-F0.9441103515625
-sS'right_j4'
-p5
-F1.55927734375
-sS'right_j3'
-p6
-F2.0156728515625
-sS'right_j2'
-p7
-F-0.99840625
-sS'right_j1'
-p8
-F-0.1582705078125
-sS'right_j0'
-p9
-F0.24664453125
-ssS'end_eff_xyz'
-p10
-cnumpy.core.multiarray
-_reconstruct
-p11
-(cnumpy
-ndarray
-p12
-(I0
-tp13
-S'b'
-p14
-tp15
-Rp16
-(I1
-(I3
-tp17
-cnumpy
-dtype
-p18
-(S'f8'
-p19
-I0
-I1
-tp20
-Rp21
-(I3
-S'<'
-p22
-NNNI-1
-I-1
-I0
-tp23
-bI00
-S'y\xed{\x1b\xf0p\xde?CO\xa9\t\x04\xff\xc1\xbf\x9c8\xa0\x94\xa3\xbc\xc3?'
-p24
-tp25
-bsS'end_eff_quat'
-p26
-g11
-(g12
-(I0
-tp27
-g14
-tp28
-Rp29
-(I1
-(I4
-tp30
-g21
-I00
-S'j\xdf\xbeIb\xf1\xef?\xad\xd8\xc6\x9d\x0e\x94\x99\xbf{\xf8\x87\x17\xfcQ\xaa\xbf\xe2F\xff\xfe\xd1\xb1\x91?'
-p31
-tp32
-bs.
\ No newline at end of file
diff --git a/experiments/ashvin/rfeatures/sawyer/spacemouse_robot.py b/experiments/ashvin/rfeatures/sawyer/spacemouse_robot.py
deleted file mode 100644
index a29127c..0000000
--- a/experiments/ashvin/rfeatures/sawyer/spacemouse_robot.py
+++ /dev/null
@@ -1,64 +0,0 @@
-from railrl.demos.collect_demo import collect_demos_fixed
-from railrl.demos.spacemouse.input_server import SpaceMouseExpert
-
-from multiworld.core.image_env import ImageEnv
-# from multiworld.envs.mujoco.cameras import sawyer_pusher_camera_upright_v2
-
-# from multiworld.envs.mujoco.sawyer_xyz.sawyer_push_multiobj import SawyerMultiobjectEnv
-# from multiworld.envs.pygame.point2d import Point2DWallEnv
-
-import gym
-import numpy as np
-
-# from sawyer_control.envs.sawyer_reaching import SawyerReachXYZEnv
-from multiworld.envs.real_world.sawyer.sawyer_reaching import SawyerReachXYZEnv
-
-import time
-import rospy
-
-# from sawyer_control.envs.sawyer_insertion_refined_USB_sparse_RLonly import SawyerHumanControlEnv
-
-if __name__ == '__main__':
-    scale = 0.1
-    expert = SpaceMouseExpert(
-        xyz_dims=3,
-        xyz_remap=[0, 1, 2],
-        xyz_scale=[-scale, -scale, scale],
-    )
-
-    # env = gym.make("MountainCarContinuous-v0")
-    # env = SawyerHumanControlEnv(action_mode='joint_space_impd', position_action_scale=1, max_speed=0.015)
-    env = SawyerReachXYZEnv(action_mode="position", max_speed = 0.05, camera="sawyer_head")
-
-    # env = SawyerMultiobjectEnv(
-    #     num_objects=1,
-    #     preload_obj_dict=[
-    #         dict(color2=(0.1, 0.1, 0.9)),
-    #     ],
-    # )
-    env = ImageEnv(env,
-        recompute_reward=False,
-        transpose=True,
-        image_length=3072000,
-        # init_camera=sawyer_pusher_camera_upright_v2,
-    )
-
-    # env.reset()
-
-    o = None
-    while True:
-        a, valid, reset, accept = expert.get_action(o)
-
-        if valid:
-            o, r, done, info = env.step(a)
-            time.sleep(0.05)
-
-        if reset or accept:
-            env.reset()
-
-        if rospy.is_shutdown():
-            break
-
-        time.sleep(0.01)
-
-    exit()
\ No newline at end of file
diff --git a/experiments/graham/Graham_Notes.txt b/experiments/graham/Graham_Notes.txt
deleted file mode 100644
index b51e83d..0000000
--- a/experiments/graham/Graham_Notes.txt
+++ /dev/null
@@ -1,19 +0,0 @@
-Pro tips:
-- When CUDA stops working, just restart the computer :(
-
-
-Processing Demos:
-- saw
-- source activate railr-env
-- cd src/railrl-private
-- python scripts/update_demo_with_latents.py
-- Make sure to edit variables in this file to use imagenet/not imagenet, and to save the correct path for the processed pkl file
-
-Running Behavioral Cloning:
-- saw
-- source activate railr-env
-- cd src/railrl-private
-- python experiments/ashvin/rfeatures/sawyer/door2/bc1.py --local --gpu --1
-- Make sure to edit vars in this file to use the correct model/processed demo files
-
-
diff --git a/experiments/murtaza/multiworld/skew_fit/door/generate_uniform_dataset.py b/experiments/murtaza/multiworld/skew_fit/door/generate_uniform_dataset.py
index 7bb817a..1ae6a64 100644
--- a/experiments/murtaza/multiworld/skew_fit/door/generate_uniform_dataset.py
+++ b/experiments/murtaza/multiworld/skew_fit/door/generate_uniform_dataset.py
@@ -1,5 +1,9 @@
 import os.path as osp
+
+import sys
+sys.path.remove('/opt/ros/kinetic/lib/python2.7/dist-packages')
 import cv2
+
 import numpy as np
 
 from multiworld.core.image_env import unormalize_image, ImageEnv
diff --git a/experiments/murtaza/multiworld/skew_fit/reacher/generate_uniform_dataset.py b/experiments/murtaza/multiworld/skew_fit/reacher/generate_uniform_dataset.py
index 728fecc..b7fadfc 100644
--- a/experiments/murtaza/multiworld/skew_fit/reacher/generate_uniform_dataset.py
+++ b/experiments/murtaza/multiworld/skew_fit/reacher/generate_uniform_dataset.py
@@ -1,5 +1,9 @@
 import os.path as osp
+
+import sys
+sys.path.remove('/opt/ros/kinetic/lib/python2.7/dist-packages')
 import cv2
+
 import numpy as np
 from multiworld.core.image_env import unormalize_image, ImageEnv
 from railrl.misc.asset_loader import load_local_or_remote_file
diff --git a/experiments/sasha/cond_rig/hyp_tuning/tuning.py b/experiments/sasha/cond_rig/hyp_tuning/tuning.py
new file mode 100644
index 0000000..7b8edb1
--- /dev/null
+++ b/experiments/sasha/cond_rig/hyp_tuning/tuning.py
@@ -0,0 +1,216 @@
+import railrl.misc.hyperparameter as hyp
+from experiments.murtaza.multiworld.skew_fit.reacher.generate_uniform_dataset import generate_uniform_dataset_reacher
+from multiworld.envs.mujoco.cameras import sawyer_init_camera_zoomed_in, sawyer_pusher_camera_upright_v2
+from railrl.launchers.launcher_util import run_experiment
+import railrl.torch.vae.vae_schedules as vae_schedules
+from railrl.torch.vae.conv_vae import imsize48_default_architecture, imsize48_default_architecture_with_more_hidden_layers
+from railrl.launchers.arglauncher import run_variants
+from railrl.torch.grill.cvae_experiments import (
+    grill_her_td3_offpolicy_online_vae_full_experiment,
+)
+from multiworld.envs.pygame.multiobject_pygame_env import Multiobj2DEnv
+from multiworld.envs.mujoco.sawyer_xyz.sawyer_push_multiobj_subset import SawyerMultiobjectEnv
+from railrl.torch.vae.conditional_conv_vae import DeltaCVAE
+from railrl.torch.vae.conditional_vae_trainer import DeltaCVAETrainer
+from railrl.data_management.online_conditional_vae_replay_buffer import \
+        OnlineConditionalVaeRelabelingBuffer
+
+x_var = 0.2
+x_low = -x_var
+x_high = x_var
+y_low = 0.5
+y_high = 0.7
+t = 0.05
+
+if __name__ == "__main__":
+    variant = dict(
+        double_algo=False,
+        online_vae_exploration=False,
+        imsize=48,
+        init_camera=sawyer_init_camera_zoomed_in,
+        env_class=SawyerMultiobjectEnv,
+        env_kwargs=dict(
+            num_objects=1,
+            object_meshes=None,
+            fixed_start=True,
+            num_scene_objects=[1],
+            maxlen=0.1,
+            action_repeat=1,
+            puck_goal_low=(x_low + 0.01, y_low + 0.01),
+            puck_goal_high=(x_high - 0.01, y_high - 0.01),
+            hand_goal_low=(x_low + 3*t, y_low + t),
+            hand_goal_high=(x_high - 3*t, y_high -t),
+            mocap_low=(x_low + 2*t, y_low , 0.0),
+            mocap_high=(x_high - 2*t, y_high, 0.5),
+            object_low=(x_low + 0.01, y_low + 0.01, 0.02),
+            object_high=(x_high - 0.01, y_high - 0.01, 0.02),
+            use_textures=True,
+            init_camera=sawyer_init_camera_zoomed_in,
+        ),
+
+        grill_variant=dict(
+            save_video=True,
+            custom_goal_sampler='replay_buffer',
+            online_vae_trainer_kwargs=dict(
+                beta=20,
+                lr=0,
+            ),
+            save_video_period=10,
+            qf_kwargs=dict(
+                hidden_sizes=[400, 300],
+            ),
+            policy_kwargs=dict(
+                hidden_sizes=[400, 300],
+            ),
+            vf_kwargs=dict(
+                hidden_sizes=[400, 300],
+            ),
+            max_path_length=100,
+            algo_kwargs=dict(
+                batch_size=128,
+                num_epochs=1000,
+                num_eval_steps_per_epoch=1000,
+                num_expl_steps_per_train_loop=1000,
+                num_trains_per_train_loop=1000,
+                min_num_steps_before_training=4000,
+                vae_training_schedule=vae_schedules.never_train,
+                oracle_data=False,
+                vae_save_period=25,
+                parallel_vae_train=False,
+                dataset_path=None,
+                rl_offpolicy_num_training_steps=0,
+            ),
+            td3_trainer_kwargs=dict(
+                discount=0.99,
+                reward_scale=1.0,
+                tau=1e-2,
+            ),
+            replay_buffer_class=OnlineConditionalVaeRelabelingBuffer,
+            replay_buffer_kwargs=dict(
+                start_skew_epoch=10,
+                max_size=int(100000),
+                fraction_goals_rollout_goals=0.2,
+                fraction_goals_env_goals=0.5,
+                exploration_rewards_type='None',
+                vae_priority_type='vae_prob',
+                priority_function_kwargs=dict(
+                    sampling_method='importance_sampling',
+                    decoder_distribution='gaussian_identity_variance',
+                    num_latents_to_sample=10,
+                ),
+                power=-1,
+                relabeling_goal_sampling_mode='vae_prior',
+            ),
+            exploration_goal_sampling_mode='vae_prior',
+            evaluation_goal_sampling_mode='reset_of_env',
+            normalize=False,
+            render=False,
+            exploration_noise=0.2,
+            exploration_type='ou',
+            training_mode='train',
+            testing_mode='test',
+            reward_params=dict(
+                epsilon=0.05,
+            ),
+            observation_key='latent_observation',
+            desired_goal_key='latent_desired_goal',
+            vae_wrapped_env_kwargs=dict(
+                sample_from_true_prior=True,
+            ),
+            algorithm='ONLINE-VAE-SAC-BERNOULLI',
+            vae_path="/home/ashvin/data/sasha/cond-rig/hyp-tuning/tuning/run0/id0/itr_500.pkl",
+        ),
+        train_vae_variant=dict(
+            latent_sizes=4,
+            beta=10,
+            beta_schedule_kwargs=dict(
+                x_values=(0, 500),
+                y_values=(1, 20),
+            ),
+            context_weight=1,
+            num_epochs=1500,
+            dump_skew_debug_plots=False,
+            decoder_activation='sigmoid',
+            use_linear_dynamics=False,
+            generate_vae_dataset_kwargs=dict(
+                N=102000,
+                dataset_path="/home/ashvin/Desktop/sim_puck_data.npy",
+                n_random_steps=51,
+                test_p=.9,
+                use_cached=False,
+                show=False,
+                oracle_dataset=False,
+                oracle_dataset_using_set_to_goal=False,
+                non_presampled_goal_img_is_garbage=False,
+                random_rollout_data=True,
+                random_rollout_data_set_to_goal=False,
+                conditional_vae_dataset=True,
+                save_trajectories=False,
+                enviorment_dataset=False,
+                tag="ccrig_tuning",
+
+                train_batch_loader_kwargs=dict(
+                    batch_size=128,
+                ),
+                test_batch_loader_kwargs=dict(
+                    batch_size=128,
+                ),
+            ),
+            vae_trainer_class=DeltaCVAETrainer,
+            vae_class=DeltaCVAE,
+            vae_kwargs=dict(
+                input_channels=3,
+                architecture=imsize48_default_architecture_with_more_hidden_layers,
+                decoder_distribution='gaussian_identity_variance',
+            ),
+
+            algo_kwargs=dict(
+                start_skew_epoch=5000,
+                is_auto_encoder=False,
+                batch_size=128,
+                lr=1e-3,
+                skew_config=dict(
+                    method='vae_prob',
+                    power=0,
+                ),
+                skew_dataset=False,
+                priority_function_kwargs=dict(
+                    decoder_distribution='gaussian_identity_variance',
+                    sampling_method='importance_sampling',
+                    num_latents_to_sample=10,
+                ),
+                use_parallel_dataloading=False,
+            ),
+
+            save_period=25,
+        ),
+        region='us-west-2',
+
+        logger_variant=dict(
+            tensorboard=True,
+        ),
+
+        slurm_variant=dict(
+            timeout_min=48 * 60,
+            cpus_per_task=10,
+            gpus_per_node=1,
+        ),
+    )
+
+    search_space = {
+        'seedid': range(1),
+        'grill_variant.exploration_noise': [0.5, ],
+        'train_vae_variant.latent_sizes': [(4, 8),],
+        'train_vae_variant.context_weight': [0.5,],
+        'grill_variant.algo_kwargs.num_trains_per_train_loop':[1000,], #4000, ],
+        'grill_variant.algo_kwargs.batch_size': [128, ],
+    }
+    sweeper = hyp.DeterministicHyperparameterSweeper(
+        search_space, default_parameters=variant,
+    )
+
+    variants = []
+    for variant in sweeper.iterate_hyperparameters():
+        variants.append(variant)
+
+    run_variants(grill_her_td3_offpolicy_online_vae_full_experiment, variants, run_id=2)
\ No newline at end of file
diff --git a/experiments/sasha/sap/planner_pm.py b/experiments/sasha/sap/planner_pm.py
new file mode 100644
index 0000000..8c1e81d
--- /dev/null
+++ b/experiments/sasha/sap/planner_pm.py
@@ -0,0 +1,243 @@
+# from sawyer_control.envs.sawyer_reaching import SawyerReachXYZEnv
+from multiworld.envs.real_world.sawyer.sawyer_reaching import SawyerReachXYZEnv
+
+import railrl.misc.hyperparameter as hyp
+from experiments.murtaza.multiworld.skew_fit.reacher.generate_uniform_dataset import generate_uniform_dataset_reacher
+from multiworld.envs.mujoco.cameras import sawyer_init_camera_zoomed_in, sawyer_pusher_camera_upright_v2
+from railrl.launchers.launcher_util import run_experiment
+import railrl.torch.vae.vae_schedules as vae_schedules
+from railrl.torch.vae.conv_vae import imsize48_default_architecture, imsize48_default_architecture_with_more_hidden_layers
+from railrl.launchers.arglauncher import run_variants
+from railrl.torch.grill.cvae_experiments import (
+    grill_her_td3_offpolicy_online_vae_full_experiment, grill_her_planning_offpolicy_online_vae_full_experiment
+)
+from multiworld.envs.pygame.point2d import Point2DWallEnv, Point2DEnv
+# from multiworld.envs.pygame.multiobject_pygame_env import Multiobj2DEnv
+# from multiworld.envs.mujoco.sawyer_xyz.sawyer_push_multiobj_subset import SawyerMultiobjectEnv
+from railrl.torch.vae.conditional_conv_vae import DeltaCVAE, CDVAE
+from railrl.torch.vae.conditional_vae_trainer import DeltaCVAETrainer, CDVAETrainer
+from railrl.data_management.online_conditional_vae_replay_buffer import \
+        OnlineConditionalVaeRelabelingBuffer
+
+x_var = 0.2
+x_low = -x_var
+x_high = x_var
+y_low = 0.5
+y_high = 0.7
+t = 0.05
+
+if __name__ == "__main__":
+    variant = dict(
+        double_algo=False,
+        online_vae_exploration=False,
+        imsize=48,
+        init_camera=sawyer_init_camera_zoomed_in,
+        env_class=Point2DWallEnv,
+        env_kwargs=dict(
+            #wall_shape="big-u",
+            reward_type="dense",
+            render_onscreen=False,
+            images_are_rgb=True,
+            wall_shape="big-u",
+            show_goal=False,
+            change_walls=False,
+            ball_radius=0.75,
+        ),
+        eval_env_kwargs=dict(
+            wall_shape="big-u",
+            reward_type="dense",
+            render_onscreen=False,
+            images_are_rgb=True,
+            show_goal=False,
+            change_walls=False,
+            ball_radius=0.75,
+        ),
+
+        grill_variant=dict(
+            save_video=True,
+            custom_goal_sampler='replay_buffer',
+            online_vae_trainer_kwargs=dict(
+                beta=20,
+                lr=0,
+            ),
+            save_video_period=10,
+            qf_kwargs=dict(
+                hidden_sizes=[400, 300],
+            ),
+            policy_kwargs=dict(
+                hidden_sizes=[400, 300],
+            ),
+            vf_kwargs=dict(
+                hidden_sizes=[400, 300],
+            ),
+            max_path_length=100,
+            algo_kwargs=dict(
+                batch_size=128,
+                num_epochs=100,
+                num_eval_steps_per_epoch=1000,
+                num_expl_steps_per_train_loop=1000,
+                num_trains_per_train_loop=1000,
+                min_num_steps_before_training=4000,
+                vae_training_schedule=vae_schedules.never_train,
+                oracle_data=False,
+                vae_save_period=25,
+                parallel_vae_train=False,
+            ),
+        twin_sac_trainer_kwargs=dict(
+            discount=0.98,
+            reward_scale=1.0,
+            #soft_target_tau=1e-3,
+            #target_update_period=1,  # 1
+            use_automatic_entropy_tuning=True,
+        ),
+            # td3_trainer_kwargs=dict(
+            #     discount=0.99,
+            #     # min_num_steps_before_training=4000,
+            #     reward_scale=1.0,
+            #     # render=False,
+            #     tau=1e-2,
+            # ),
+            replay_buffer_class=OnlineConditionalVaeRelabelingBuffer,
+            replay_buffer_kwargs=dict(
+                start_skew_epoch=10,
+                max_size=int(100000),
+                fraction_goals_rollout_goals=0.2,
+                fraction_goals_env_goals=0.5,
+                exploration_rewards_type='None',
+                vae_priority_type='vae_prob',
+                priority_function_kwargs=dict(
+                    sampling_method='importance_sampling',
+                    decoder_distribution='gaussian_identity_variance',
+                    # decoder_distribution='bernoulli',
+                    num_latents_to_sample=10,
+                ),
+                power=-1,
+                relabeling_goal_sampling_mode='vae_prior',
+            ),
+            exploration_goal_sampling_mode='vae_prior',
+            evaluation_goal_sampling_mode='reset_of_env',
+            #presampled_goals="/home/ashvin/data/pusher_pucks/arm_data_big.npy",
+            normalize=False,
+            render=False,
+            exploration_noise=0.2,
+            exploration_type='ou',
+            training_mode='train',
+            testing_mode='test',
+            reward_params=dict(
+                epsilon=0.05,
+            ),
+            observation_key='latent_observation',
+            desired_goal_key='latent_desired_goal',
+            vae_wrapped_env_kwargs=dict(
+                sample_from_true_prior=True,
+            ),
+            algorithm='ONLINE-VAE-SAC-BERNOULLI',
+            #vae_path="/home/ashvin/data/s3doodad/sasha/sap/planner-pm/run2/id0/vae.pkl",
+        ),
+        train_vae_variant=dict(
+            latent_sizes=4,
+            beta=10,
+            beta_schedule_kwargs=dict(
+                x_values=(0, 250),
+                y_values=(1, 20),
+            ),
+            num_epochs=250,
+            dump_skew_debug_plots=False,
+            # decoder_activation='gaussian',
+            decoder_activation='sigmoid',
+            use_linear_dynamics=True,
+            generate_vae_dataset_kwargs=dict(
+                N=100000,
+                n_random_steps=100,
+                # dataset_path=["/home/ashvin/data/pusher_pucks/puck_black.npy",
+                # "/home/ashvin/data/pusher_pucks/puck_blue.npy",
+                # "/home/ashvin/data/pusher_pucks/puck_blue1.npy",
+                # "/home/ashvin/data/pusher_pucks/puck_gold.npy",
+                # "/home/ashvin/data/pusher_pucks/puck_green.npy",
+                # "/home/ashvin/data/pusher_pucks/puck_purple.npy",
+                # "/home/ashvin/data/pusher_pucks/puck_purple1.npy",
+                # "/home/ashvin/data/pusher_pucks/puck_red.npy",
+                # "/home/ashvin/data/pusher_pucks/puck_red1.npy",
+                # "/home/ashvin/data/pusher_pucks/puck_white.npy",
+                # "/home/ashvin/data/pusher_pucks/bear.npy",
+                # "/home/ashvin/data/pusher_pucks/cat.npy",
+                # "/home/ashvin/data/pusher_pucks/dog.npy",
+                # "/home/ashvin/data/pusher_pucks/jeans.npy",
+                # "/home/ashvin/data/pusher_pucks/star.npy",
+                # "/home/ashvin/data/pusher_pucks/towel_brown.npy",
+                # "/home/ashvin/data/pusher_pucks/towel_purple.npy",
+                # "/home/ashvin/data/pusher_pucks/towel_red.npy",
+                # "/home/ashvin/data/pusher_pucks/lego_blue.npy",
+                # "/home/ashvin/data/pusher_pucks/lego_red.npy",
+                # "/home/ashvin/data/pusher_pucks/lego_green.npy",
+                # "/home/ashvin/data/pusher_pucks/lego_yellow.npy",],
+                test_p=.9,
+                use_cached=False,
+                show=False,
+                oracle_dataset=False,
+                oracle_dataset_using_set_to_goal=False,
+                random_rollout_data=True,
+                random_rollout_data_set_to_goal=True,
+                conditional_vae_dataset=True,
+                save_trajectories=True,
+                tag="ccrig1",
+
+                train_batch_loader_kwargs=dict(
+                    batch_size=64,
+                ),
+                test_batch_loader_kwargs=dict(
+                    batch_size=64,
+                ),
+            ),
+            vae_trainer_class=CDVAETrainer,
+            vae_class=CDVAE,
+            vae_kwargs=dict(
+                input_channels=3,
+                architecture=imsize48_default_architecture_with_more_hidden_layers,
+                decoder_distribution='gaussian_identity_variance',
+            ),
+            # TODO: why the redundancy?
+            algo_kwargs=dict(
+                start_skew_epoch=5000,
+                is_auto_encoder=False,
+                batch_size=64,
+                lr=1e-3,
+                skew_config=dict(
+                    method='vae_prob',
+                    power=0,
+                ),
+                skew_dataset=False,
+                priority_function_kwargs=dict(
+                    decoder_distribution='gaussian_identity_variance',
+                    sampling_method='importance_sampling',
+                    # sampling_method='true_prior_sampling',
+                    num_latents_to_sample=10,
+                ),
+                use_parallel_dataloading=False,
+            ),
+
+            save_period=10,
+        ),
+        region='us-west-2',
+
+        logger_variant=dict(
+            tensorboard=True,
+        ),
+    )
+
+    search_space = {
+        'seedid': range(1),
+        'grill_variant.exploration_noise': [0.0, ],
+        'train_vae_variant.latent_sizes': [(3, 2),], #(3 * objects, 3 * colors)
+        #'train_vae_variant.representation_size': [3, ],
+        'grill_variant.algo_kwargs.num_trains_per_train_loop':[1000, ],
+    }
+    sweeper = hyp.DeterministicHyperparameterSweeper(
+        search_space, default_parameters=variant,
+    )
+
+    variants = []
+    for variant in sweeper.iterate_hyperparameters():
+        variants.append(variant)
+
+    run_variants(grill_her_planning_offpolicy_online_vae_full_experiment, variants, run_id=3)
\ No newline at end of file
diff --git a/experiments/sasha/sap/planning.py b/experiments/sasha/sap/planning.py
new file mode 100644
index 0000000..75422c3
--- /dev/null
+++ b/experiments/sasha/sap/planning.py
@@ -0,0 +1,253 @@
+# from sawyer_control.envs.sawyer_reaching import SawyerReachXYZEnv
+from multiworld.envs.real_world.sawyer.sawyer_reaching import SawyerReachXYZEnv
+
+import railrl.misc.hyperparameter as hyp
+from experiments.murtaza.multiworld.skew_fit.reacher.generate_uniform_dataset import generate_uniform_dataset_reacher
+from multiworld.envs.mujoco.cameras import sawyer_init_camera_zoomed_in, sawyer_pusher_camera_upright_v2
+from railrl.launchers.launcher_util import run_experiment
+import railrl.torch.vae.vae_schedules as vae_schedules
+from railrl.torch.vae.conv_vae import imsize48_default_architecture, imsize48_default_architecture_with_more_hidden_layers
+from railrl.launchers.arglauncher import run_variants
+from railrl.torch.grill.cvae_experiments import (
+    grill_her_td3_offpolicy_online_vae_full_experiment, grill_her_planning_offpolicy_online_vae_full_experiment
+)
+# from multiworld.envs.pygame.multiobject_pygame_env import Multiobj2DEnv
+# from multiworld.envs.mujoco.sawyer_xyz.sawyer_push_multiobj_subset import SawyerMultiobjectEnv
+from railrl.torch.vae.conditional_conv_vae import DeltaCVAE, CDVAE
+from railrl.torch.vae.conditional_vae_trainer import DeltaCVAETrainer
+from railrl.data_management.online_conditional_vae_replay_buffer import \
+        OnlineConditionalVaeRelabelingBuffer
+
+x_var = 0.2
+x_low = -x_var
+x_high = x_var
+y_low = 0.5
+y_high = 0.7
+t = 0.05
+
+if __name__ == "__main__":
+    variant = dict(
+        double_algo=False,
+        online_vae_exploration=False,
+        imsize=48,
+        init_camera=sawyer_init_camera_zoomed_in,
+        env_class=SawyerReachXYZEnv,
+        env_kwargs=dict(
+            action_mode="position", 
+            max_speed = 0.05, 
+            camera="sawyer_head",
+            fixed_z=True,
+        ),
+
+        grill_variant=dict(
+            save_video=True,
+            custom_goal_sampler='replay_buffer',
+            online_vae_trainer_kwargs=dict(
+                beta=20,
+                lr=0,
+            ),
+            save_video_period=1,
+            qf_kwargs=dict(
+                hidden_sizes=[400, 300],
+            ),
+            policy_kwargs=dict(
+                hidden_sizes=[400, 300],
+            ),
+            vf_kwargs=dict(
+                hidden_sizes=[400, 300],
+            ),
+            max_path_length=100,
+            algo_kwargs=dict(
+                batch_size=128,
+                num_epochs=300,
+                num_eval_steps_per_epoch=20000,
+                num_expl_steps_per_train_loop=1000,
+                num_trains_per_train_loop=1000,
+                min_num_steps_before_training=0,
+                vae_training_schedule=vae_schedules.never_train,
+                oracle_data=False,
+                vae_save_period=25,
+                parallel_vae_train=False,
+                dataset_path=["/home/ashvin/data/pusher_pucks/puck_black.npy",
+                "/home/ashvin/data/pusher_pucks/puck_blue.npy",
+                "/home/ashvin/data/pusher_pucks/puck_blue1.npy",
+                "/home/ashvin/data/pusher_pucks/puck_gold.npy",
+                "/home/ashvin/data/pusher_pucks/puck_green.npy",
+                "/home/ashvin/data/pusher_pucks/puck_purple.npy",
+                "/home/ashvin/data/pusher_pucks/puck_purple1.npy",
+                "/home/ashvin/data/pusher_pucks/puck_red.npy",
+                "/home/ashvin/data/pusher_pucks/puck_red1.npy",
+                "/home/ashvin/data/pusher_pucks/puck_white.npy",
+                "/home/ashvin/data/pusher_pucks/bear.npy",
+                "/home/ashvin/data/pusher_pucks/cat.npy",
+                "/home/ashvin/data/pusher_pucks/dog.npy",
+                "/home/ashvin/data/pusher_pucks/jeans.npy",
+                "/home/ashvin/data/pusher_pucks/star.npy",
+                "/home/ashvin/data/pusher_pucks/towel_brown.npy",
+                "/home/ashvin/data/pusher_pucks/towel_purple.npy",
+                "/home/ashvin/data/pusher_pucks/towel_red.npy",
+                "/home/ashvin/data/pusher_pucks/lego_blue.npy",
+                "/home/ashvin/data/pusher_pucks/lego_red.npy",
+                "/home/ashvin/data/pusher_pucks/lego_green.npy",
+                "/home/ashvin/data/pusher_pucks/lego_yellow.npy",],
+                rl_offpolicy_num_training_steps=50000,
+            ),
+        twin_sac_trainer_kwargs=dict(
+            discount=0.98,
+            reward_scale=1.0,
+            #soft_target_tau=1e-3,
+            target_update_period=1,  # 1
+            use_automatic_entropy_tuning=True,
+        ),
+            # td3_trainer_kwargs=dict(
+            #     discount=0.99,
+            #     # min_num_steps_before_training=4000,
+            #     reward_scale=1.0,
+            #     # render=False,
+            #     tau=1e-2,
+            # ),
+            replay_buffer_class=OnlineConditionalVaeRelabelingBuffer,
+            replay_buffer_kwargs=dict(
+                start_skew_epoch=10,
+                max_size=int(100000),
+                fraction_goals_rollout_goals=0.2,
+                fraction_goals_env_goals=0.5,
+                exploration_rewards_type='None',
+                vae_priority_type='vae_prob',
+                priority_function_kwargs=dict(
+                    sampling_method='importance_sampling',
+                    decoder_distribution='gaussian_identity_variance',
+                    # decoder_distribution='bernoulli',
+                    num_latents_to_sample=10,
+                ),
+                power=-1,
+                relabeling_goal_sampling_mode='vae_prior',
+            ),
+            exploration_goal_sampling_mode='vae_prior',
+            evaluation_goal_sampling_mode='env',
+            #presampled_goals="/home/ashvin/data/pusher_pucks/arm_data_big.npy",
+            normalize=False,
+            render=False,
+            exploration_noise=0.2,
+            exploration_type='ou',
+            training_mode='train',
+            testing_mode='test',
+            reward_params=dict(
+                epsilon=0.05,
+            ),
+            observation_key='latent_observation',
+            desired_goal_key='latent_desired_goal',
+            vae_wrapped_env_kwargs=dict(
+                sample_from_true_prior=True,
+            ),
+            algorithm='ONLINE-VAE-SAC-BERNOULLI',
+            vae_path="/home/ashvin/data/s3doodad/ashvin/corl2019/robot/test1/run32/id0/vae.pkl",
+        ),
+        train_vae_variant=dict(
+            latent_sizes=4,
+            beta=10,
+            beta_schedule_kwargs=dict(
+                x_values=(0, 1500),
+                y_values=(1, 50),
+            ),
+            num_epochs=1500,
+            dump_skew_debug_plots=False,
+            # decoder_activation='gaussian',
+            decoder_activation='sigmoid',
+            use_linear_dynamics=False,
+            generate_vae_dataset_kwargs=dict(
+                N=0,
+                n_random_steps=200,
+                dataset_path=["/home/ashvin/data/pusher_pucks/puck_black.npy",
+                "/home/ashvin/data/pusher_pucks/puck_blue.npy",
+                "/home/ashvin/data/pusher_pucks/puck_blue1.npy",
+                "/home/ashvin/data/pusher_pucks/puck_gold.npy",
+                "/home/ashvin/data/pusher_pucks/puck_green.npy",
+                "/home/ashvin/data/pusher_pucks/puck_purple.npy",
+                "/home/ashvin/data/pusher_pucks/puck_purple1.npy",
+                "/home/ashvin/data/pusher_pucks/puck_red.npy",
+                "/home/ashvin/data/pusher_pucks/puck_red1.npy",
+                "/home/ashvin/data/pusher_pucks/puck_white.npy",
+                "/home/ashvin/data/pusher_pucks/bear.npy",
+                "/home/ashvin/data/pusher_pucks/cat.npy",
+                "/home/ashvin/data/pusher_pucks/dog.npy",
+                "/home/ashvin/data/pusher_pucks/jeans.npy",
+                "/home/ashvin/data/pusher_pucks/star.npy",
+                "/home/ashvin/data/pusher_pucks/towel_brown.npy",
+                "/home/ashvin/data/pusher_pucks/towel_purple.npy",
+                "/home/ashvin/data/pusher_pucks/towel_red.npy",
+                "/home/ashvin/data/pusher_pucks/lego_blue.npy",
+                "/home/ashvin/data/pusher_pucks/lego_red.npy",
+                "/home/ashvin/data/pusher_pucks/lego_green.npy",
+                "/home/ashvin/data/pusher_pucks/lego_yellow.npy",],
+                test_p=.9,
+                use_cached=False,
+                show=False,
+                oracle_dataset=False,
+                oracle_dataset_using_set_to_goal=False,
+                random_rollout_data=True,
+                random_rollout_data_set_to_goal=False,
+                conditional_vae_dataset=True,
+                save_trajectories=True,
+                save_file_prefix="delete",
+                save_directory="/home/ashvin/data/pusher_pucks/",
+                tag="ccrig1",
+
+                train_batch_loader_kwargs=dict(
+                    batch_size=128,
+                ),
+                test_batch_loader_kwargs=dict(
+                    batch_size=128,
+                ),
+            ),
+            vae_trainer_class=DeltaCVAETrainer,
+            vae_class=DeltaCVAE,
+            vae_kwargs=dict(
+                input_channels=3,
+                architecture=imsize48_default_architecture_with_more_hidden_layers,
+                decoder_distribution='gaussian_identity_variance',
+            ),
+            # TODO: why the redundancy?
+            algo_kwargs=dict(
+                start_skew_epoch=5000,
+                is_auto_encoder=False,
+                batch_size=128,
+                lr=1e-3,
+                skew_config=dict(
+                    method='vae_prob',
+                    power=0,
+                ),
+                skew_dataset=False,
+                priority_function_kwargs=dict(
+                    decoder_distribution='gaussian_identity_variance',
+                    sampling_method='importance_sampling',
+                    # sampling_method='true_prior_sampling',
+                    num_latents_to_sample=10,
+                ),
+                use_parallel_dataloading=False,
+            ),
+
+            save_period=10,
+        ),
+        region='us-west-2',
+
+        logger_variant=dict(
+            tensorboard=True,
+        ),
+    )
+
+    search_space = {
+        'seedid': [3],
+        'grill_variant.exploration_noise': [0.3, ],
+        'train_vae_variant.latent_sizes': [(6, 6),], #(3 * objects, 3 * colors)
+        'grill_variant.algo_kwargs.num_trains_per_train_loop':[1000, ],
+    }
+    sweeper = hyp.DeterministicHyperparameterSweeper(
+        search_space, default_parameters=variant,
+    )
+
+    variants = []
+    for variant in sweeper.iterate_hyperparameters():
+        variants.append(variant)
+
+    run_variants(grill_her_planning_offpolicy_online_vae_full_experiment, variants, run_id=3)
\ No newline at end of file
diff --git a/experiments/sasha/sap/sap.py b/experiments/sasha/sap/sap.py
new file mode 100644
index 0000000..7a897f0
--- /dev/null
+++ b/experiments/sasha/sap/sap.py
@@ -0,0 +1,198 @@
+from railrl.launchers.experiments.ashvin.multiworld import her_sap_experiment, her_sap_model_experiment, her_sac_curious_experiment, her_ensemble_planning_experiment
+import railrl.misc.hyperparameter as hyp
+from multiworld.envs.mujoco.cameras import sawyer_pusher_camera_upright_v2
+from multiworld.envs.pygame.point2d import Point2DWallEnv, Point2DEnv
+#from multiworld.envs.pygame.multiobject_pygame_env import Multiobj2DWallEnv
+from multiworld.envs.mujoco.sawyer_xyz.sawyer_push_multiobj_subset import SawyerMultiobjectEnv
+#from multiworld.envs.mujoco.sawyer_xyz.sawyer_pick_and_place import SawyerPickAndPlaceEnv
+
+from railrl.launchers.launcher_util import run_experiment
+from railrl.launchers.arglauncher import run_variants
+from multiworld.envs.mujoco.cameras import sawyer_init_camera_zoomed_in, sawyer_pusher_camera_upright_v2
+
+import numpy as np
+
+x_var = 0.2
+x_low = -x_var
+x_high = x_var
+y_low = 0.5
+y_high = 0.7
+t = 0.05
+
+if __name__ == "__main__":
+    # noinspection PyTypeChecker
+    variant = dict(
+        # algo_kwargs=dict(
+        #     base_kwargs=dict(
+        #         num_epochs=2001,
+        #         num_steps_per_epoch=1000,
+        #         num_steps_per_eval=1000,
+        #         max_path_length=100,
+        #         num_updates_per_env_step=4,
+        #         batch_size=128,
+        #         discount=0.99,
+        #         min_num_steps_before_training=4000,
+        #         reward_scale=1.0,
+        #         render=False,
+        #         collection_mode='online',
+        #         tau=1e-2,
+        #         parallel_env_params=dict(
+        #             num_workers=1,
+        #         ),
+        #     ),
+        #     her_kwargs=dict(
+        #         observation_key='state_observation',
+        #         desired_goal_key='state_desired_goal',
+        #     ),
+        #     td3_kwargs=dict(),
+        # ),
+        algo_kwargs=dict(
+            batch_size=128,
+            num_epochs=100,
+            num_eval_steps_per_epoch=1000,
+            num_expl_steps_per_train_loop=1000,
+            num_trains_per_train_loop=4000,
+            min_num_steps_before_training=4000,
+            max_path_length=50,
+            # oracle_data=False,
+            # vae_save_period=25,
+            # parallel_vae_train=False,
+            # dataset_path=None,
+            # rl_offpolicy_num_training_steps=0,
+        ),
+
+        twin_sac_trainer_kwargs=dict(
+            discount=0.98,
+            reward_scale=1.0,
+            #soft_target_tau=1e-3,
+            #target_update_period=1,  # 1
+            use_automatic_entropy_tuning=True,
+        ),
+        trainer_kwargs=dict(),
+        replay_buffer_kwargs=dict(
+            max_size=int(1E6),
+            # ob_keys_to_save=[],
+        ),
+        qf_kwargs=dict(
+            hidden_sizes=[400, 300, ],
+        ),
+        policy_kwargs=dict(
+            hidden_sizes=[400, 300, ],
+        ),
+        reward_params=dict(
+            type='latent_distance',
+        ),
+        algorithm='HER-SAC',
+        version='normal',
+        es_kwargs=dict(
+            max_sigma=.2,
+        ),
+        exploration_type='ou',
+        observation_key='state_achieved_goal',
+        # init_camera=sawyer_pusher_camera_upright_v2,
+        do_state_exp=True,
+
+        save_video=True,
+        save_video_period=5,
+        imsize=48,
+        init_camera=sawyer_init_camera_zoomed_in,
+
+        snapshot_mode='gap_and_last',
+        snapshot_gap=50,
+
+        # env_class=SawyerMultiobjectEnv,
+        # env_kwargs=dict(
+        #     fixed_start=True, #CHECK
+        #     reset_frequency=1, #CHECK
+        #     fixed_colors=False,
+        #     num_objects=1,
+        #     object_meshes=None,
+        #     num_scene_objects=[1],
+        #     maxlen=0.1,
+        #     action_repeat=1,
+        #     puck_goal_low=(x_low + 0.01, y_low + 0.01),
+        #     puck_goal_high=(x_high - 0.01, y_high - 0.01),
+        #     hand_goal_low=(x_low + 3*t, y_low + t),
+        #     hand_goal_high=(x_high - 3*t, y_high -t),
+        #     mocap_low=(x_low + 2*t, y_low , 0.0),
+        #     mocap_high=(x_high - 2*t, y_high, 0.5),
+        #     object_low=(x_low + 0.01, y_low + 0.01, 0.02),
+        #     object_high=(x_high - 0.01, y_high - 0.01, 0.02),
+        #     use_textures=False,
+        #     init_camera=sawyer_init_camera_zoomed_in,
+        # ),
+        # eval_env_kwargs=dict(
+        #     fixed_start=True, #CHECK
+        #     reset_frequency=1, #CHECK
+        #     fixed_colors=False,
+        #     num_objects=1,
+        #     object_meshes=None,
+        #     num_scene_objects=[1],
+        #     maxlen=0.1,
+        #     action_repeat=1,
+        #     puck_goal_low=(x_low + 0.01, y_low + 0.01),
+        #     puck_goal_high=(x_high - 0.01, y_high - 0.01),
+        #     hand_goal_low=(x_low + 3*t, y_low + t),
+        #     hand_goal_high=(x_high - 3*t, y_high -t),
+        #     mocap_low=(x_low + 2*t, y_low , 0.0),
+        #     mocap_high=(x_high - 2*t, y_high, 0.5),
+        #     object_low=(x_low + 0.01, y_low + 0.01, 0.02),
+        #     object_high=(x_high - 0.01, y_high - 0.01, 0.02),
+        #     use_textures=False,
+        #     init_camera=sawyer_init_camera_zoomed_in,
+        # ),
+        env_class=Point2DWallEnv,
+        env_kwargs=dict(
+            #wall_shape="big-u",
+            reward_type="sparse",
+            render_onscreen=False,
+            images_are_rgb=True,
+            wall_shape="big-u",
+            #change_walls=True,
+        ),
+        eval_env_kwargs=dict(
+            #wall_shape="big-u",
+            reward_type="sparse",
+            render_onscreen=False,
+            images_are_rgb=True,
+            wall_shape="big-u",
+            #change_walls=True,
+        ),
+
+        wrap_mujoco_gym_to_multi_env=False,
+        num_exps_per_instance=1,
+        region='us-west-2',
+
+        logger_variant=dict(
+            tensorboard=True,
+        ),
+    )
+
+    search_space = {
+        # 'env_id': ['SawyerPushAndReacherXYEnv-v0', ],
+        'seedid': range(1),
+        'algo_kwargs.num_trains_per_train_loop': [1000, ],
+        'algo_kwargs.batch_size': [128, ],
+        'replay_buffer_kwargs.fraction_goals_rollout_goals': [0.2, ],
+        'replay_buffer_kwargs.fraction_goals_env_goals': [0.5, ], 
+        'env_kwargs.fixed_start': [True],
+        'eval_env_kwargs.fixed_start': [True, ],
+    }
+
+    sweeper = hyp.DeterministicHyperparameterSweeper(
+        search_space, default_parameters=variant,
+    )
+
+    # n_seeds = 1
+    # mode = 'local'
+    # exp_prefix = 'test'
+
+    n_seeds = 1
+    mode = 'ec2'
+    exp_prefix = 'sawyer_pusher_state_final'
+
+    variants = []
+    for variant in sweeper.iterate_hyperparameters():
+        variants.append(variant)
+
+    run_variants(her_sap_model_experiment, variants, run_id=112)
\ No newline at end of file
diff --git a/experiments/sasha/sap/smap.py b/experiments/sasha/sap/smap.py
new file mode 100644
index 0000000..96bc827
--- /dev/null
+++ b/experiments/sasha/sap/smap.py
@@ -0,0 +1,162 @@
+from railrl.launchers.experiments.ashvin.multiworld import her_sap_model_experiment
+import railrl.misc.hyperparameter as hyp
+from multiworld.envs.mujoco.cameras import sawyer_pusher_camera_upright_v2
+from multiworld.envs.pygame.point2d import Point2DWallEnv, Point2DEnv
+#from multiworld.envs.pygame.multiobject_pygame_env import Multiobj2DWallEnv
+from multiworld.envs.mujoco.sawyer_xyz.sawyer_push_multiobj_subset import SawyerMultiobjectEnv
+#from multiworld.envs.mujoco.sawyer_xyz.sawyer_pick_and_place import SawyerPickAndPlaceEnv
+from railrl.launchers.launcher_util import run_experiment
+from railrl.launchers.arglauncher import run_variants
+from multiworld.envs.mujoco.cameras import sawyer_init_camera_zoomed_in, sawyer_pusher_camera_upright_v2
+import numpy as np
+
+x_var = 0.2
+x_low = -x_var
+x_high = x_var
+y_low = 0.5
+y_high = 0.7
+t = 0.05
+
+if __name__ == "__main__":
+    variant = dict(
+        algo_kwargs=dict(
+            batch_size=128,
+            num_epochs=100,
+            num_eval_steps_per_epoch=1000,
+            num_expl_steps_per_train_loop=1000,
+            num_trains_per_train_loop=4000,
+            min_num_steps_before_training=10000,
+            max_path_length=50,
+            # oracle_data=False,
+            # vae_save_period=25,
+            # parallel_vae_train=False,
+            # dataset_path=None,
+            # rl_offpolicy_num_training_steps=0,
+        ),
+
+        twin_sac_trainer_kwargs=dict(
+            discount=0.98,
+            reward_scale=1.0,
+            #soft_target_tau=1e-3,
+            target_update_period=1,  # 1
+            use_automatic_entropy_tuning=True,
+        ),
+        trainer_kwargs=dict(),
+        replay_buffer_kwargs=dict(
+            max_size=int(1E6),
+            # ob_keys_to_save=[],
+        ),
+        qf_kwargs=dict(
+            hidden_sizes=[400, 300, ],
+        ),
+        policy_kwargs=dict(
+            hidden_sizes=[400, 300,],
+        ),
+        reward_params=dict(
+            type='latent_distance',
+        ),
+        algorithm='HER-SAC',
+        version='normal',
+        es_kwargs=dict(
+            max_sigma=.2,
+        ),
+        exploration_type='ou',
+        observation_key='state_achieved_goal',
+        # init_camera=sawyer_pusher_camera_upright_v2,
+        do_state_exp=True,
+
+        save_video=True,
+        save_video_period=5,
+        imsize=48,
+        init_camera=sawyer_init_camera_zoomed_in,
+
+        snapshot_mode='gap_and_last',
+        snapshot_gap=50,
+
+        # env_class=SawyerMultiobjectEnv,
+        # env_kwargs=dict(
+        #     fixed_start=True, #CHECK
+        #     reset_frequency=1, #CHECK
+        #     fixed_colors=False,
+        #     num_objects=1,
+        #     object_meshes=None,
+        #     num_scene_objects=[1],
+        #     maxlen=0.1,
+        #     action_repeat=1,
+        #     puck_goal_low=(x_low + 0.01, y_low + 0.01),
+        #     puck_goal_high=(x_high - 0.01, y_high - 0.01),
+        #     hand_goal_low=(x_low + 3*t, y_low + t),
+        #     hand_goal_high=(x_high - 3*t, y_high -t),
+        #     mocap_low=(x_low + 2*t, y_low , 0.0),
+        #     mocap_high=(x_high - 2*t, y_high, 0.5),
+        #     object_low=(x_low + 0.01, y_low + 0.01, 0.02),
+        #     object_high=(x_high - 0.01, y_high - 0.01, 0.02),
+        #     use_textures=False,
+        #     init_camera=sawyer_init_camera_zoomed_in,
+        # ),
+        # eval_env_kwargs=dict(
+        #     fixed_start=True, #CHECK
+        #     reset_frequency=1, #CHECK
+        #     fixed_colors=False,
+        #     num_objects=1,
+        #     object_meshes=None,
+        #     num_scene_objects=[1],
+        #     maxlen=0.1,
+        #     action_repeat=1,
+        #     puck_goal_low=(x_low + 0.01, y_low + 0.01),
+        #     puck_goal_high=(x_high - 0.01, y_high - 0.01),
+        #     hand_goal_low=(x_low + 3*t, y_low + t),
+        #     hand_goal_high=(x_high - 3*t, y_high -t),
+        #     mocap_low=(x_low + 2*t, y_low , 0.0),
+        #     mocap_high=(x_high - 2*t, y_high, 0.5),
+        #     object_low=(x_low + 0.01, y_low + 0.01, 0.02),
+        #     object_high=(x_high - 0.01, y_high - 0.01, 0.02),
+        #     use_textures=False,
+        #     init_camera=sawyer_init_camera_zoomed_in,
+        # ),
+        env_class=Point2DWallEnv,
+        env_kwargs=dict(
+            #wall_shape="big-u",
+            reward_type="dense",
+            render_onscreen=False,
+            images_are_rgb=True,
+            wall_shape="big-u",
+            #change_walls=True,
+        ),
+        eval_env_kwargs=dict(
+            #wall_shape="big-u",
+            reward_type="dense",
+            render_onscreen=False,
+            images_are_rgb=True,
+            wall_shape="big-u",
+            #change_walls=True,
+        ),
+
+        wrap_mujoco_gym_to_multi_env=False,
+        num_exps_per_instance=1,
+        region='us-west-2',
+
+        logger_variant=dict(
+            tensorboard=True,
+        ),
+    )
+
+    search_space = {
+        'seedid': range(1),
+        'algo_kwargs.num_trains_per_train_loop': [1000, ],
+        'algo_kwargs.batch_size': [128, ],
+        'replay_buffer_kwargs.fraction_goals_rollout_goals': [0.2, ],
+        'replay_buffer_kwargs.fraction_goals_env_goals': [0.5, ], 
+        'env_kwargs.fixed_start': [True],
+        'eval_env_kwargs.fixed_start': [True, ],
+    }
+
+    sweeper = hyp.DeterministicHyperparameterSweeper(
+        search_space, default_parameters=variant,
+    )
+
+    variants = []
+    for variant in sweeper.iterate_hyperparameters():
+        variants.append(variant)
+
+    run_variants(her_sap_model_experiment, variants, run_id=4)
diff --git a/file_name.ps b/file_name.ps
new file mode 100644
index 0000000..0ef6672
--- /dev/null
+++ b/file_name.ps
@@ -0,0 +1,88 @@
+%!PS-Adobe-3.0 EPSF-3.0
+%%Creator: Tk Canvas Widget
+%%For: ashvin,,,
+%%Title: Window .140473157036632
+%%CreationDate: Wed Oct 30 01:42:44 2019
+%%BoundingBox: 306 396 307 397
+%%Pages: 1
+%%DocumentData: Clean7Bit
+%%Orientation: Portrait
+%%EndComments
+
+%%BeginProlog
+% This is a standard prolog for Postscript generated by Tk's canvas
+% widget.
+/CurrentEncoding [
+/space/space/space/space/space/space/space/space
+/space/space/space/space/space/space/space/space
+/space/space/space/space/space/space/space/space
+/space/space/space/space/space/space/space/space
+/space/exclam/quotedbl/numbersign/dollar/percent/ampersand/quotesingle
+/parenleft/parenright/asterisk/plus/comma/hyphen/period/slash
+/zero/one/two/three/four/five/six/seven
+/eight/nine/colon/semicolon/less/equal/greater/question
+/at/A/B/C/D/E/F/G
+/H/I/J/K/L/M/N/O
+/P/Q/R/S/T/U/V/W
+/X/Y/Z/bracketleft/backslash/bracketright/asciicircum/underscore
+/grave/a/b/c/d/e/f/g
+/h/i/j/k/l/m/n/o
+/p/q/r/s/t/u/v/w
+/x/y/z/braceleft/bar/braceright/asciitilde/space
+/space/space/space/space/space/space/space/space
+/space/space/space/space/space/space/space/space
+/space/space/space/space/space/space/space/space
+/space/space/space/space/space/space/space/space
+/space/exclamdown/cent/sterling/currency/yen/brokenbar/section
+/dieresis/copyright/ordfeminine/guillemotleft/logicalnot/hyphen/registered/macron
+/degree/plusminus/twosuperior/threesuperior/acute/mu/paragraph/periodcentered
+/cedilla/onesuperior/ordmasculine/guillemotright/onequarter/onehalf/threequarters/questiondown
+/Agrave/Aacute/Acircumflex/Atilde/Adieresis/Aring/AE/Ccedilla
+/Egrave/Eacute/Ecircumflex/Edieresis/Igrave/Iacute/Icircumflex/Idieresis
+/Eth/Ntilde/Ograve/Oacute/Ocircumflex/Otilde/Odieresis/multiply
+/Oslash/Ugrave/Uacute/Ucircumflex/Udieresis/Yacute/Thorn/germandbls
+/agrave/aacute/acircumflex/atilde/adieresis/aring/ae/ccedilla
+/egrave/eacute/ecircumflex/edieresis/igrave/iacute/icircumflex/idieresis
+/eth/ntilde/ograve/oacute/ocircumflex/otilde/odieresis/divide
+/oslash/ugrave/uacute/ucircumflex/udieresis/yacute/thorn/ydieresis
+] def
+50 dict begin
+/baseline 0 def
+/stipimage 0 def
+/height 0 def
+/justify 0 def
+/lineLength 0 def
+/spacing 0 def
+/stipple 0 def
+/strings 0 def
+/xoffset 0 def
+/yoffset 0 def
+/tmpstip null def
+/baselineSampler ( TXygqPZ) def
+baselineSampler 0 196 put
+/cstringshow {{ dup type /stringtype eq { show } { glyphshow } ifelse } forall } bind def
+/cstringwidth {0 exch 0 exch { dup type /stringtype eq { stringwidth } { currentfont /Encoding get exch 1 exch put (\001) stringwidth } ifelse exch 3 1 roll add 3 1 roll add exch } forall } bind def
+/ISOEncode {dup length dict begin {1 index /FID ne {def} {pop pop} ifelse} forall /Encoding CurrentEncoding def currentdict end /Temporary exch definefont } bind def
+/StrokeClip {{strokepath} stopped { (This Postscript printer gets limitcheck overflows when) = (stippling dashed lines;  lines will be printed solid instead.) = [] 0 setdash strokepath} if clip } bind def
+/EvenPixels {dup 0 matrix currentmatrix dtransform dup mul exch dup mul add sqrt dup round dup 1 lt {pop 1} if exch div mul } bind def
+/StippleFill {/tmpstip 1 index def 1 EvenPixels dup scale pathbbox 4 2 roll 5 index div dup 0 lt {1 sub} if cvi 5 index mul 4 1 roll 6 index div dup 0 lt {1 sub} if cvi 6 index mul 3 2 roll 6 index exch { 2 index 5 index 3 index { gsave 1 index exch translate 5 index 5 index true matrix tmpstip imagemask grestore } for pop } for pop pop pop pop pop } bind def
+/AdjustColor {CL 2 lt { currentgray CL 0 eq { .5 lt {0} {1} ifelse } if setgray } if } bind def
+/DrawText {/stipple exch def /justify exch def /yoffset exch def /xoffset exch def /spacing exch def /strings exch def /lineLength 0 def strings { cstringwidth pop dup lineLength gt {/lineLength exch def} {pop} ifelse newpath } forall 0 0 moveto baselineSampler false charpath pathbbox dup /baseline exch def exch pop exch sub /height exch def pop newpath translate rotate lineLength xoffset mul strings length 1 sub spacing mul height add yoffset mul translate justify lineLength mul baseline neg translate strings { dup cstringwidth pop justify neg mul 0 moveto stipple { gsave /char (X) def { dup type /stringtype eq { { char 0 3 -1 roll put currentpoint gsave char true charpath clip StippleText grestore char stringwidth translate moveto } forall } { currentfont /Encoding get exch 1 exch put currentpoint gsave (\001) true charpath clip StippleText grestore (\001) stringwidth translate moveto } ifelse } forall grestore } {cstringshow} ifelse 0 spacing neg translate } forall } bind def
+/TkPhotoColor {gsave 32 dict begin /tinteger exch def /transparent 1 string def transparent 0 tinteger put /olddict exch def olddict /DataSource get dup type /filetype ne { olddict /DataSource 3 -1 roll 0 () /SubFileDecode filter put } { pop } ifelse /newdict olddict maxlength dict def olddict newdict copy pop /w newdict /Width get def /crpp newdict /Decode get length 2 idiv def /str w string def /pix w crpp mul string def /substrlen 2 w log 2 log div floor exp cvi def /substrs [ { substrlen string 0 1 substrlen 1 sub { 1 index exch tinteger put } for /substrlen substrlen 2 idiv def substrlen 0 eq {exit} if } loop ] def /h newdict /Height get def 1 w div 1 h div matrix scale olddict /ImageMatrix get exch matrix concatmatrix matrix invertmatrix concat newdict /Height 1 put newdict /DataSource pix put /mat [w 0 0 h 0 0] def newdict /ImageMatrix mat put 0 1 h 1 sub { mat 5 3 -1 roll neg put olddict /DataSource get str readstring pop pop /tail str def /x 0 def olddict /DataSource get pix readstring pop pop { tail transparent search dup /done exch not def {exch pop exch pop} if /w1 exch length def w1 0 ne { newdict /DataSource pix x crpp mul w1 crpp mul getinterval put newdict /Width w1 put mat 4 x neg put /x x w1 add def newdict image /tail tail w1 tail length w1 sub getinterval def } if done {exit} if tail substrs { anchorsearch {pop} if } forall /tail exch def tail length 0 eq {exit} if /x w tail length sub def } loop } for end grestore } bind def
+/TkPhotoMono {gsave 32 dict begin /dummyInteger exch def /olddict exch def olddict /DataSource get dup type /filetype ne { olddict /DataSource 3 -1 roll 0 () /SubFileDecode filter put } { pop } ifelse /newdict olddict maxlength dict def olddict newdict copy pop /w newdict /Width get def /pix w 7 add 8 idiv string def /h newdict /Height get def 1 w div 1 h div matrix scale olddict /ImageMatrix get exch matrix concatmatrix matrix invertmatrix concat newdict /Height 1 put newdict /DataSource pix put /mat [w 0 0 h 0 0] def newdict /ImageMatrix mat put 0 1 h 1 sub { mat 5 3 -1 roll neg put 0.000 0.000 0.000 setrgbcolor olddict /DataSource get pix readstring pop pop newdict /DataSource pix put newdict imagemask 1.000 1.000 1.000 setrgbcolor olddict /DataSource get pix readstring pop pop newdict /DataSource pix put newdict imagemask } for end grestore } bind def
+%%EndProlog
+%%BeginSetup
+/CL 2 def
+%%EndSetup
+
+%%Page: 1 1
+save
+306.0 396.0 translate
+0.75 0.75 scale
+0 0 translate
+0 1 moveto 1 1 lineto 1 0 lineto 0 0 lineto closepath clip newpath
+restore showpage
+
+%%Trailer
+end
+%%EOF
diff --git a/railrl/core/batch_rl_algorithm.py b/railrl/core/batch_rl_algorithm.py
index b952f25..160fd67 100644
--- a/railrl/core/batch_rl_algorithm.py
+++ b/railrl/core/batch_rl_algorithm.py
@@ -45,11 +45,12 @@ class BatchRLAlgorithm(BaseRLAlgorithm):
             self.replay_buffer.add_paths(init_expl_paths)
             self.expl_data_collector.end_epoch(-1)
 
-        self.eval_data_collector.collect_new_paths(
+        new_eval_paths = self.eval_data_collector.collect_new_paths(
             self.max_path_length,
             self.num_eval_steps_per_epoch,
             discard_incomplete_paths=True,
         )
+
         timer.stamp('evaluation sampling')
 
         for _ in range(self.num_train_loops_per_epoch):
diff --git a/railrl/core/logging.py b/railrl/core/logging.py
index 8c5dcdb..5aa2c98 100644
--- a/railrl/core/logging.py
+++ b/railrl/core/logging.py
@@ -129,12 +129,6 @@ class Logger(object):
         self._add_output(file_name, self._text_outputs, self._text_fds,
                          mode='a')
 
-    def add_tensorboard_output(self, file_name):
-        import tensorboard_logger
-        self._use_tensorboard = True
-        self.tensorboard_logger = tensorboard_logger
-        self.tensorboard_logger.configure(file_name)
-
     def remove_text_output(self, file_name):
         self._remove_output(file_name, self._text_outputs, self._text_fds)
 
diff --git a/railrl/data_management/dataset.py b/railrl/data_management/dataset.py
index 14ffd4c..4eba09e 100644
--- a/railrl/data_management/dataset.py
+++ b/railrl/data_management/dataset.py
@@ -5,7 +5,7 @@ import torch
 from railrl.data_management.images import normalize_image, unnormalize_image
 from railrl.torch.core import np_to_pytorch_batch
 import railrl.torch.pytorch_util as ptu
-
+import random
 from torch.utils import data
 from torchvision.transforms import ColorJitter, RandomResizedCrop
 from PIL import Image
@@ -185,35 +185,8 @@ class TripletReplayBufferWrapper(BatchLoader):
         )
         return np_to_pytorch_batch(batch)
 
-class InitialObservationNumpyDataset(data.Dataset):
-    def __init__(self, data, info=None):
-        assert data['observations'].dtype == np.uint8
-
-        self.size = data['observations'].shape[0]
-        self.traj_length = data['observations'].shape[1]
-        self.data = data
-        self.info = info
-
-        if 'env' not in self.data:
-            self.data['env'] = self.data['observations'][:, 0, :]
-
-    def __len__(self):
-        return self.size * self.traj_length
 
-    def __getitem__(self, idx):
-        traj_i = idx // self.traj_length
-        trans_i = idx % self.traj_length
-
-        env = normalize_image(self.data['env'][traj_i, :])
-        x_t = normalize_image(self.data['observations'][traj_i, trans_i, :])
-
-        data_dict = {
-            'x_t': x_t,
-            'env': env,
-        }
-        return data_dict
-
-class InitialObservationNumpyJitteringDataset(data.Dataset):
+class InitialObservationNumpyDataset(data.Dataset):
     def __init__(self, data, info=None):
         assert data['observations'].dtype == np.uint8
 
@@ -222,7 +195,7 @@ class InitialObservationNumpyJitteringDataset(data.Dataset):
         self.data = data
         self.info = info
 
-        self.jitter = ColorJitter((0.5,1.5), (0.9,1.1), (0.9,1.1), (-0.1,0.1))
+        self.jitter = ColorJitter((0.7,1.3), (0.95,1.05), (0.95,1.05), (-0.05,0.05))
         self.crop = RandomResizedCrop((48, 48), (0.9, 0.9), (1, 1))
         # RandomResizedCrop((int(sqrt(self.imlength)), int(sqrt(self.imlength))), (0.9, 0.9), (1, 1))
 
@@ -232,22 +205,46 @@ class InitialObservationNumpyJitteringDataset(data.Dataset):
     def __len__(self):
         return self.size * self.traj_length
 
+    # def __getitem__(self, idx):
+    #     traj_i = idx // self.traj_length
+    #     trans_i = idx % self.traj_length
+
+    #     # x = Image.fromarray(self.data['observations'][traj_i, trans_i].reshape(48, 48, 3), mode='RGB')
+    #     # c = Image.fromarray(self.data['env'][traj_i].reshape(48, 48, 3), mode='RGB')
+
+    #     # # upsampling gives bad images so random resizing params set to 1 for now
+    #     # # crop = self.crop.get_params(c, (0.9, 0.9), (1, 1))
+    #     # crop = self.crop.get_params(c, (1, 1), (1, 1))
+    #     # jitter = self.jitter.get_params((0.7,1.3), (0.95,1.05), (0.95,1.05), (-0.05,0.05))
+    #     # # jitter = self.jitter.get_params((0.5,1.5), (0.9,1.1), (0.9,1.1), (-0.1,0.1))
+    #     # # jitter = self.jitter.get_params(0.5, 0.1, 0.1, 0.1)
+
+    #     # x = jitter(F.resized_crop(x, crop[0], crop[1], crop[2], crop[3], (48, 48), Image.BICUBIC))
+    #     # c = jitter(F.resized_crop(c, crop[0], crop[1], crop[2], crop[3], (48, 48), Image.BICUBIC))
+    #     # x_t = normalize_image(np.array(x).flatten()).squeeze()
+    #     # env = normalize_image(np.array(c).flatten()).squeeze()
+
+    #     data_dict = {
+    #         'x_t': self.data['observations'][traj_i, trans_i] / 255.0,
+    #         'env': self.data['env'][traj_i] / 255.0,
+    #     }
+    #     return data_dict
     def __getitem__(self, idx):
         traj_i = idx // self.traj_length
         trans_i = idx % self.traj_length
-
         x = Image.fromarray(self.data['observations'][traj_i, trans_i].reshape(48, 48, 3), mode='RGB')
         c = Image.fromarray(self.data['env'][traj_i].reshape(48, 48, 3), mode='RGB')
 
         # upsampling gives bad images so random resizing params set to 1 for now
         # crop = self.crop.get_params(c, (0.9, 0.9), (1, 1))
         crop = self.crop.get_params(c, (1, 1), (1, 1))
-
+        jitter = self.jitter.get_params((0.7,1.3), (0.95,1.05), (0.95,1.05), (-0.05,0.05))
         # jitter = self.jitter.get_params((0.5,1.5), (0.9,1.1), (0.9,1.1), (-0.1,0.1))
-        jitter = self.jitter.get_params(0.5, 0.1, 0.1, 0.1)
+        # jitter = self.jitter.get_params(0.5, 0.1, 0.1, 0.1)
 
         x = jitter(F.resized_crop(x, crop[0], crop[1], crop[2], crop[3], (48, 48), Image.BICUBIC))
         c = jitter(F.resized_crop(c, crop[0], crop[1], crop[2], crop[3], (48, 48), Image.BICUBIC))
+
         x_t = normalize_image(np.array(x).flatten()).squeeze()
         env = normalize_image(np.array(c).flatten()).squeeze()
 
diff --git a/railrl/data_management/external/epic_kitchens_data_stub.py b/railrl/data_management/external/epic_kitchens_data_stub.py
deleted file mode 100644
index 7547069..0000000
--- a/railrl/data_management/external/epic_kitchens_data_stub.py
+++ /dev/null
@@ -1,352 +0,0 @@
-import numpy as np
-import glob
-import skvideo.io
-from matplotlib.image import imread
-
-import io
-import base64
-from IPython.display import HTML
-import os.path
-from os import path
-import pickle
-
-import csv
-
-from matplotlib.image import imread
-from torch.utils import data
-
-from railrl.data_management.images import normalize_image, unnormalize_image
-
-from railrl.torch import pytorch_util as ptu
-# import matplotlib.pyplot as plt
-
-import torchvision
-
-import torchvision.transforms.functional as TF
-from PIL import Image
-
-import random
-import math
-
-# output_dir = "/private/home/anair17/ashvindev/rlkit/notebooks/outputs/"
-
-# f_action_labels = "/datasets01_101/EPIC_KITCHENS_2018/061218/annotations/EPIC_train_action_labels.csv"
-# rows = []
-# with open(f_action_labels, 'r') as csvfile:
-#     spamreader = csv.reader(csvfile, delimiter=',', quotechar='|')
-#     i = 0
-#     for row in spamreader:
-#         rows.append(row)
-#         i += 1
-# action_labels = rows[1:] # get rid of header
-
-# action_labels_dict = {}
-# for row in action_labels:
-#     uid = int(row[0])
-#     action_labels_dict[uid] = row
-
-# # loads 28470 action sequences
-
-# def get_frame_file(participant_id, video_id, frame_id):
-#     frame_string = str(frame_id).zfill(10) 
-#     return "/datasets01_101/EPIC_KITCHENS_2018/061218/frames_rgb_flow/rgb/train/%s/%s/frame_%s.jpg" % (participant_id, video_id, frame_string)
-
-# def save_clip(uid, use_cache=True):
-#     output_file = output_dir + "clip_%d.mp4" % uid
-    
-#     row = action_labels_dict[uid]
-#     assert uid == int(row[0]), "did not match uid %d %d" % (uid, int(row[0]))
-    
-#     participant_id = row[1]
-#     video_id = row[2]
-#     action = row[3]
-#     start_frame = int(row[6])
-#     end_frame = int(row[7])
-    
-#     if use_cache and path.exists(output_file):
-#         return None
-#     else:
-#         frames = []
-
-#         for frame in range(start_frame, end_frame):
-#             frame_file = get_frame_file(participant_id, video_id, frame)
-#             frame = imread(frame_file)
-#             frames.append(frame)
-
-#         outputdata = np.array(frames)
-#         skvideo.io.vwrite(output_file, outputdata)
-
-#         return frames
-
-# def load_clip(uid, max_frames=-1): # timeit: ~450ms
-#     row = action_labels_dict[uid]
-#     assert uid == int(row[0]), "did not match uid %d %d" % (uid, int(row[0]))
-    
-#     participant_id = row[1]
-#     video_id = row[2]
-#     action = row[3]
-#     start_frame = int(row[6])
-#     end_frame = int(row[7])
-    
-#     frames = []
-
-#     if max_frames > 0:
-#         idxs = np.linspace(start_frame, end_frame, max_frames).astype(int)
-#     else:
-#         idxs = range(start_frame, end_frame)
-
-#     for frame in idxs:
-#         frame_file = get_frame_file(participant_id, video_id, frame)
-#         frame = imread(frame_file)
-#         frames.append(frame)
-
-#     outputdata = np.array(frames)
-
-#     return outputdata
-
-# def load_video(uid): # timeit: ~300ms
-#     output_file = output_dir + "clip_%d.mp4" % uid
-#     videodata = skvideo.io.vread(output_file)
-    
-#     return videodata
-
-# def show_clip(uid):
-#     output_file = output_dir + "clip_%d.mp4" % uid
-
-#     video = io.open(output_file, 'r+b').read()
-#     encoded = base64.b64encode(video)
-#     return HTML(data='''<video alt="test" controls>
-#                     <source src="data:video/mp4;base64,{0}" type="video/mp4" />
-#                  </video>'''.format(encoded.decode('ascii')))
-
-# def generate_clips(uids):
-#     data = ""
-#     for uid in uids:
-#         save_clip(uid)
-        
-#         output_file = output_dir + "clip_%d.mp4" % uid
-
-#         video = io.open(output_file, 'r+b').read()
-#         encoded = base64.b64encode(video)
-#         data += '''<video alt="test" controls>
-#                         <source src="data:video/mp4;base64,{0}" type="video/mp4" />
-#                      </video>'''.format(encoded.decode('ascii'))
-#     return HTML(data=data)
-
-# def str_in_filter_fn(s):
-#     return lambda row: s in row[3]
-
-# def find_label(filter_fn):
-#     open_actions = []
-#     for row in action_labels:
-#         if filter_fn(row):
-#             open_actions.append(row)
-#     return open_actions
-
-# def search_label(filter_fn, num_videos):
-#     open_actions = find_label(filter_fn)
-    
-#     filt = np.random.choice(open_actions, num_videos)
-#     print(filt)
-#     ids = [int(row[0]) for row in filt]
-    
-#     return generate_clips(ids)
-
-# def dataset_stats(rows):
-#     L = len(rows)
-#     H = np.array([int(row[7]) - int(row[6]) for row in rows])
-#     max_H = max(H)
-#     min_H = min(H)
-#     mean_H = np.mean(H)
-#     return (L, min_H, max_H, mean_H)
-
-RANDOM_CROP_X = 16
-RANDOM_CROP_Y = 16
-WIDTH = 456
-HEIGHT = 256
-CROP_WIDTH = WIDTH - RANDOM_CROP_X
-CROP_HEIGHT = HEIGHT - RANDOM_CROP_Y
-
-def transform_image(img):
-    # m = img.shape[1] // 2
-    # m0, m1 = m - 120, m + 120
-    # img = img[:, m0:m1, :]
-    img = img / 255.0
-    # img = img - np.array([0.485, 0.456, 0.406])
-    # img = img / np.array([0.229, 0.224, 0.225])
-    img = img.transpose()
-    x, y = np.random.randint(RANDOM_CROP_X), np.random.randint(RANDOM_CROP_Y)
-    img = img[:, x:x+CROP_WIDTH, y:y+CROP_HEIGHT]
-    return img.flatten()
-
-def transform_batch(img):
-    # m = img.shape[2] // 2
-    # m0, m1 = m - 120, m + 120
-    # img = img[:, 8:248, m0:m1, :]
-    img = img.transpose([0, 3, 2, 1])
-    x, y = np.random.randint(RANDOM_CROP_X), np.random.randint(RANDOM_CROP_Y)
-    return img[:, :, x:x+CROP_WIDTH, y:y+CROP_HEIGHT]
-
-# def viz_rewards(id, savefile):
-#     clip_data = epic.load_clip(id)
-#     t_clip = epic.transform_batch(clip_data)
-#     batch = ptu.from_numpy(t_clip / 255.0)
-#     batch = batch.to("cuda")
-#     zs = model.encoder(batch)
-#     z = ptu.get_numpy(zs)
-    
-#     z_goal = z[-1, :]
-#     distances = []
-#     for t in range(len(z)):
-#         d = np.linalg.norm(z[t, :] - z_goal)
-#         distances.append(d)
-    
-#     plt.plot(distances)
-#     plt.show()
-    
-#     epic.save_clip(id)
-#     return epic.show_clip(id)
-
-def get_clip_as_batch(id, max_frames=-1):
-    clip_data = load_clip(id, max_frames)
-    t_clip = transform_batch(clip_data)
-    batch = ptu.from_numpy(normalize_image(t_clip))
-    # batch = batch.to("cuda")
-    return batch
-
-def viz_rewards(model, id, savefile=None):
-    clip_data = load_clip(id)
-    t_clip = transform_batch(clip_data)
-    batch = ptu.from_numpy(normalize_image(t_clip))
-    batch = batch.to("cuda")
-
-    z = ptu.get_numpy(model.encoder(batch).cpu())
-    
-    z_goal = z[-1, :]
-    distances = []
-    for t in range(len(z)):
-        d = np.linalg.norm(z[t, :] - z_goal)
-        distances.append(d)
-    
-    plt.figure()
-    plt.plot(distances)
-    if savefile:
-        plt.savefig(savefile)
-
-    return np.array(distances)
-
-def normalize(img):
-    return img
-    # img = normalize_image(img) # rescale to 0-1
-    # img = img - np.array([0.485, 0.456, 0.406])
-    # img = img / np.array([0.229, 0.224, 0.225])
-    # return img
-
-def get_random_crop_params(img, scale_x, scale_y):
-    """Get parameters for ``crop`` for a random sized crop.
-
-    Args:
-        img (PIL Image): Image to be cropped.
-        scale (tuple): range of size of the origin size cropped
-        ratio (tuple): range of aspect ratio of the origin aspect ratio cropped
-
-    Returns:
-        tuple: params (i, j, h, w) to be passed to ``crop`` for a random
-            sized crop.
-    """
-    w = int(random.uniform(*scale_x) * CROP_WIDTH)
-    h = int(random.uniform(*scale_y) * CROP_HEIGHT)
-
-    i = random.randint(0, img.size[1] - h)
-    j = random.randint(0, img.size[0] - w)
-    
-    return i, j, h, w
-
-class EpicTimePredictionDataset(data.Dataset):
-    def __init__(self, dataset, output_classes=100):
-        self.dataset = dataset
-        self.num_traj = len(dataset)
-        self.output_classes = output_classes
-
-        self.t_to_pil = torchvision.transforms.ToPILImage()
-        self.t_random_resize = torchvision.transforms.RandomResizedCrop(
-            size=(CROP_WIDTH, CROP_HEIGHT,),
-            scale=(0.8, 1.0),
-            ratio=(1.0, 1.0), # don't change aspect ratio
-        )
-        self.t_color_jitter = torchvision.transforms.ColorJitter(
-            brightness=0.2, # (0.8, 1.2),
-            contrast=0.2, # (0.8, 1.2),
-            saturation=0.2, # (0.8, 1.2),
-            hue=0.1, # (-0.2, 0.2),
-        )
-        self.t_to_tensor = torchvision.transforms.ToTensor()
-
-    def load_frame(self, participant_id, video_id, frame_id):
-        # img = imread(get_frame_file(participant_id, video_id, frame_id))
-        # if img.shape != (256, 456, 3): # (256, 342, 3)
-        #     print(participant_id, video_id, frame_id, img.shape)
-        # return transform_image(img)
-
-        img = Image.open(get_frame_file(participant_id, video_id, frame_id))
-        return img
-
-    def __len__(self):
-        return self.num_traj
-
-    def __getitem__(self, index):
-        row = self.dataset[index]
-
-        participant_id = row[1]
-        video_id = row[2]
-        action = row[3]
-        start_frame = int(row[6])
-        end_frame = int(row[7])
-        traj_length = end_frame - start_frame
-
-        d0, dt, dT = sorted(np.random.randint(0, traj_length, 3))
-        x0 = self.load_frame(participant_id, video_id, start_frame + d0)
-        xt = self.load_frame(participant_id, video_id, start_frame + dt)
-        xT = self.load_frame(participant_id, video_id, start_frame + dT + 1)
-        yt = int((dt - d0) / (dT + 1 - d0) * self.output_classes)
-
-        # x0 = self.t_to_pil(x0)
-
-        i, j, h, w = get_random_crop_params(
-            x0, 
-            self.t_random_resize.scale, 
-            self.t_random_resize.scale,
-        )
-
-        t_color_jitter = self.t_color_jitter.get_params(
-            self.t_color_jitter.brightness,
-            self.t_color_jitter.contrast,
-            self.t_color_jitter.saturation,
-            self.t_color_jitter.hue,
-        )
-
-        x0 = TF.resized_crop(x0, i, j, h, w, (CROP_HEIGHT, CROP_WIDTH,), self.t_random_resize.interpolation)
-        x0 = t_color_jitter(x0)
-        x0 = self.t_to_tensor(x0)
-
-        # xt = self.t_to_pil(xt)
-        xt = TF.resized_crop(xt, i, j, h, w, (CROP_HEIGHT, CROP_WIDTH,), self.t_random_resize.interpolation)
-        xt = t_color_jitter(xt)
-        xt = self.t_to_tensor(xt)
-
-        # xT = self.t_to_pil(xT)
-        xT = TF.resized_crop(xT, i, j, h, w, (CROP_HEIGHT, CROP_WIDTH,), self.t_random_resize.interpolation)
-        xT = t_color_jitter(xT)
-        xT = self.t_to_tensor(xT)
-
-        batch = dict(
-            # x0=normalize(x0),
-            # xt=normalize(xt),
-            # xT=normalize(xT),
-            x0=x0,
-            xt=xt,
-            xT=xT,
-            yt=yt,
-        )
-
-        return batch
diff --git a/railrl/data_management/normalizer.py b/railrl/data_management/normalizer.py
index 9701eaa..a1fda79 100644
--- a/railrl/data_management/normalizer.py
+++ b/railrl/data_management/normalizer.py
@@ -2,6 +2,8 @@
 Based on code from Marcin Andrychowicz
 """
 import numpy as np
+import railrl.torch.pytorch_util as ptu
+import torch
 
 
 class Normalizer(object):
diff --git a/railrl/data_management/obs_dict_replay_buffer.py b/railrl/data_management/obs_dict_replay_buffer.py
index d0e4cd4..d706ee5 100644
--- a/railrl/data_management/obs_dict_replay_buffer.py
+++ b/railrl/data_management/obs_dict_replay_buffer.py
@@ -39,7 +39,7 @@ class ObsDictReplayBuffer(ReplayBuffer):
             ob_keys_to_save = list(ob_keys_to_save)
         if internal_keys is None:
             internal_keys = []
-        self.internal_keys = internal_keys
+        self.internal_keys = []#internal_keys
         assert isinstance(env.observation_space, Dict)
         self.max_size = max_size
         self.env = env
@@ -239,7 +239,6 @@ class ObsDictRelabelingBuffer(ObsDictReplayBuffer):
             achieved_goal_key='achieved_goal',
             vectorized=False,
             ob_keys_to_save=None,
-            use_multitask_rewards=True,
             **kwargs
     ):
         """
@@ -280,7 +279,6 @@ class ObsDictRelabelingBuffer(ObsDictReplayBuffer):
         self.desired_goal_key = desired_goal_key
         self.achieved_goal_key = achieved_goal_key
         self.vectorized = vectorized
-        self.use_multitask_rewards = use_multitask_rewards
 
     def random_batch(self, batch_size):
         indices = self._sample_indices(batch_size)
@@ -329,7 +327,7 @@ class ObsDictRelabelingBuffer(ObsDictReplayBuffer):
 
         new_actions = self._actions[indices]
 
-        if self.use_multitask_rewards: # isinstance(self.env, MultitaskEnv):
+        if isinstance(self.env, MultitaskEnv):
             new_rewards = self.env.compute_rewards(
                 new_actions,
                 new_next_obs_dict,
diff --git a/railrl/data_management/online_conditional_vae_replay_buffer.py b/railrl/data_management/online_conditional_vae_replay_buffer.py
index 1ef6f2c..6ebf065 100644
--- a/railrl/data_management/online_conditional_vae_replay_buffer.py
+++ b/railrl/data_management/online_conditional_vae_replay_buffer.py
@@ -27,7 +27,6 @@ from multiworld.core.multitask_env import MultitaskEnv
 from railrl.data_management.replay_buffer import ReplayBuffer
 import railrl.data_management.images as image_np
 from railrl.data_management.online_vae_replay_buffer import OnlineVaeRelabelingBuffer
-from railrl.torch.vae.conditional_conv_vae import DeltaCVAE
 
 class OnlineConditionalVaeRelabelingBuffer(OnlineVaeRelabelingBuffer):
     def random_vae_training_data(self, batch_size, epoch):
@@ -80,6 +79,8 @@ class OnlineConditionalVaeRelabelingBuffer(OnlineVaeRelabelingBuffer):
         obs_square_sum = np.zeros(np.array([self.vae.latent_sizes, ]).sum())
         while cur_idx < self._size:
             idxs = np.arange(cur_idx, next_idx)
+            #FIX THIS ASSUMES TRAJ LENGTH 100
+            import ipdb; ipdb.set_trace()
             x_0_idxs = (idxs // 100) * 100
 
             x_0 = ptu.from_numpy(self._obs[self.decoded_obs_key][x_0_idxs])
@@ -166,10 +167,7 @@ class OnlineConditionalVaeRelabelingBuffer(OnlineVaeRelabelingBuffer):
         new_next_obs_dict = self._batch_next_obs_dict(indices)
 
         if num_env_goals > 0:
-            if isinstance(self.env.vae, DeltaCVAE):
-                r1, r2 = self.env.vae.latent_sizes
-            else:
-                r1 = self.env.representation_size
+            r1, r2 = self.env.vae.latent_sizes
 
             env_goals = np.random.randn(num_env_goals, r1) # self._sample_goals_from_env(num_env_goals)
             last_env_goal_idx = num_rollout_goals + num_env_goals
diff --git a/railrl/data_management/online_vae_replay_buffer.py b/railrl/data_management/online_vae_replay_buffer.py
index ee2cbc3..074e6c6 100644
--- a/railrl/data_management/online_vae_replay_buffer.py
+++ b/railrl/data_management/online_vae_replay_buffer.py
@@ -47,7 +47,6 @@ class OnlineVaeRelabelingBuffer(ObsDictRelabelingBuffer):
             exploration_counter_kwargs=None,
             relabeling_goal_sampling_mode='vae_prior',
             decode_vae_goals=False,
-            save_decoded_to_internal_keys=True,
             **kwargs
     ):
         if internal_keys is None:
@@ -58,9 +57,8 @@ class OnlineVaeRelabelingBuffer(ObsDictRelabelingBuffer):
             decoded_achieved_goal_key,
             decoded_desired_goal_key
         ]:
-            if save_decoded_to_internal_keys and key not in internal_keys:
+            if key not in internal_keys:
                 internal_keys.append(key)
-
         super().__init__(internal_keys=internal_keys, *args, **kwargs)
         # assert isinstance(self.env, VAEWrappedEnv)
         self.vae = vae
diff --git a/railrl/demos/collect_demo.py b/railrl/demos/collect_demo.py
index fd8c19c..94e449f 100644
--- a/railrl/demos/collect_demo.py
+++ b/railrl/demos/collect_demo.py
@@ -24,19 +24,18 @@ import time
 # import robosuite.utils.transform_utils as T
 
 # from multiworld.envs.mujoco.sawyer_xyz.sawyer_multiple_objects import MultiSawyerEnv
-# from multiworld.core.image_env import ImageEnv
-# from multiworld.envs.mujoco.cameras import sawyer_pusher_camera_upright_v2
+from multiworld.core.image_env import ImageEnv
+from multiworld.envs.mujoco.cameras import sawyer_pusher_camera_upright_v2
 
 
 import sys
-import pickle
 
 ### workaround to solve cv2 version conflicts (ROS adds Python2 version of cv2)
 # sys.path.remove('/opt/ros/kinetic/lib/python2.7/dist-packages')
 # import cv2
 # sys.path.insert(0,'/opt/ros/kinetic/lib/python2.7/dist-packages')
 
-# import cv2
+import cv2
 
 class Expert:
     def __init__(self, action_dim=3, **kwargs):
@@ -93,7 +92,7 @@ class SpaceMouseExpert(Expert):
 
         return (a, valid, reset, accept)
 
-def collect_one_rollout(env, expert, horizon=200, render=False, pause=0):
+def collect_one_rollout(env, expert, horizon=200, render=False):
     o = env.reset()
 
     traj = dict(
@@ -106,7 +105,7 @@ def collect_one_rollout(env, expert, horizon=200, render=False, pause=0):
         env_infos=[],
     )
 
-    for i in range(10000):
+    for _ in range(horizon):
         a, valid, reset, accept = expert.get_action(o)
 
         if valid:
@@ -125,9 +124,6 @@ def collect_one_rollout(env, expert, horizon=200, render=False, pause=0):
             if render:
                 env.render()
 
-            if pause:
-                time.sleep(pause)
-
         if reset or accept:
             if len(traj["rewards"]) == 0:
                 accept = False
@@ -137,7 +133,7 @@ def collect_one_rollout(env, expert, horizon=200, render=False, pause=0):
 
     return False, []
 
-def draw_grid(img, line_color=(0, 0, 0), thickness=1, type_=None, pxstep=20):
+def draw_grid(img, line_color=(0, 0, 0), thickness=1, type_=cv2.LINE_AA, pxstep=20):
     '''(ndarray, 3-tuple, int, int) -> void
     draw gridlines on img
     line_color:
@@ -149,8 +145,6 @@ def draw_grid(img, line_color=(0, 0, 0), thickness=1, type_=None, pxstep=20):
     pxstep:
         grid line frequency in pixels
     '''
-    if type_ is None:
-        type_ = cv2.LINE_AA
     x = pxstep
     y = pxstep
     while x < img.shape[1]:
@@ -233,13 +227,11 @@ def collect_demos(env, expert, path="demos.npy", N=10, horizon=200):
     np.save(path, data)
 
 
-def collect_demos_fixed(env, expert, path="demos.npy", N=10, **kwargs):
+def collect_demos_fixed(env, expert, path="demos.npy", N=10, horizon=200, render=False):
     data = []
 
-    i = 0
-    while len(data) < N and i < 1000:
-        i += 1
-        accept, traj = collect_one_rollout(env, expert, **kwargs)
+    while len(data) < N:
+        accept, traj = collect_one_rollout(env, expert, horizon, render=render)
         if accept:
             data.append(traj)
             print("accepted trajectory length", len(traj["observations"]))
@@ -248,8 +240,7 @@ def collect_demos_fixed(env, expert, path="demos.npy", N=10, **kwargs):
         else:
             print("discarded trajectory")
 
-    # np.save(path, data)
-    pickle.dump(data, open(path, "wb"), protocol=2)
+    np.save(path, data)
 
 if __name__ == '__main__':
     # device = SpaceMouse()
@@ -284,4 +275,4 @@ if __name__ == '__main__':
     )
     # env.set_goal(env.sample_goals(1))
 
-    collect_demos(env, expert)
\ No newline at end of file
+    collect_demos(env, expert)
diff --git a/railrl/demos/play_demo.py b/railrl/demos/play_demo.py
index 395bc06..94b306e 100644
--- a/railrl/demos/play_demo.py
+++ b/railrl/demos/play_demo.py
@@ -1,26 +1,16 @@
 import numpy as np
-
-import sys
-# print(sys.path)
-sys.path.remove("/opt/ros/kinetic/lib/python2.7/dist-packages")
-
 import cv2
-import sys
-import pickle
 
 def play_demos(path):
-    data = pickle.load(open(path, "rb"))
-    # data = np.load(path, allow_pickle=True)
+    data = np.load(path)
 
     for traj in data:
         obs = traj["observations"]
 
         for o in obs:
-            img = o["image_observation"].reshape(3, 500, 300)[:, 60:, :240].transpose()
-            img = img[:, :, ::-1]
+            img = o["image_observation"].reshape((84, 84, 3))
             cv2.imshow('window', img)
             cv2.waitKey(100)
 
 if __name__ == '__main__':
-    demo_path = sys.argv[1]
-    play_demos(demo_path)
+    play_demos("/Users/ashvin/data/s3doodad/demos/multiobj1_demos_100.npy")
diff --git a/railrl/demos/spacemouse/README.md b/railrl/demos/spacemouse/README.md
index 2732e71..dd7f925 100644
--- a/railrl/demos/spacemouse/README.md
+++ b/railrl/demos/spacemouse/README.md
@@ -24,7 +24,7 @@ On the client, run the setup above. On the server, run:
 
 1. On the server, start the nameserver:
 ```export PYRO_SERIALIZERS_ACCEPTED=serpent,json,marshal,pickle
-python -m Pyro4.naming -n euler1.banatao.berkeley.edu
+python -m Pyro4.naming -n gauss1.banatao.berkeley.edu
 ```
 2. On the server, run a script that uses the `SpaceMouseExpert` imported from `railrl/demos/spacemouse/input_server.py` such as ```python experiments/ashvin/iros2019/collect_demos_spacemouse.py```
 2. On the client, run ```python railrl/demos/spacemouse/input_client.py```
diff --git a/railrl/demos/spacemouse/config.py b/railrl/demos/spacemouse/config.py
new file mode 100644
index 0000000..6cc64f7
--- /dev/null
+++ b/railrl/demos/spacemouse/config.py
@@ -0,0 +1 @@
+HOSTNAME = "192.168.0.100"
diff --git a/railrl/demos/spacemouse/input_client.py b/railrl/demos/spacemouse/input_client.py
index 80122d4..f5d013c 100644
--- a/railrl/demos/spacemouse/input_client.py
+++ b/railrl/demos/spacemouse/input_client.py
@@ -2,7 +2,7 @@
 Should be run on a machine connected to a spacemouse
 """
 
-# from robosuite.devices import SpaceMouse
+from robosuite.devices import SpaceMouse
 import time
 import Pyro4
 from railrl.demos.spacemouse.config import HOSTNAME
@@ -13,9 +13,9 @@ Pyro4.config.SERIALIZER='pickle'
 nameserver = Pyro4.locateNS(host=HOSTNAME)
 uri = nameserver.lookup("example.greeting")
 device_state = Pyro4.Proxy(uri)
-# device = SpaceMouse()
-# while True:
-#     state = device.get_controller_state()
-#     print(state)
-#     time.sleep(0.1)
-#     device_state.set_state(state)
+device = SpaceMouse()
+while True:
+    state = device.get_controller_state()
+    print(state)
+    time.sleep(0.1)
+    device_state.set_state(state)
diff --git a/railrl/demos/spacemouse/input_server.py b/railrl/demos/spacemouse/input_server.py
index 6d75e0d..91921f9 100644
--- a/railrl/demos/spacemouse/input_server.py
+++ b/railrl/demos/spacemouse/input_server.py
@@ -33,14 +33,12 @@ class SpaceMouseExpert:
         self.xyz_remap = np.array(xyz_remap)
         self.xyz_scale = np.array(xyz_scale)
         self.thread = Thread(target = start_server)
-        self.thread.daemon = True
         self.thread.start()
         self.device_state = DeviceState()
 
     def get_action(self, obs):
         """Must return (action, valid, reset, accept)"""
         state = self.device_state.get_state()
-        print(state)
         if state is None:
             return None, False, False, False
 
@@ -74,4 +72,4 @@ if __name__ == "__main__":
 
     for i in range(100):
         time.sleep(1)
-        print(expert.get_action(None))
\ No newline at end of file
+        print(expert.get_action(None))
diff --git a/railrl/demos/td3_bc.py b/railrl/demos/td3_bc.py
index 5bca0d4..cd58498 100644
--- a/railrl/demos/td3_bc.py
+++ b/railrl/demos/td3_bc.py
@@ -7,7 +7,7 @@ from torch import nn as nn
 
 import railrl.torch.pytorch_util as ptu
 from railrl.misc.eval_util import create_stats_ordered_dict
-from railrl.torch.torch_rl_algorithm import TorchTrainer
+from railrl.torch.torch_rl_algorithm import TorchRLAlgorithm
 
 from railrl.misc.asset_loader import load_local_or_remote_file
 
@@ -15,13 +15,7 @@ import random
 from railrl.torch.core import PyTorchModule, np_to_pytorch_batch
 from railrl.data_management.path_builder import PathBuilder
 
-import matplotlib
-matplotlib.use('TkAgg')
-import matplotlib.pyplot as plt
-
-from railrl.core import logger
-
-class TD3BCTrainer(TorchTrainer):
+class TD3BC(TorchRLAlgorithm):
     """
     Twin Delayed Deep Deterministic policy gradients
     """
@@ -35,51 +29,47 @@ class TD3BCTrainer(TorchTrainer):
             target_qf1,
             target_qf2,
             target_policy,
+            exploration_policy,
             demo_path,
-            replay_buffer,
             demo_train_buffer,
             demo_test_buffer,
-            demo_off_policy_path=[],
             apply_her_to_demos=False,
             add_demo_latents=False,
             demo_train_split=0.9,
             add_demos_to_replay_buffer=True,
-            bc_num_pretrain_steps=0,
-            bc_batch_size=64,
             bc_weight=1.0,
             rl_weight=1.0,
-            q_num_pretrain_steps=0,
             weight_decay=0,
             eval_policy=None,
 
-            reward_scale=1.0,
-            discount=0.99,
             target_policy_noise=0.2,
             target_policy_noise_clip=0.5,
 
             policy_learning_rate=1e-3,
             qf_learning_rate=1e-3,
-            target_update_period=2,
-            policy_update_period=10,
+            policy_and_target_update_period=2,
             tau=0.005,
             qf_criterion=None,
             optimizer_class=optim.Adam,
 
             **kwargs
     ):
-        super().__init__()
+        super().__init__(
+            env,
+            exploration_policy,
+            eval_policy=eval_policy or policy,
+            **kwargs
+        )
         if qf_criterion is None:
             qf_criterion = nn.MSELoss()
         self.qf1 = qf1
         self.qf2 = qf2
         self.policy = policy
-        self.env = env
 
         self.target_policy_noise = target_policy_noise
         self.target_policy_noise_clip = target_policy_noise_clip
 
-        self.target_update_period = target_update_period
-        self.policy_update_period = policy_update_period
+        self.policy_and_target_update_period = policy_and_target_update_period
         self.tau = tau
         self.qf_criterion = qf_criterion
 
@@ -99,79 +89,62 @@ class TD3BCTrainer(TorchTrainer):
             lr=policy_learning_rate,
             weight_decay=weight_decay,
         )
-        self.bc_batch_size = bc_batch_size
         self.bc_weight = bc_weight
         self.rl_weight = rl_weight
 
-        self.discount = discount
-        self.reward_scale = reward_scale
-
         self.add_demos_to_replay_buffer = add_demos_to_replay_buffer
         self.demo_train_split = demo_train_split
-        self.replay_buffer = replay_buffer
         self.demo_train_buffer = demo_train_buffer
         self.demo_test_buffer = demo_test_buffer
         self.add_demo_latents = add_demo_latents
         self.apply_her_to_demos = apply_her_to_demos
 
         self.demo_path = demo_path
-        self.demo_off_policy_path = demo_off_policy_path
-
-        self.eval_statistics = OrderedDict()
-        self._n_train_steps_total = 0
-        self._need_to_update_eval_statistics = True
-
-        self.bc_num_pretrain_steps = bc_num_pretrain_steps
-        self.q_num_pretrain_steps = q_num_pretrain_steps
-        self.demo_trajectory_rewards = []
+        self.load_demos(self.demo_path)
 
     def _update_obs_with_latent(self, obs):
         latent_obs = self.env._encode_one(obs["image_observation"])
-        latent_goal = np.array([]) # self.env._encode_one(obs["image_desired_goal"])
+        latent_goal = self.env._encode_one(obs["image_desired_goal"])
         obs['latent_observation'] = latent_obs
-        obs['latent_achieved_goal'] = latent_goal
+        obs['latent_achieved_goal'] = latent_obs
         obs['latent_desired_goal'] = latent_goal
         obs['observation'] = latent_obs
-        obs['achieved_goal'] = latent_goal
+        obs['achieved_goal'] = latent_obs
         obs['desired_goal'] = latent_goal
         return obs
 
     def load_path(self, path, replay_buffer):
-        # print("Loading path: ", path)
-        # print("Path len", len(path))
-        # print("Path observations: ", type(path), type(path[0]), print(path[0].keys()))
-        # import ipdb; ipdb.set_trace()
-        path = path[0]
         final_achieved_goal = path["observations"][-1]["state_achieved_goal"].copy()
-        rewards = []
         path_builder = PathBuilder()
-
-        print("loading path, length", len(path["observations"]), len(path["actions"]))
-        H = min(len(path["observations"]), len(path["actions"]))
-        print("actions", np.min(path["actions"]), np.max(path["actions"]))
-
-        zs = []
-        for i in range(H):
-            ob = path["observations"][i]
-            action = path["actions"][i]
-            reward = path["rewards"][i]
-            next_ob = path["next_observations"][i]
-            terminal = path["terminals"][i]
-            agent_info = path["agent_infos"][i]
-            env_info = path["env_infos"][i]
-
-            zs.append(ob['latent_observation'])
+        for (
+            ob,
+            action,
+            reward,
+            next_ob,
+            terminal,
+            agent_info,
+            env_info,
+        ) in zip(
+            path["observations"],
+            path["actions"],
+            path["rewards"],
+            path["next_observations"],
+            path["terminals"],
+            path["agent_infos"],
+            path["env_infos"],
+        ):
             # goal = path["goal"]["state_desired_goal"][0, :]
             # import pdb; pdb.set_trace()
             # print(goal.shape, ob["state_observation"])
             # state_observation = np.concatenate((ob["state_observation"], goal))
-            # action = action[:2]
+            action = action[:2]
             if self.add_demo_latents:
                 self._update_obs_with_latent(ob)
                 self._update_obs_with_latent(next_ob)
                 reward = self.env.compute_reward(
                     action,
-                    next_ob,
+                    {'latent_achieved_goal': next_ob['latent_achieved_goal'],
+                     'latent_desired_goal': next_ob['latent_desired_goal']}
                 )
             if self.apply_her_to_demos:
                 ob["state_desired_goal"] = final_achieved_goal
@@ -180,15 +153,8 @@ class TD3BCTrainer(TorchTrainer):
                     action,
                     next_ob,
                 )
-
-            reward = self.env.compute_reward(
-                action,
-                next_ob,
-            )
-
             reward = np.array([reward])
-            rewards.append(reward)
-            terminal = np.array([terminal]).reshape((1, ))
+            terminal = np.array([terminal])
             path_builder.add_all(
                 observations=ob,
                 actions=action,
@@ -198,127 +164,68 @@ class TD3BCTrainer(TorchTrainer):
                 agent_infos=agent_info,
                 env_infos=env_info,
             )
-        self.demo_trajectory_rewards.append(rewards)
         path = path_builder.get_all_stacked()
         replay_buffer.add_path(path)
-        self.env.initialize(zs)
-
-    def load_demos(self, ):
-        # Off policy
-        if type(self.demo_off_policy_path) is list:
-            for demo_path in self.demo_off_policy_path:
-                self.load_demo_path(demo_path, False)
-        else:
-            self.load_demo_path(self.demo_off_policy_path, False)
-        
-        if type(self.demo_path) is list:
-            for demo_path in self.demo_path:
-                self.load_demo_path(demo_path)
-        else:
-            self.load_demo_path(self.demo_path)
-
-
-    # Parameterize which demo is being tested (and all jitter variants)
-    # If on_policy is False, we only add the demos to the
-    # replay buffer, and not to the demo_test or demo_train buffers
-    def load_demo_path(self, demo_path, on_policy=True):
-        data = list(load_local_or_remote_file(demo_path))
-        if not on_policy:
-            data = [data]
-        # random.shuffle(data)
+
+    def load_demos(self, demo_path):
+        data = load_local_or_remote_file(demo_path)
+        random.shuffle(data)
         N = int(len(data) * self.demo_train_split)
         print("using", N, "paths for training")
 
-        if on_policy:
-            for path in data[:N]:
-                self.load_path(path, self.demo_train_buffer)
-
-        plt.figure(figsize=(8, 8))
-        for r in self.demo_trajectory_rewards:
-            plt.plot(r)
-        plt.savefig("demo_rewards.png")
+        for path in data[:N]:
+            self.load_path(path, self.demo_train_buffer)
 
         if self.add_demos_to_replay_buffer:
             for path in data[:N]:
                 self.load_path(path, self.replay_buffer)
 
-        if on_policy:
-            for path in data[N:]:
-                self.load_path(path, self.demo_test_buffer)
-
-    def get_batch_from_buffer(self, replay_buffer):
-        batch = replay_buffer.random_batch(self.bc_batch_size)
-        batch = np_to_pytorch_batch(batch)
-        # obs = batch['observations']
-        # next_obs = batch['next_observations']
-        # goals = batch['resampled_goals']
-        # import ipdb; ipdb.set_trace()
-        # batch['observations'] = torch.cat((
-        #     obs,
-        #     goals
-        # ), dim=1)
-        # batch['next_observations'] = torch.cat((
-        #     next_obs,
-        #     goals
-        # ), dim=1)
-        return batch
-
-    def pretrain_policy_with_bc(self):
-        for i in range(self.bc_num_pretrain_steps):
-            train_batch = self.get_batch_from_buffer(self.demo_train_buffer)
-            train_o = train_batch["observations"]
-            train_u = train_batch["actions"]
-            train_pred_u = self.policy(train_o)
-            train_error = (train_pred_u - train_u) ** 2
-            train_bc_loss = train_error.mean()
-
-            policy_loss = self.bc_weight * train_bc_loss.mean()
-
-            self.policy_optimizer.zero_grad()
-            policy_loss.backward()
-            self.policy_optimizer.step()
-
-            test_batch = self.get_batch_from_buffer(self.demo_test_buffer)
-            test_o = test_batch["observations"]
-            test_u = test_batch["actions"]
-            test_pred_u = self.policy(test_o)
-            test_error = (test_pred_u - test_u) ** 2
-            test_bc_loss = test_error.mean()
-
-            train_loss_mean = np.mean(ptu.get_numpy(train_bc_loss))
-            test_loss_mean = np.mean(ptu.get_numpy(test_bc_loss))
-
-            stats = {
-                "pretrain_bc/train_loss_mean": train_loss_mean,
-                "pretrain_bc/test_loss_mean": test_loss_mean,
-            }
-            logger.record_dict(stats)
-            logger.dump_tabular(with_prefix=True, with_timestamp=False)
-
-
-    def pretrain_q_with_bc_data(self):
-        logger.push_tabular_prefix("pretrain_q/")
-        for i in range(self.q_num_pretrain_steps):
-            # self.eval_statistics = dict()
-            # self._need_to_update_eval_statistics = True
-
-            train_data = self.replay_buffer.random_batch(128)
-            self.train(train_data)
-
-            # logger.record_dict(self.eval_statistics)
-            # logger.dump_tabular(with_prefix=True, with_timestamp=False)
-        logger.pop_tabular_prefix()
-
-    def train_from_torch(self, batch):
-        logger.push_tabular_prefix("train_q/")
-        self.eval_statistics = dict()
-        self._need_to_update_eval_statistics = True
+        for path in data[N:]:
+            self.load_path(path, self.demo_test_buffer)
 
+    def _do_training(self):
+        batch = self.get_batch()
         rewards = batch['rewards']
         terminals = batch['terminals']
         obs = batch['observations']
         actions = batch['actions']
         next_obs = batch['next_observations']
+        self._train_given_data(
+            rewards,
+            terminals,
+            obs,
+            actions,
+            next_obs,
+        )
+
+    def get_batch_from_buffer(self, replay_buffer):
+        batch = replay_buffer.random_batch(self.batch_size)
+        batch = np_to_pytorch_batch(batch)
+        obs = batch['observations']
+        next_obs = batch['next_observations']
+        goals = batch['resampled_goals']
+        batch['observations'] = torch.cat((
+            obs,
+            goals
+        ), dim=1)
+        batch['next_observations'] = torch.cat((
+            next_obs,
+            goals
+        ), dim=1)
+        return batch
+
+    def _train_given_data(
+        self,
+        rewards,
+        terminals,
+        obs,
+        actions,
+        next_obs,
+        logger_prefix="",
+    ):
+        """
+        Critic operations.
+        """
 
         next_actions = self.target_policy(next_obs)
         noise = ptu.randn(next_actions.shape) * self.target_policy_noise
@@ -355,7 +262,7 @@ class TD3BCTrainer(TorchTrainer):
         self.qf2_optimizer.step()
 
         policy_actions = policy_loss = None
-        if self._n_train_steps_total % self.policy_update_period == 0:
+        if self._n_train_steps_total % self.policy_and_target_update_period == 0:
             policy_actions = self.policy(obs)
             q_output = self.qf1(obs, policy_actions)
 
@@ -372,50 +279,47 @@ class TD3BCTrainer(TorchTrainer):
             policy_loss.backward()
             self.policy_optimizer.step()
 
-
-            self.eval_statistics['Policy Loss'] = np.mean(ptu.get_numpy(
-                policy_loss
-            ))
-            self.eval_statistics['BC Loss'] = np.mean(ptu.get_numpy(
-                train_bc_loss
-            ))
-
-        if self._n_train_steps_total % self.target_update_period == 0:
             ptu.soft_update_from_to(self.policy, self.target_policy, self.tau)
             ptu.soft_update_from_to(self.qf1, self.target_qf1, self.tau)
             ptu.soft_update_from_to(self.qf2, self.target_qf2, self.tau)
 
-        if self._need_to_update_eval_statistics:
-            self._need_to_update_eval_statistics = False
+        if self.need_to_update_eval_statistics:
+            self.need_to_update_eval_statistics = False
             if policy_loss is None:
                 policy_actions = self.policy(obs)
                 q_output = self.qf1(obs, policy_actions)
                 policy_loss = - q_output.mean()
 
-            self.eval_statistics['QF1 Loss'] = np.mean(ptu.get_numpy(qf1_loss))
-            self.eval_statistics['QF2 Loss'] = np.mean(ptu.get_numpy(qf2_loss))
+            self.eval_statistics[logger_prefix + 'QF1 Loss'] = np.mean(ptu.get_numpy(qf1_loss))
+            self.eval_statistics[logger_prefix + 'QF2 Loss'] = np.mean(ptu.get_numpy(qf2_loss))
+            self.eval_statistics[logger_prefix + 'Policy Loss'] = np.mean(ptu.get_numpy(
+                policy_loss
+            ))
+            self.eval_statistics[logger_prefix + 'BC Loss'] = np.mean(ptu.get_numpy(
+                train_bc_loss
+            ))
             self.eval_statistics.update(create_stats_ordered_dict(
-                'Q1 Predictions',
+                logger_prefix + 'Q1 Predictions',
                 ptu.get_numpy(q1_pred),
             ))
             self.eval_statistics.update(create_stats_ordered_dict(
-                'Q2 Predictions',
+                logger_prefix + 'Q2 Predictions',
                 ptu.get_numpy(q2_pred),
             ))
             self.eval_statistics.update(create_stats_ordered_dict(
-                'Q Targets',
+                logger_prefix + 'Q Targets',
                 ptu.get_numpy(q_target),
             ))
             self.eval_statistics.update(create_stats_ordered_dict(
-                'Bellman Errors 1',
+                logger_prefix + 'Bellman Errors 1',
                 ptu.get_numpy(bellman_errors_1),
             ))
             self.eval_statistics.update(create_stats_ordered_dict(
-                'Bellman Errors 2',
+                logger_prefix + 'Bellman Errors 2',
                 ptu.get_numpy(bellman_errors_2),
             ))
             self.eval_statistics.update(create_stats_ordered_dict(
-                'Policy Action',
+                logger_prefix + 'Policy Action',
                 ptu.get_numpy(policy_actions),
             ))
 
@@ -425,22 +329,24 @@ class TD3BCTrainer(TorchTrainer):
             test_pred_u = self.policy(test_o)
             test_error = (test_pred_u - test_u) ** 2
             test_bc_loss = test_error.mean()
-            self.eval_statistics['Test BC Loss'] = np.mean(ptu.get_numpy(
+            self.eval_statistics[logger_prefix + 'Test BC Loss'] = np.mean(ptu.get_numpy(
                 test_bc_loss
             ))
-        self._n_train_steps_total += 1
-
-        logger.record_dict(self.eval_statistics)
-        logger.dump_tabular(with_prefix=True, with_timestamp=False)
-        logger.pop_tabular_prefix()
 
-    def get_diagnostics(self):
-        stats = super().get_diagnostics()
-        stats.update(self.eval_statistics)
-        return stats
+    def get_epoch_snapshot(self, epoch):
+        snapshot = super().get_epoch_snapshot(epoch)
+        self.update_epoch_snapshot(snapshot)
+        return snapshot
 
-    def end_epoch(self, epoch):
-        self._need_to_update_eval_statistics = True
+    def update_epoch_snapshot(self, snapshot):
+        snapshot.update(
+            qf1=self.qf1,
+            qf2=self.qf2,
+            policy=self.eval_policy,
+            trained_policy=self.policy,
+            target_policy=self.target_policy,
+            exploration_policy=self.exploration_policy,
+        )
 
     @property
     def networks(self):
@@ -452,11 +358,3 @@ class TD3BCTrainer(TorchTrainer):
             self.target_qf1,
             self.target_qf2,
         ]
-
-    def get_snapshot(self):
-        return dict(
-            qf1=self.qf1,
-            qf2=self.qf2,
-            trained_policy=self.policy,
-            target_policy=self.target_policy,
-        )
diff --git a/railrl/envs/vae_wrappers.py b/railrl/envs/vae_wrappers.py
index ede73ad..5c58367 100644
--- a/railrl/envs/vae_wrappers.py
+++ b/railrl/envs/vae_wrappers.py
@@ -3,7 +3,8 @@ import random
 import warnings
 
 import torch
-
+import sys
+#sys.path.remove('/opt/ros/kinetic/lib/python2.7/dist-packages')
 # import cv2
 import numpy as np
 from gym import Env
@@ -30,7 +31,7 @@ class VAEWrappedEnv(ProxyEnv, MultitaskEnv):
         decode_goals=False,
         decode_goals_on_reset=True,
         render_goals=False,
-        render_rollouts=False,
+        render_rollouts=False, #change this back
         reward_params=None,
         goal_sampling_mode="vae_prior",
         imsize=84,
@@ -96,13 +97,13 @@ class VAEWrappedEnv(ProxyEnv, MultitaskEnv):
         self._custom_goal_sampler = None
         self._goal_sampling_mode = goal_sampling_mode
 
-
     def reset(self):
         self.vae.eval()
         obs = self.wrapped_env.reset()
         self._initial_obs = obs
         goal = self.sample_goal()
         self.set_goal(goal)
+        #obs = self.wrapped_env.reset()
 
         if self.decode_goals_on_reset:
             reconstructions, _ = self.vae.decode(ptu.from_numpy(goal["latent_desired_goal"]).view(1, -1))
@@ -114,6 +115,26 @@ class VAEWrappedEnv(ProxyEnv, MultitaskEnv):
         obs = self._update_obs(obs)
         return obs
 
+
+    # def reset(self):
+    #     self.vae.eval()
+    #     obs = self.wrapped_env.reset()
+    #     self._initial_obs = obs
+    #     goal = self.sample_goal()
+    #     self.set_goal(goal)
+    #     obs = self.wrapped_env.reset()
+    #     # self._initial_obs = obs
+
+    #     if self.decode_goals_on_reset:
+    #         reconstructions, _ = self.vae.decode(ptu.from_numpy(goal["latent_desired_goal"]).view(1, -1))
+    #         imgs = ptu.get_numpy(reconstructions)
+    #         self.decoded_goal_image = imgs.reshape(
+    #             self.input_channels, self.imsize, self.imsize
+    #         ).flatten()
+
+    #     obs = self._update_obs(obs)
+    #     return obs
+
     def step(self, action):
         self.vae.eval()
         obs, reward, done, info = self.wrapped_env.step(action)
@@ -220,6 +241,7 @@ class VAEWrappedEnv(ProxyEnv, MultitaskEnv):
 
     def compute_reward(self, action, obs):
         actions = action[None]
+        
         next_obs = {
             k: v[None] for k, v in obs.items()
         }
@@ -373,6 +395,7 @@ class VAEWrappedEnv(ProxyEnv, MultitaskEnv):
         self.render_rollouts = False
 
     def try_render(self, obs):
+        return
         if self.render_rollouts:
             img = obs['image_observation'].reshape(
                 self.input_channels,
diff --git a/railrl/envs/wrappers.py b/railrl/envs/wrappers.py
index 4ca1e27..d9ee5cd 100644
--- a/railrl/envs/wrappers.py
+++ b/railrl/envs/wrappers.py
@@ -9,6 +9,8 @@ from PIL import Image
 from collections import deque
 import mujoco_py
 import torch
+import sys
+# sys.path.remove('/opt/ros/kinetic/lib/python2.7/dist-packages')
 # import cv2
 
 
diff --git a/railrl/exploration_strategies/PlanningStrategy.py b/railrl/exploration_strategies/PlanningStrategy.py
new file mode 100644
index 0000000..7fa82e7
--- /dev/null
+++ b/railrl/exploration_strategies/PlanningStrategy.py
@@ -0,0 +1,16 @@
+import random
+from railrl.exploration_strategies.base import RawExplorationStrategy
+import numpy as np
+
+
+class PlanningStrategy(RawExplorationStrategy):
+    """
+    With probability epsilon, take a completely random action.
+    with probability 1-epsilon, add Gaussian noise to the action taken by a
+    deterministic policy.
+    """
+    def __init__(self, action_dim):
+        self.action_dim = action_dim
+
+    def get_action_from_raw_action(self, action, t=None, **kwargs):
+        return action[:self.action_dim]
\ No newline at end of file
diff --git a/railrl/exploration_strategies/base.py b/railrl/exploration_strategies/base.py
index cea59ef..803948f 100644
--- a/railrl/exploration_strategies/base.py
+++ b/railrl/exploration_strategies/base.py
@@ -1,5 +1,5 @@
 import abc
-
+import numpy as np
 from railrl.policies.base import ExplorationPolicy
 
 
@@ -73,3 +73,56 @@ class PolicyWrappedWithExplorationStrategy(ExplorationPolicy):
 
     def to(self, device):
         self.policy.to(device)
+
+class PolicyWrappedWithPlanner(ExplorationPolicy):
+    def __init__(
+            self,
+            agent,
+            planner,
+            state_dim,
+    ):
+        self.agent = agent
+        self.planner = planner
+        self.state_dim = state_dim
+        self.subgoal = None
+        self.t = 0
+
+    def set_num_steps_total(self, t):
+        self.t = t
+
+    def get_action(self, observation):
+        if self.t % 5 == 0:
+            self.subgoal, agent_info = self.planner.get_action(observation)
+            
+        curr_state = observation[:self.state_dim]
+        abstract_obs = np.concatenate((curr_state, self.subgoal))
+        action, agent_info = self.agent.get_action(abstract_obs)
+        return action, agent_info
+
+    def get_actions(self, *args, **kwargs):
+        return self.es.get_actions(self.t, self.policy, *args, **kwargs)
+
+    def reset(self):
+        self.planner.reset()
+        self.agent.reset()
+        self.subgoal = None
+        self.t = 0
+
+    def get_param_values(self):
+        import ipdb; ipdb.set_trace()
+        return self.policy.get_param_values()
+
+    def set_param_values(self, param_values):
+        import ipdb; ipdb.set_trace()
+        self.policy.set_param_values(param_values)
+
+    def get_param_values_np(self):
+        import ipdb; ipdb.set_trace()
+        return self.policy.get_param_values_np()
+
+    def set_param_values_np(self, param_values):
+        import ipdb; ipdb.set_trace()
+        self.policy.set_param_values_np(param_values)
+
+    def to(self, device):
+        self.policy.to(device)
diff --git a/railrl/launchers/experiments/ashvin/multiworld.py b/railrl/launchers/experiments/ashvin/multiworld.py
index 6cb37a6..93a0e1b 100644
--- a/railrl/launchers/experiments/ashvin/multiworld.py
+++ b/railrl/launchers/experiments/ashvin/multiworld.py
@@ -287,3 +287,1025 @@ def her_td3_experiment(variant):
     algorithm.to(ptu.device)
     algorithm.train()
 
+
+def her_sap_model_experiment(variant):
+    import gym
+    import railrl.torch.pytorch_util as ptu
+    from railrl.data_management.obs_dict_replay_buffer import ObsDictRelabelingBuffer
+    from railrl.exploration_strategies.base import \
+        PolicyWrappedWithPlanner
+    from railrl.launchers.launcher_util import setup_logger
+    from railrl.samplers.data_collector import GoalConditionedPathCollector
+    from railrl.torch.sac.policies import MakeDeterministic
+    from railrl.torch.her.her import HERTrainer
+    from railrl.torch.sac.policies import TanhGaussianPolicy, GaussianPolicy
+    from railrl.torch.networks import FlattenMlp, TanhMlpPolicy, MlpPolicy
+    from railrl.torch.sac.sap import SAPTrainer
+    from railrl.torch.torch_rl_algorithm import TorchBatchRLAlgorithm
+    import railrl.samplers.rollout_functions as rf
+    from railrl.torch.grill.launcher import get_state_experiment_video_save_function
+
+    if 'env_id' in variant:
+        eval_env = gym.make(variant['env_id'])
+        expl_env = gym.make(variant['env_id'])
+    else:
+        eval_env_kwargs = variant.get('eval_env_kwargs', variant['env_kwargs'])
+        eval_env = variant['env_class'](**eval_env_kwargs)
+        expl_env = variant['env_class'](**variant['env_kwargs'])
+
+    observation_key = 'state_achieved_goal'
+    desired_goal_key = 'state_desired_goal'
+    achieved_goal_key = desired_goal_key.replace("desired", "achieved")
+
+    obs_dim = (
+            expl_env.observation_space.spaces['achieved_goal'].low.size
+            + expl_env.observation_space.spaces['desired_goal'].low.size
+    )
+    state_dim = expl_env.observation_space.spaces['achieved_goal'].low.size
+    action_dim = expl_env.action_space.low.size
+    hidden_sizes = variant.get('hidden_sizes', [400, 300])
+
+    qf1 = FlattenMlp(
+        input_size=obs_dim + action_dim,
+        output_size=1,
+        hidden_sizes=hidden_sizes,
+    )
+    qf2 = FlattenMlp(
+        input_size=obs_dim + action_dim,
+        output_size=1,
+        hidden_sizes=hidden_sizes,
+    )
+    target_qf1 = FlattenMlp(
+        input_size=obs_dim + action_dim,
+        output_size=1,
+        hidden_sizes=hidden_sizes,
+    )
+    target_qf2 = FlattenMlp(
+        input_size=obs_dim + action_dim,
+        output_size=1,
+        hidden_sizes=hidden_sizes,
+    )
+    actor_policy = TanhGaussianPolicy(
+        obs_dim=obs_dim,
+        action_dim=action_dim,
+        hidden_sizes=hidden_sizes,
+    )
+
+    planner_policy = TanhGaussianPolicy(
+        obs_dim=obs_dim,
+        action_dim=state_dim,
+        hidden_sizes=hidden_sizes,
+    )
+
+    model = FlattenMlp(
+        input_size=state_dim + action_dim,
+        output_size=state_dim,
+        hidden_sizes=[128, 128, 128, 128, 128,],
+    )
+
+    eval_policy = PolicyWrappedWithPlanner(
+        MakeDeterministic(actor_policy),
+        MakeDeterministic(planner_policy),
+        state_dim)
+
+    expl_policy = PolicyWrappedWithPlanner(
+        actor_policy,
+        planner_policy,
+        state_dim)
+
+    trainer = SAPTrainer(
+        env=eval_env,
+        actor_policy=actor_policy,
+        planner_policy=planner_policy,
+        model=model,
+        qf1=qf1,
+        qf2=qf2,
+        target_qf1=target_qf1,
+        target_qf2=target_qf2,
+        state_dim=state_dim,
+        act_dim=action_dim,
+        **variant['twin_sac_trainer_kwargs']
+    )
+    trainer = HERTrainer(trainer)
+    eval_path_collector = GoalConditionedPathCollector(
+        eval_env,
+        eval_policy,
+        observation_key=observation_key,
+        desired_goal_key=desired_goal_key,
+    )
+    expl_path_collector = GoalConditionedPathCollector(
+        eval_env,
+        expl_policy,
+        observation_key=observation_key,
+        desired_goal_key=desired_goal_key,
+    )
+
+    replay_buffer = ObsDictRelabelingBuffer(
+        env=eval_env,
+        observation_key=observation_key,
+        desired_goal_key=desired_goal_key,
+        achieved_goal_key=achieved_goal_key,
+        **variant['replay_buffer_kwargs']
+    )
+
+    algorithm = TorchBatchRLAlgorithm(
+        trainer=trainer,
+        exploration_env=expl_env,
+        evaluation_env=eval_env,
+        exploration_data_collector=expl_path_collector,
+        evaluation_data_collector=eval_path_collector,
+        replay_buffer=replay_buffer,
+        **variant['algo_kwargs']
+    )
+
+    if variant.get("save_video", False):
+        rollout_function = rf.create_rollout_function(
+            rf.multitask_rollout,
+            max_path_length=algorithm.max_path_length,
+            observation_key=observation_key,
+            desired_goal_key=desired_goal_key,
+        )
+        eval_video_func = get_state_experiment_video_save_function(
+            rollout_function,
+            eval_env,
+            expl_policy,
+            variant,
+        )
+        algorithm.post_epoch_funcs.append(eval_video_func)
+    algorithm.to(ptu.device)
+    algorithm.train()
+
+def her_sap_experiment(variant):
+    import gym
+
+    import railrl.torch.pytorch_util as ptu
+    from railrl.data_management.obs_dict_replay_buffer import ObsDictRelabelingBuffer
+    from railrl.exploration_strategies.base import \
+        PolicyWrappedWithPlanner
+    from railrl.exploration_strategies.PlanningStrategy import \
+        PlanningStrategy
+    from railrl.launchers.launcher_util import setup_logger
+    from railrl.samplers.data_collector import GoalConditionedPathCollector
+    from railrl.torch.sac.policies import MakeDeterministic
+    from railrl.torch.her.her import HERTrainer
+    from railrl.torch.sac.policies import TanhGaussianPolicy, GaussianPolicy
+    from railrl.torch.networks import FlattenMlp, TanhMlpPolicy, MlpPolicy
+    from railrl.torch.networks import FlattenMlp
+    from railrl.torch.sac.sap_seperate import SAPTrainer
+    from railrl.torch.sac.sac_dist import ZACTrainer
+    from railrl.torch.sac.sac import SACTrainer
+    from railrl.torch.torch_rl_algorithm import TorchBatchRLAlgorithm
+    import railrl.samplers.rollout_functions as rf
+    from railrl.torch.grill.launcher import get_state_experiment_video_save_function
+
+    if 'env_id' in variant:
+        eval_env = gym.make(variant['env_id'])
+        expl_env = gym.make(variant['env_id'])
+    else:
+        eval_env_kwargs = variant.get('eval_env_kwargs', variant['env_kwargs'])
+        eval_env = variant['env_class'](**eval_env_kwargs)
+        expl_env = variant['env_class'](**variant['env_kwargs'])
+
+    observation_key = 'state_achieved_goal'
+    desired_goal_key = 'state_desired_goal'
+    achieved_goal_key = desired_goal_key.replace("desired", "achieved")
+
+    obs_dim = (
+            expl_env.observation_space.spaces['achieved_goal'].low.size
+            + expl_env.observation_space.spaces['desired_goal'].low.size
+    )
+    action_dim = expl_env.action_space.low.size
+    hidden_sizes = variant.get('hidden_sizes', [400, 300])
+
+    qf1 = FlattenMlp(
+        input_size=obs_dim + action_dim,
+        output_size=1,
+        hidden_sizes=hidden_sizes,
+    )
+    qf2 = FlattenMlp(
+        input_size=obs_dim + action_dim,
+        output_size=1,
+        hidden_sizes=hidden_sizes,
+    )
+    target_qf1 = FlattenMlp(
+        input_size=obs_dim + action_dim,
+        output_size=1,
+        hidden_sizes=hidden_sizes,
+    )
+    target_qf2 = FlattenMlp(
+        input_size=obs_dim + action_dim,
+        output_size=1,
+        hidden_sizes=hidden_sizes,
+    )
+    actor_policy = TanhGaussianPolicy(
+        obs_dim=obs_dim,
+        action_dim=action_dim,
+        hidden_sizes=hidden_sizes,
+    )
+
+    planner_policy = TanhGaussianPolicy(
+        obs_dim=obs_dim,
+        action_dim=obs_dim,#expl_env.observation_space.spaces['observation'].low.size,
+        hidden_sizes=hidden_sizes,
+    )
+
+    eval_policy = PolicyWrappedWithPlanner(
+        MakeDeterministic(actor_policy),
+        MakeDeterministic(planner_policy),
+        expl_env.observation_space.spaces['observation'].low.size,
+        eval_env)
+
+    expl_policy = PolicyWrappedWithPlanner(
+        actor_policy,
+        planner_policy,
+        expl_env.observation_space.spaces['observation'].low.size,
+        eval_env)
+
+    trainer = SAPTrainer(
+        env=eval_env,
+        actor_policy=actor_policy,
+        planner_policy=planner_policy,
+        qf1=qf1,
+        qf2=qf2,
+        target_qf1=target_qf1,
+        target_qf2=target_qf2,
+        act_dim=expl_env.action_space.low.size,
+        obs_dim=obs_dim,
+        **variant['twin_sac_trainer_kwargs']
+    )
+    trainer = HERTrainer(trainer)
+    eval_path_collector = GoalConditionedPathCollector(
+        eval_env,
+        eval_policy,
+        observation_key=observation_key,
+        desired_goal_key=desired_goal_key,
+    )
+    expl_path_collector = GoalConditionedPathCollector(
+        eval_env,
+        expl_policy, #THIS IS JUST ACTOR
+        observation_key=observation_key,
+        desired_goal_key=desired_goal_key,
+    )
+
+    replay_buffer = ObsDictRelabelingBuffer(
+        env=eval_env,
+        observation_key=observation_key,
+        desired_goal_key=desired_goal_key,
+        achieved_goal_key=achieved_goal_key,
+        **variant['replay_buffer_kwargs']
+    )
+
+    algorithm = TorchBatchRLAlgorithm(
+        trainer=trainer,
+        exploration_env=expl_env,
+        evaluation_env=eval_env,
+        exploration_data_collector=expl_path_collector,
+        evaluation_data_collector=eval_path_collector,
+        replay_buffer=replay_buffer,
+        **variant['algo_kwargs']
+    )
+
+    if variant.get("save_video", False):
+        rollout_function = rf.create_rollout_function(
+            rf.multitask_rollout,
+            max_path_length=algorithm.max_path_length,
+            observation_key=observation_key,
+            desired_goal_key=desired_goal_key,
+        )
+        eval_video_func = get_state_experiment_video_save_function(
+            rollout_function,
+            eval_env,
+            expl_policy,
+            variant,
+        )
+        algorithm.post_epoch_funcs.append(eval_video_func)
+
+    algorithm.to(ptu.device)
+    algorithm.train()
+# def her_sac_ensemble_experiment(variant):
+#     import gym
+
+#     import railrl.torch.pytorch_util as ptu
+#     from railrl.data_management.obs_dict_replay_buffer import ObsDictRelabelingBuffer
+#     from railrl.exploration_strategies.base import \
+#         PolicyWrappedWithPlanner
+#     from railrl.exploration_strategies.PlanningStrategy import \
+#         PlanningStrategy
+#     from railrl.launchers.launcher_util import setup_logger
+#     from railrl.samplers.data_collector import GoalConditionedPathCollector
+#     from railrl.torch.sac.policies import MakeDeterministic
+#     from railrl.torch.her.her import HERTrainer
+#     from railrl.torch.sac.policies import TanhGaussianPolicy
+#     from railrl.torch.networks import FlattenMlp, TanhMlpPolicy
+#     from railrl.torch.networks import FlattenMlp
+#     from railrl.torch.sac.sac_ensemble import SACTrainer
+#     #from railrl.torch.sac.sac import SACTrainer
+#     from railrl.torch.torch_rl_algorithm import TorchBatchRLAlgorithm
+#     import railrl.samplers.rollout_functions as rf
+#     from railrl.torch.grill.launcher import get_state_experiment_video_save_function
+
+#     if 'env_id' in variant:
+#         eval_env = gym.make(variant['env_id'])
+#         expl_env = gym.make(variant['env_id'])
+#     else:
+#         eval_env_kwargs = variant.get('eval_env_kwargs', variant['env_kwargs'])
+#         eval_env = variant['env_class'](**eval_env_kwargs)
+#         expl_env = variant['env_class'](**variant['env_kwargs'])
+
+#     observation_key = 'state_observation'
+#     desired_goal_key = 'state_desired_goal'
+#     achieved_goal_key = desired_goal_key.replace("desired", "achieved")
+
+#     obs_dim = (
+#             expl_env.observation_space.spaces['observation'].low.size
+#             + expl_env.observation_space.spaces['desired_goal'].low.size
+#     )
+#     action_dim = expl_env.action_space.low.size
+#     hidden_sizes = variant.get('hidden_sizes', [50, 50])
+
+#     eval_ensemble = [FlattenMlp(
+#         input_size=obs_dim + action_dim,
+#         output_size=1,
+#         hidden_sizes=hidden_sizes,
+#     ) for i in range(5)]
+
+#     expl_ensemble = [FlattenMlp(
+#         input_size=obs_dim + action_dim,
+#         output_size=1,
+#         hidden_sizes=hidden_sizes,
+#     ) for i in range(5)]
+
+#     averse_ensemble = [FlattenMlp(
+#         input_size=obs_dim + action_dim,
+#         output_size=1,
+#         hidden_sizes=hidden_sizes,
+#     ) for i in range(5)]
+
+#     eval_policy = TanhGaussianPolicy(
+#         obs_dim=obs_dim,
+#         action_dim=action_dim,
+#         hidden_sizes=hidden_sizes,
+#     )
+#     expl_policy = TanhGaussianPolicy(
+#         obs_dim=obs_dim,
+#         action_dim=action_dim,
+#         hidden_sizes=hidden_sizes,
+#     )
+#     averse_policy = TanhGaussianPolicy(
+#         obs_dim=obs_dim,
+#         action_dim=action_dim,
+#         hidden_sizes=hidden_sizes,
+#     )
+#     trainer = SACTrainer(
+#         env=eval_env,
+#         eval_policy=eval_policy,
+#         expl_policy=expl_policy,
+#         averse_policy=averse_policy,
+#         eval_ensemble=eval_ensemble,
+#         expl_ensemble=expl_ensemble,
+#         averse_ensemble=averse_ensemble,
+#         **variant['twin_sac_trainer_kwargs']
+#     )
+
+#     trainer = HERTrainer(trainer)
+#     eval_path_collector = GoalConditionedPathCollector(
+#         eval_env,
+#         MakeDeterministic(averse_policy),
+#         observation_key=observation_key,
+#         desired_goal_key=desired_goal_key,
+#     )
+#     expl_path_collector = GoalConditionedPathCollector(
+#         eval_env,
+#         expl_policy, #THIS IS JUST ACTOR
+#         observation_key=observation_key,
+#         desired_goal_key=desired_goal_key,
+#     )
+
+#     replay_buffer = ObsDictRelabelingBuffer(
+#         env=eval_env,
+#         observation_key=observation_key,
+#         desired_goal_key=desired_goal_key,
+#         achieved_goal_key=achieved_goal_key,
+#         **variant['replay_buffer_kwargs']
+#     )
+
+#     algorithm = TorchBatchRLAlgorithm(
+#         trainer=trainer,
+#         exploration_env=expl_env,
+#         evaluation_env=eval_env,
+#         exploration_data_collector=expl_path_collector,
+#         evaluation_data_collector=eval_path_collector,
+#         replay_buffer=replay_buffer,
+#         **variant['algo_kwargs']
+#     )
+
+#     if variant.get("save_video", False):
+#         rollout_function = rf.create_rollout_function(
+#             rf.multitask_rollout,
+#             max_path_length=algorithm.max_path_length,
+#             observation_key=observation_key,
+#             desired_goal_key=desired_goal_key,
+#         )
+#         eval_video_func = get_state_experiment_video_save_function(
+#             rollout_function,
+#             eval_env,
+#             averse_policy,
+#             variant,
+#         )
+#         algorithm.post_epoch_funcs.append(eval_video_func)
+#         # expl_video_func = get_state_experiment_video_save_function(
+#         #     rollout_function,
+#         #     eval_env,
+#         #     actor_policy,
+#         #     variant,
+#         # )
+
+#         # algorithm.post_epoch_funcs.append(expl_video_func)
+#     algorithm.to(ptu.device)
+#     algorithm.train()
+
+def her_ensemble_planning_experiment(variant):
+    import gym
+
+    import railrl.torch.pytorch_util as ptu
+    from railrl.data_management.obs_dict_replay_buffer import ObsDictRelabelingBuffer
+    from railrl.exploration_strategies.base import \
+        PolicyWrappedWithPlanner
+    from railrl.launchers.launcher_util import setup_logger
+    from railrl.samplers.data_collector import GoalConditionedPathCollector
+    from railrl.torch.sac.policies import MakeDeterministic
+    from railrl.torch.her.her import HERTrainer
+    from railrl.torch.sac.policies import TanhGaussianPolicy
+    from railrl.torch.networks import FlattenMlp, TanhMlpPolicy
+    from railrl.torch.networks import FlattenMlp
+    from railrl.torch.sac.ensemble_planning import SACTrainer
+    #from railrl.torch.sac.sac import SACTrainer
+    from railrl.torch.torch_rl_algorithm import TorchBatchRLAlgorithm
+    import railrl.samplers.rollout_functions as rf
+    from railrl.torch.grill.launcher import get_state_experiment_video_save_function
+
+    if 'env_id' in variant:
+        eval_env = gym.make(variant['env_id'])
+        expl_env = gym.make(variant['env_id'])
+    else:
+        eval_env_kwargs = variant.get('eval_env_kwargs', variant['env_kwargs'])
+        eval_env = variant['env_class'](**eval_env_kwargs)
+        expl_env = variant['env_class'](**variant['env_kwargs'])
+
+    observation_key = 'state_observation'
+    desired_goal_key = 'state_desired_goal'
+    achieved_goal_key = desired_goal_key.replace("desired", "achieved")
+
+    obs_dim = (
+            expl_env.observation_space.spaces['observation'].low.size
+            + expl_env.observation_space.spaces['desired_goal'].low.size
+    )
+    action_dim = expl_env.action_space.low.size
+    hidden_sizes = variant.get('hidden_sizes', [50, 50])
+
+    eval_ensemble = [FlattenMlp(
+        input_size=obs_dim + action_dim,
+        output_size=1,
+        hidden_sizes=hidden_sizes,
+    ) for i in range(5)]
+
+    expl_ensemble = [FlattenMlp(
+        input_size=obs_dim + action_dim,
+        output_size=1,
+        hidden_sizes=hidden_sizes,
+    ) for i in range(5)]
+
+    eval_policy = TanhGaussianPolicy(
+        obs_dim=obs_dim,
+        action_dim=action_dim,
+        hidden_sizes=hidden_sizes,
+    )
+    expl_policy = TanhGaussianPolicy(
+        obs_dim=obs_dim,
+        action_dim=action_dim,
+        hidden_sizes=hidden_sizes,
+    )
+
+    planner_policy = TanhMlpPolicy(
+        input_size=obs_dim,
+        output_size=expl_env.observation_space.spaces['observation'].low.size,
+        hidden_sizes=hidden_sizes,
+    )
+
+    planning_policy = PolicyWrappedWithPlanner(
+        MakeDeterministic(eval_policy),
+        planner_policy,
+        expl_env.observation_space.spaces['observation'].low.size,
+        eval_env)
+
+    trainer = SACTrainer(
+        env=eval_env,
+        eval_policy=eval_policy,
+        expl_policy=expl_policy,
+        planner_policy=planner_policy,
+        eval_ensemble=eval_ensemble,
+        expl_ensemble=expl_ensemble,
+        **variant['twin_sac_trainer_kwargs']
+    )
+
+    trainer = HERTrainer(trainer)
+    eval_path_collector = GoalConditionedPathCollector(
+        eval_env,
+        planning_policy,
+        observation_key=observation_key,
+        desired_goal_key=desired_goal_key,
+    )
+    expl_path_collector = GoalConditionedPathCollector(
+        eval_env,
+        expl_policy, #THIS IS JUST ACTOR
+        observation_key=observation_key,
+        desired_goal_key=desired_goal_key,
+    )
+
+    replay_buffer = ObsDictRelabelingBuffer(
+        env=eval_env,
+        observation_key=observation_key,
+        desired_goal_key=desired_goal_key,
+        achieved_goal_key=achieved_goal_key,
+        **variant['replay_buffer_kwargs']
+    )
+
+    algorithm = TorchBatchRLAlgorithm(
+        trainer=trainer,
+        exploration_env=expl_env,
+        evaluation_env=eval_env,
+        exploration_data_collector=expl_path_collector,
+        evaluation_data_collector=eval_path_collector,
+        replay_buffer=replay_buffer,
+        **variant['algo_kwargs']
+    )
+
+    if variant.get("save_video", False):
+        rollout_function = rf.create_rollout_function(
+            rf.multitask_rollout,
+            max_path_length=algorithm.max_path_length,
+            observation_key=observation_key,
+            desired_goal_key=desired_goal_key,
+        )
+        eval_video_func = get_state_experiment_video_save_function(
+            rollout_function,
+            eval_env,
+            planning_policy,
+            variant,
+        )
+        algorithm.post_epoch_funcs.append(eval_video_func)
+
+    algorithm.to(ptu.device)
+    algorithm.train()
+
+def her_sac_ensemble_experiment(variant):
+    import gym
+
+    import railrl.torch.pytorch_util as ptu
+    from railrl.data_management.obs_dict_replay_buffer import ObsDictRelabelingBuffer
+    from railrl.exploration_strategies.base import \
+        PolicyWrappedWithPlanner
+    from railrl.exploration_strategies.PlanningStrategy import \
+        PlanningStrategy
+    from railrl.launchers.launcher_util import setup_logger
+    from railrl.samplers.data_collector import GoalConditionedPathCollector
+    from railrl.torch.sac.policies import MakeDeterministic
+    from railrl.torch.her.her import HERTrainer
+    from railrl.torch.sac.policies import TanhGaussianPolicy
+    from railrl.torch.networks import FlattenMlp, TanhMlpPolicy
+    from railrl.torch.networks import FlattenMlp
+    from railrl.torch.sac.sac_ensemble import SACTrainer
+    #from railrl.torch.sac.sac import SACTrainer
+    from railrl.torch.torch_rl_algorithm import TorchBatchRLAlgorithm
+    import railrl.samplers.rollout_functions as rf
+    from railrl.torch.grill.launcher import get_state_experiment_video_save_function
+
+    if 'env_id' in variant:
+        eval_env = gym.make(variant['env_id'])
+        expl_env = gym.make(variant['env_id'])
+    else:
+        eval_env_kwargs = variant.get('eval_env_kwargs', variant['env_kwargs'])
+        eval_env = variant['env_class'](**eval_env_kwargs)
+        expl_env = variant['env_class'](**variant['env_kwargs'])
+
+    observation_key = 'state_observation'
+    desired_goal_key = 'state_desired_goal'
+    achieved_goal_key = desired_goal_key.replace("desired", "achieved")
+
+    obs_dim = (
+            expl_env.observation_space.spaces['observation'].low.size
+            + expl_env.observation_space.spaces['desired_goal'].low.size
+    )
+    action_dim = expl_env.action_space.low.size
+    hidden_sizes = variant.get('hidden_sizes', [50, 50])
+
+    eval_ensemble = [FlattenMlp(
+        input_size=obs_dim + action_dim,
+        output_size=1,
+        hidden_sizes=hidden_sizes,
+    ) for i in range(5)]
+
+    expl_ensemble = [FlattenMlp(
+        input_size=obs_dim + action_dim,
+        output_size=1,
+        hidden_sizes=hidden_sizes,
+    ) for i in range(5)]
+
+    eval_policy = TanhGaussianPolicy(
+        obs_dim=obs_dim,
+        action_dim=action_dim,
+        hidden_sizes=hidden_sizes,
+    )
+    expl_policy = TanhGaussianPolicy(
+        obs_dim=obs_dim,
+        action_dim=action_dim,
+        hidden_sizes=hidden_sizes,
+    )
+    trainer = SACTrainer(
+        env=eval_env,
+        eval_policy=eval_policy,
+        expl_policy=expl_policy,
+        eval_ensemble=eval_ensemble,
+        expl_ensemble=expl_ensemble,
+        **variant['twin_sac_trainer_kwargs']
+    )
+
+    trainer = HERTrainer(trainer)
+    eval_path_collector = GoalConditionedPathCollector(
+        eval_env,
+        MakeDeterministic(eval_policy),
+        observation_key=observation_key,
+        desired_goal_key=desired_goal_key,
+    )
+    expl_path_collector = GoalConditionedPathCollector(
+        eval_env,
+        expl_policy, #THIS IS JUST ACTOR
+        observation_key=observation_key,
+        desired_goal_key=desired_goal_key,
+    )
+
+    replay_buffer = ObsDictRelabelingBuffer(
+        env=eval_env,
+        observation_key=observation_key,
+        desired_goal_key=desired_goal_key,
+        achieved_goal_key=achieved_goal_key,
+        **variant['replay_buffer_kwargs']
+    )
+
+    algorithm = TorchBatchRLAlgorithm(
+        trainer=trainer,
+        exploration_env=expl_env,
+        evaluation_env=eval_env,
+        exploration_data_collector=expl_path_collector,
+        evaluation_data_collector=eval_path_collector,
+        replay_buffer=replay_buffer,
+        **variant['algo_kwargs']
+    )
+
+    if variant.get("save_video", False):
+        rollout_function = rf.create_rollout_function(
+            rf.multitask_rollout,
+            max_path_length=algorithm.max_path_length,
+            observation_key=observation_key,
+            desired_goal_key=desired_goal_key,
+        )
+        eval_video_func = get_state_experiment_video_save_function(
+            rollout_function,
+            eval_env,
+            expl_policy,
+            variant,
+        )
+        algorithm.post_epoch_funcs.append(eval_video_func)
+
+    algorithm.to(ptu.device)
+    algorithm.train()
+
+def her_sac_curious_experiment(variant):
+    import gym
+
+    import railrl.torch.pytorch_util as ptu
+    from railrl.data_management.obs_dict_replay_buffer import ObsDictRelabelingBuffer
+    from railrl.exploration_strategies.base import \
+        PolicyWrappedWithPlanner
+    from railrl.exploration_strategies.PlanningStrategy import \
+        PlanningStrategy
+    from railrl.launchers.launcher_util import setup_logger
+    from railrl.samplers.data_collector import GoalConditionedPathCollector
+    from railrl.torch.sac.policies import MakeDeterministic
+    from railrl.torch.her.her import HERTrainer
+    from railrl.torch.sac.policies import TanhGaussianPolicy
+    from railrl.torch.networks import FlattenMlp, TanhMlpPolicy
+    from railrl.torch.networks import FlattenMlp
+    from railrl.torch.sac.sac_curious import SACTrainer
+    #from railrl.torch.sac.sac import SACTrainer
+    from railrl.torch.torch_rl_algorithm import TorchBatchRLAlgorithm
+    import railrl.samplers.rollout_functions as rf
+    from railrl.torch.grill.launcher import get_state_experiment_video_save_function
+
+    if 'env_id' in variant:
+        eval_env = gym.make(variant['env_id'])
+        expl_env = gym.make(variant['env_id'])
+    else:
+        eval_env_kwargs = variant.get('eval_env_kwargs', variant['env_kwargs'])
+        eval_env = variant['env_class'](**eval_env_kwargs)
+        expl_env = variant['env_class'](**variant['env_kwargs'])
+
+    observation_key = 'state_observation'
+    desired_goal_key = 'state_desired_goal'
+    achieved_goal_key = desired_goal_key.replace("desired", "achieved")
+
+    obs_dim = (
+            expl_env.observation_space.spaces['observation'].low.size
+            + expl_env.observation_space.spaces['desired_goal'].low.size
+    )
+    action_dim = expl_env.action_space.low.size
+    hidden_sizes = variant.get('hidden_sizes', [50, 50])
+
+    qf1 = FlattenMlp(
+        input_size=obs_dim + action_dim,
+        output_size=1,
+        hidden_sizes=hidden_sizes,
+    )
+    qf2 = FlattenMlp(
+        input_size=obs_dim + action_dim,
+        output_size=1,
+        hidden_sizes=hidden_sizes,
+    )
+    target_qf1 = FlattenMlp(
+        input_size=obs_dim + action_dim,
+        output_size=1,
+        hidden_sizes=hidden_sizes,
+    )
+    target_qf2 = FlattenMlp(
+        input_size=obs_dim + action_dim,
+        output_size=1,
+        hidden_sizes=hidden_sizes,
+    )
+
+    expl_qf1 = FlattenMlp(
+        input_size=obs_dim + action_dim,
+        output_size=1,
+        hidden_sizes=hidden_sizes,
+    )
+    expl_qf2 = FlattenMlp(
+        input_size=obs_dim + action_dim,
+        output_size=1,
+        hidden_sizes=hidden_sizes,
+    )
+    expl_target_qf1 = FlattenMlp(
+        input_size=obs_dim + action_dim,
+        output_size=1,
+        hidden_sizes=hidden_sizes,
+    )
+    expl_target_qf2 = FlattenMlp(
+        input_size=obs_dim + action_dim,
+        output_size=1,
+        hidden_sizes=hidden_sizes,
+    )
+
+    policy = TanhGaussianPolicy(
+        obs_dim=obs_dim,
+        action_dim=action_dim,
+        hidden_sizes=hidden_sizes,
+    )
+    expl_policy = TanhGaussianPolicy(
+        obs_dim=obs_dim,
+        action_dim=action_dim,
+        hidden_sizes=hidden_sizes,
+    )
+
+
+    trainer = SACTrainer(
+        env=eval_env,
+        policy=policy,
+        expl_policy=expl_policy,
+        qf1=qf1,
+        expl_qf1=expl_qf1,
+        qf2=qf2,
+        expl_qf2=expl_qf2,
+        target_qf1=target_qf1,
+        expl_target_qf1=expl_target_qf1,
+        target_qf2=target_qf2,
+        expl_target_qf2=expl_target_qf2)
+    # es = GaussianAndEpislonStrategy(
+    #     action_space=expl_env.observation_space.spaces['observation'].low.size,
+    #     max_sigma=.2,
+    #     min_sigma=.2,  # constant sigma
+    #     epsilon=.3,
+    # )
+
+    # expl_planner = PolicyWrappedWithExplorationStrategy(
+    #     exploration_strategy=es,
+    #     policy=planner_policy,
+    # )
+
+    # expl_policy = PolicyWrappedWithPlanner(
+    #     actor_policy,
+    #     expl_planner,
+    #     expl_env.observation_space.spaces['observation'].low.size)
+
+
+
+
+    # trainer = SAPTrainer(
+    #     env=eval_env,
+    #     actor_policy=actor_policy,
+    #     planner_policy=planner_policy,
+    #     qf1=qf1,
+    #     qf2=qf2,
+    #     target_qf1=target_qf1,
+    #     target_qf2=target_qf2,
+    #     act_dim=expl_env.action_space.low.size,
+    #     obs_dim=expl_env.observation_space.spaces['observation'].low.size,
+    #     **variant['twin_sac_trainer_kwargs']
+    # )
+    trainer = HERTrainer(trainer)
+    eval_path_collector = GoalConditionedPathCollector(
+        eval_env,
+        MakeDeterministic(policy),
+        observation_key=observation_key,
+        desired_goal_key=desired_goal_key,
+    )
+    expl_path_collector = GoalConditionedPathCollector(
+        eval_env,
+        expl_policy, #THIS IS JUST ACTOR
+        observation_key=observation_key,
+        desired_goal_key=desired_goal_key,
+    )
+
+    replay_buffer = ObsDictRelabelingBuffer(
+        env=eval_env,
+        observation_key=observation_key,
+        desired_goal_key=desired_goal_key,
+        achieved_goal_key=achieved_goal_key,
+        **variant['replay_buffer_kwargs']
+    )
+
+    algorithm = TorchBatchRLAlgorithm(
+        trainer=trainer,
+        exploration_env=expl_env,
+        evaluation_env=eval_env,
+        exploration_data_collector=expl_path_collector,
+        evaluation_data_collector=eval_path_collector,
+        replay_buffer=replay_buffer,
+        **variant['algo_kwargs']
+    )
+
+    if variant.get("save_video", False):
+        rollout_function = rf.create_rollout_function(
+            rf.multitask_rollout,
+            max_path_length=algorithm.max_path_length,
+            observation_key=observation_key,
+            desired_goal_key=desired_goal_key,
+        )
+        eval_video_func = get_state_experiment_video_save_function(
+            rollout_function,
+            eval_env,
+            expl_policy,
+            variant,
+        )
+        algorithm.post_epoch_funcs.append(eval_video_func)
+        # expl_video_func = get_state_experiment_video_save_function(
+        #     rollout_function,
+        #     eval_env,
+        #     actor_policy,
+        #     variant,
+        # )
+
+        # algorithm.post_epoch_funcs.append(expl_video_func)
+    algorithm.to(ptu.device)
+    algorithm.train()
+
+
+def her_sap2_experiment(variant):
+    import gym
+
+    import railrl.torch.pytorch_util as ptu
+    from railrl.data_management.obs_dict_replay_buffer import ObsDictRelabelingBuffer
+    from railrl.exploration_strategies.base import \
+        PolicyWrappedWithExplorationStrategy
+    from railrl.exploration_strategies.PlanningStrategy import \
+        PlanningStrategy
+    from railrl.launchers.launcher_util import setup_logger
+    from railrl.samplers.data_collector import GoalConditionedPathCollector
+    from railrl.torch.sac.policies import MakeDeterministic
+    from railrl.torch.her.her import HERTrainer
+    from railrl.torch.sac.policies import TanhGaussianPolicy, GaussianPolicy
+    from railrl.torch.networks import FlattenMlp
+    from railrl.torch.sac.sap_joined import SAPTrainer
+    from railrl.torch.torch_rl_algorithm import TorchBatchRLAlgorithm
+    import railrl.samplers.rollout_functions as rf
+    from railrl.torch.grill.launcher import get_state_experiment_video_save_function
+
+    if 'env_id' in variant:
+        eval_env = gym.make(variant['env_id'])
+        expl_env = gym.make(variant['env_id'])
+    else:
+        eval_env_kwargs = variant.get('eval_env_kwargs', variant['env_kwargs'])
+        eval_env = variant['env_class'](**eval_env_kwargs)
+        expl_env = variant['env_class'](**variant['env_kwargs'])
+
+    observation_key = 'state_observation'
+    desired_goal_key = 'state_desired_goal'
+    achieved_goal_key = desired_goal_key.replace("desired", "achieved")
+
+    obs_dim = (
+            expl_env.observation_space.spaces['observation'].low.size
+            + expl_env.observation_space.spaces['desired_goal'].low.size
+    )
+    action_dim = expl_env.action_space.low.size + expl_env.observation_space.spaces['observation'].low.size
+    hidden_sizes = variant.get('hidden_sizes', [400, 300])
+
+    es = PlanningStrategy(
+        action_dim=expl_env.action_space.low.size
+    )
+    qf1 = FlattenMlp(
+        input_size=obs_dim + action_dim,
+        output_size=1,
+        hidden_sizes=hidden_sizes,
+    )
+    qf2 = FlattenMlp(
+        input_size=obs_dim + action_dim,
+        output_size=1,
+        hidden_sizes=hidden_sizes,
+    )
+    target_qf1 = FlattenMlp(
+        input_size=obs_dim + action_dim,
+        output_size=1,
+        hidden_sizes=hidden_sizes,
+    )
+    target_qf2 = FlattenMlp(
+        input_size=obs_dim + action_dim,
+        output_size=1,
+        hidden_sizes=hidden_sizes,
+    )
+    policy = GaussianPolicy(
+        obs_dim=obs_dim,
+        action_dim=action_dim,
+        hidden_sizes=hidden_sizes,
+    )
+
+    trainer = SAPTrainer(
+        env=eval_env,
+        policy=policy,
+        qf1=qf1,
+        qf2=qf2,
+        target_qf1=target_qf1,
+        target_qf2=target_qf2,
+        act_dim=expl_env.action_space.low.size,
+        obs_dim=expl_env.observation_space.spaces['observation'].low.size,
+        **variant['twin_sac_trainer_kwargs']
+    )
+    trainer = HERTrainer(trainer)
+    eval_path_collector = GoalConditionedPathCollector(
+        eval_env,
+        PolicyWrappedWithExplorationStrategy(
+        exploration_strategy=es,
+        policy=MakeDeterministic(policy)),
+        observation_key=observation_key,
+        desired_goal_key=desired_goal_key,
+    )
+    expl_path_collector = GoalConditionedPathCollector(
+        eval_env,
+        PolicyWrappedWithExplorationStrategy(
+        exploration_strategy=es,
+        policy=policy),
+        observation_key=observation_key,
+        desired_goal_key=desired_goal_key,
+    )
+
+    replay_buffer = ObsDictRelabelingBuffer(
+        env=eval_env,
+        observation_key=observation_key,
+        desired_goal_key=desired_goal_key,
+        achieved_goal_key=achieved_goal_key,
+        **variant['replay_buffer_kwargs']
+    )
+
+    algorithm = TorchBatchRLAlgorithm(
+        trainer=trainer,
+        exploration_env=expl_env,
+        evaluation_env=eval_env,
+        exploration_data_collector=expl_path_collector,
+        evaluation_data_collector=eval_path_collector,
+        replay_buffer=replay_buffer,
+        **variant['algo_kwargs']
+    )
+
+    if variant.get("save_video", False):
+        rollout_function = rf.create_rollout_function(
+            rf.multitask_rollout,
+            max_path_length=algorithm.max_path_length,
+            observation_key=observation_key,
+            desired_goal_key=desired_goal_key,
+        )
+        video_func = get_state_experiment_video_save_function(
+            rollout_function,
+            eval_env,
+            policy,
+            variant,
+        )
+        algorithm.post_epoch_funcs.append(video_func)
+
+    algorithm.to(ptu.device)
+    algorithm.train()
\ No newline at end of file
diff --git a/railrl/launchers/experiments/ashvin/rfeatures/encoder_wrapped_env.py b/railrl/launchers/experiments/ashvin/rfeatures/encoder_wrapped_env.py
deleted file mode 100644
index 6e6bfda..0000000
--- a/railrl/launchers/experiments/ashvin/rfeatures/encoder_wrapped_env.py
+++ /dev/null
@@ -1,284 +0,0 @@
-import copy
-import random
-import warnings
-
-import torch
-
-# import cv2
-import numpy as np
-from gym import Env
-from gym.spaces import Box, Dict
-import railrl.torch.pytorch_util as ptu
-from multiworld.core.multitask_env import MultitaskEnv
-from multiworld.envs.env_util import get_stat_in_paths, create_stats_ordered_dict
-from railrl.envs.wrappers import ProxyEnv
-from railrl.misc.asset_loader import load_local_or_remote_file
-import time
-
-from os import path as osp
-from torchvision.utils import save_image
-from rlkit.core import logger
-import railrl.data_management.external.epic_kitchens_data_stub as epic
-
-eps = 1e-5
-
-class EncoderWrappedEnv(ProxyEnv):
-    """This class wraps an image-based environment with a VAE.
-    Assumes you get flattened (channels,84,84) observations from wrapped_env.
-    This class adheres to the "Silent Multitask Env" semantics: on reset,
-    it resamples a goal.
-    """
-    def __init__(
-        self,
-        wrapped_env,
-        vae,
-        reward_params=None,
-        config_params=None,
-        imsize=84,
-        obs_size=None,
-        vae_input_observation_key="image_observation",
-    ):
-        if config_params is None:
-            config_params = dict
-        if reward_params is None:
-            reward_params = dict()
-        super().__init__(wrapped_env)
-        if type(vae) is str:
-            self.vae = load_local_or_remote_file(vae)
-        else:
-            self.vae = vae
-        self.representation_size = self.vae.representation_size
-        self.input_channels = self.vae.input_channels
-        self.imsize = imsize
-        self.config_params = config_params
-        self.reward_params = reward_params
-        # self.reward_type = self.reward_params.get("type", 'latent_distance')
-        self.zT = self.reward_params["goal_latent"]
-        self.z0 = self.reward_params["initial_latent"]
-        self.dT = self.zT - self.z0
-        if self.config_params["use_initial"]:
-            self.dT = self.zT - self.z0
-        else:
-            self.dT = self.zT
-
-        self.vae_input_observation_key = vae_input_observation_key
-
-        latent_space = Box(
-            -10 * np.ones(obs_size or self.representation_size),
-            10 * np.ones(obs_size or self.representation_size),
-            dtype=np.float32,
-        )
-        goal_space = Box(
-            np.zeros((0, )),
-            np.zeros((0, )),
-            dtype=np.float32,
-        )
-        spaces = self.wrapped_env.observation_space.spaces
-        spaces['observation'] = latent_space
-        spaces['desired_goal'] = goal_space
-        spaces['achieved_goal'] = goal_space
-        spaces['latent_observation'] = latent_space
-        spaces['latent_desired_goal'] = goal_space
-        spaces['latent_achieved_goal'] = goal_space
-        self.observation_space = Dict(spaces)
-
-    def reset(self):
-        self.vae.eval()
-        obs = self.wrapped_env.reset()
-        # start_rollout(obs)
-        self.x0 = obs["image_observation"]
-        # self.save_image_util(self.x0, "initial")
-        goal = self.sample_goal()
-        self.set_goal(goal)
-        obs = self._update_obs(obs)
-        self.z0 = obs["latent_observation"]
-        if self.config_params["use_initial"]:
-            self.dT = self.zT - self.z0
-        else:
-            self.dT = self.zT
-        return obs
-
-    def initialize(self, zs):
-        if self.config_params["initial_type"] == "use_initial_from_trajectory":
-            self.z0 = zs[0]
-        if self.config_params["goal_type"] == "use_goal_from_trajectory":
-            self.zT = zs[-1]
-
-        if self.config_params["use_initial"]:
-            self.dT = self.zT - self.z0
-        else:
-            self.dT = self.zT
-
-    def step(self, action):
-        self.vae.eval()
-        obs, reward, done, info = self.wrapped_env.step(action)
-        new_obs = self._update_obs(obs)
-        self._update_info(info, new_obs)
-        reward = self.compute_reward(
-            action,
-            new_obs,
-            # {'latent_achieved_goal': new_obs['latent_achieved_goal'],
-            #  'latent_desired_goal': new_obs['latent_desired_goal']}
-        )
-        return new_obs, reward, done, info
-
-    def _update_obs(self, obs):
-        self.vae.eval()
-        self.zt = self._encode_one(obs[self.vae_input_observation_key])
-        if self.config_params["use_initial"]:
-            latent_obs = self.zt - self.z0
-        else:
-            latent_obs = self.zt
-        obs['latent_observation'] = latent_obs
-        obs['latent_achieved_goal'] = np.array([])
-        obs['latent_desired_goal'] = np.array([])
-        obs['observation'] = latent_obs
-        obs['achieved_goal'] = np.array([])
-        obs['desired_goal'] = np.array([])
-        # obs = {**obs, **self.desired_goal}
-        return obs
-
-    def _update_obs_latent(self, obs, z):
-        self.vae.eval()
-        self.zt = z
-        if self.config_params["use_initial"]:
-            latent_obs = self.zt - self.z0
-        else:
-            latent_obs = self.zt
-        obs['latent_observation'] = latent_obs
-        obs['latent_achieved_goal'] = np.array([])
-        obs['latent_desired_goal'] = np.array([])
-        obs['observation'] = latent_obs
-        obs['achieved_goal'] = np.array([])
-        obs['desired_goal'] = np.array([])
-        # obs = {**obs, **self.desired_goal}
-        return obs
-
-    def _update_info(self, info, obs):
-        pass
-        # self.vae.eval()
-        # latent_distribution_params = self.vae.encode(
-        #     ptu.from_numpy(obs[self.vae_input_observation_key].reshape(1,-1))
-        # )
-        # latent_obs, logvar = ptu.get_numpy(latent_distribution_params[0])[0], ptu.get_numpy(latent_distribution_params[1])[0]
-        # # assert (latent_obs == obs['latent_observation']).all()
-        # latent_goal = self.desired_goal['latent_desired_goal']
-        # dist = latent_goal - latent_obs
-        # var = np.exp(logvar.flatten())
-        # var = np.maximum(var, self.reward_min_variance)
-        # err = dist * dist / 2 / var
-        # mdist = np.sum(err)  # mahalanobis distance
-        # info["vae_mdist"] = mdist
-        # info["vae_success"] = 1 if mdist < self.epsilon else 0
-        # info["vae_dist"] = np.linalg.norm(dist, ord=self.norm_order)
-        # info["vae_dist_l1"] = np.linalg.norm(dist, ord=1)
-        # info["vae_dist_l2"] = np.linalg.norm(dist, ord=2)
-
-    def compute_reward(self, action, obs, info=None):
-        self.vae.eval()
-
-        dt = obs["latent_observation"]
-        dT = self.dT
-
-        # import ipdb; ipdb.set_trace()
-
-        reward_type = self.reward_params.get("type", "regression_distance")
-        if reward_type == "regression_distance":
-            regression_pred_yt = (dt * dT).sum() / ((dT ** 2).sum() + eps)
-            return -np.abs(1-regression_pred_yt)
-        if reward_type == "latent_distance":
-            return -np.linalg.norm(dt - dT)
-
-        # next_obs = {
-        #     k: v[None] for k, v in obs.items()
-        # }
-        # reward = self.compute_rewards(actions, next_obs)
-        # return reward[0]
-
-    def compute_rewards(self, actions, obs, info=None):
-        self.vae.eval()
-
-        dt = obs["latent_observation"]
-        dT = self.dT
-
-        regression_pred_yt = (dt * dT).sum(axis=1) / ((dT ** 2).sum() + eps)
-
-        return -np.abs(1-regression_pred_yt)
-
-        # TODO: implement log_prob/mdist
-        # if self.reward_type == 'latent_distance':
-        #     achieved_goals = obs['latent_achieved_goal']
-        #     desired_goals = obs['latent_desired_goal']
-        #     dist = np.linalg.norm(desired_goals - achieved_goals, ord=self.norm_order, axis=1)
-        #     return -dist
-        # elif self.reward_type == 'wrapped_env':
-        #     return self.wrapped_env.compute_rewards(actions, obs)
-        # else:
-        #     raise NotImplementedError
-
-    def get_diagnostics(self, paths, **kwargs):
-        statistics = dict()
-        # statistics = self.wrapped_env.get_diagnostics(paths, **kwargs)
-        # for stat_name_in_paths in ["vae_mdist", "vae_success", "vae_dist"]:
-        #     stats = get_stat_in_paths(paths, 'env_infos', stat_name_in_paths)
-        #     statistics.update(create_stats_ordered_dict(
-        #         stat_name_in_paths,
-        #         stats,
-        #         always_show_all_stats=True,
-        #     ))
-        #     final_stats = [s[-1] for s in stats]
-        #     statistics.update(create_stats_ordered_dict(
-        #         "Final " + stat_name_in_paths,
-        #         final_stats,
-        #         always_show_all_stats=True,
-        #     ))
-        return statistics
-
-    def _encode_one(self, img, name=None):
-        im = img.reshape(1, 3, 500, 300).transpose([0, 1, 3, 2]) / 255.0
-        im = im[:, :, 60:300, 30:470]
-        return self._encode(im, name)[0]
-
-    def _encode_batch(self, imgs):
-        im = imgs.reshape(-1, 3, 500, 300).transpose([0, 1, 3, 2]) / 255.0
-        im = im[:, :, 60:300, 30:470]
-        return self._encode(im)
-
-    def save_image_util(self, img, name):
-        pt_img = ptu.from_numpy(img).view(-1, 3, epic.CROP_HEIGHT, epic.CROP_WIDTH)
-        save_image(pt_img.data.cpu(), '%s.png'%name, nrow=1)
-        # save_image(pt_img[:1, :, :, :].data.cpu(), 'forward.png', nrow=1)
-
-    def _encode(self, imgs, name=None):
-        self.vae.eval()
-        pt_img = ptu.from_numpy(imgs).view(-1, 3, epic.CROP_HEIGHT, epic.CROP_WIDTH)
-
-        # save_dir = osp.join(logger.get_snapshot_dir(), )
-        # save_image(pt_img.data.cpu(), 'forward.png', nrow=1)
-        # save_image(pt_img[:1, :, :, :].data.cpu(), 'forward.png', nrow=1)
-
-        latent_distribution_params = self.vae.encode(pt_img)
-        return ptu.get_numpy(latent_distribution_params)
-
-    def _image_and_proprio_from_decoded(self, decoded):
-        if decoded is None:
-            return None, None
-        if self.vae_input_key_prefix == 'image_proprio':
-            images = decoded[:, :self.image_length]
-            proprio = decoded[:, self.image_length:]
-            return images, proprio
-        elif self.vae_input_key_prefix == 'image':
-            return decoded, None
-        else:
-            raise AssertionError("Bad prefix for the vae input key.")
-
-    def __getstate__(self):
-        state = super().__getstate__()
-        state = copy.copy(state)
-        state['_custom_goal_sampler'] = None
-        warnings.warn('VAEWrapperEnv.custom_goal_sampler is not saved.')
-        return state
-
-    def __setstate__(self, state):
-        warnings.warn('VAEWrapperEnv.custom_goal_sampler was not loaded.')
-        super().__setstate__(state)
diff --git a/railrl/launchers/experiments/ashvin/rfeatures/rfeatures_model.py b/railrl/launchers/experiments/ashvin/rfeatures/rfeatures_model.py
index fdb8751..7539560 100644
--- a/railrl/launchers/experiments/ashvin/rfeatures/rfeatures_model.py
+++ b/railrl/launchers/experiments/ashvin/rfeatures/rfeatures_model.py
@@ -11,7 +11,7 @@ from railrl.torch.vae.vae_base import GaussianLatentVAE
 import torchvision
 import torchvision.transforms as transforms
 
-import railrl.data_management.external.epic_kitchens_data_stub as epic
+import railrl.data_management.external.epic_kitchens_data as epic
 from torch.utils.model_zoo import load_url as load_state_dict_from_url
 
 MAX_BATCH_SIZE = 100
@@ -134,20 +134,19 @@ class TimestepPredictionModel(torch.nn.Module):
 
         x = torch.cat([x0, xt, xT], dim=0).view(-1, 3, epic.CROP_HEIGHT, epic.CROP_WIDTH, )
 
-        z = self.encode(x)
-        # # import pdb; pdb.set_trace()
-        # if self.normalize:
-        #     x = x - self.img_mean
-        #     x = x / self.img_std
-        #     # x = self.img_normalizer(x)
+        # import pdb; pdb.set_trace()
+        if self.normalize:
+            x = x - self.img_mean
+            x = x / self.img_std
+            # x = self.img_normalizer(x)
 
-        # zs = []
-        # for i in range(0, 3 * bz, MAX_BATCH_SIZE):
-        #     z = self.encoder(x[i:i+MAX_BATCH_SIZE, :, :, :])
-        #     zs.append(z)
+        zs = []
+        for i in range(0, 3 * bz, MAX_BATCH_SIZE):
+            z = self.encoder(x[i:i+MAX_BATCH_SIZE, :, :, :])
+            zs.append(z)
 
-        # # z = self.encoder(x)
-        # z = torch.cat(zs) # self.encoder(x) # .to("cuda:0")
+        # z = self.encoder(x)
+        z = torch.cat(zs) # self.encoder(x) # .to("cuda:0")
 
         z0, zt, zT = z[:bz, :], z[bz:2*bz, :], z[2*bz:3*bz, :]
 
@@ -170,19 +169,3 @@ class TimestepPredictionModel(torch.nn.Module):
         out = self.predictor(z)
         
         return out
-
-    def encode(self, x):
-        bz = x.shape[0]
-
-        if self.normalize:
-            x = x - self.img_mean
-            x = x / self.img_std
-
-        zs = []
-        for i in range(0, bz, MAX_BATCH_SIZE):
-            z = self.encoder(x[i:i+MAX_BATCH_SIZE, :, :, :])
-            zs.append(z)
-
-        z = torch.cat(zs)
-
-        return z
\ No newline at end of file
diff --git a/railrl/launchers/experiments/ashvin/rfeatures/rfeatures_rl.py b/railrl/launchers/experiments/ashvin/rfeatures/rfeatures_rl.py
deleted file mode 100644
index e984b73..0000000
--- a/railrl/launchers/experiments/ashvin/rfeatures/rfeatures_rl.py
+++ /dev/null
@@ -1,222 +0,0 @@
-import gym
-
-import railrl.torch.pytorch_util as ptu
-from railrl.data_management.obs_dict_replay_buffer import ObsDictRelabelingBuffer
-from railrl.exploration_strategies.base import \
-    PolicyWrappedWithExplorationStrategy
-from railrl.exploration_strategies.gaussian_and_epislon import \
-    GaussianAndEpislonStrategy
-from railrl.launchers.launcher_util import setup_logger
-from railrl.samplers.data_collector import GoalConditionedPathCollector
-from railrl.torch.her.her import HERTrainer
-from railrl.torch.networks import FlattenMlp, TanhMlpPolicy, TorchMaxClamp
-# from railrl.torch.td3.td3 import TD3
-from railrl.demos.td3_bc import TD3BCTrainer
-from railrl.torch.torch_rl_algorithm import TorchBatchRLAlgorithm
-# from multiworld.envs.mujoco.sawyer_xyz.sawyer_push_multiobj_subset import SawyerMultiobjectEnv
-# from multiworld.envs.mujoco.sawyer_xyz.sawyer_reach import SawyerReachXYZEnv
-
-from multiworld.core.image_env import ImageEnv
-from multiworld.envs.real_world.sawyer.sawyer_reaching import SawyerReachXYZEnv
-# from sawyer_control.envs.sawyer_reaching import SawyerReachXYZEnv
-
-from railrl.launchers.launcher_util import run_experiment
-# import railrl.util.hyperparameter as hyp
-from railrl.launchers.experiments.ashvin.rfeatures.encoder_wrapped_env import EncoderWrappedEnv
-from railrl.misc.asset_loader import load_local_or_remote_file
-
-import torch
-
-from railrl.launchers.experiments.ashvin.rfeatures.rfeatures_model import TimestepPredictionModel
-import numpy as np
-
-from railrl.torch.grill.video_gen import VideoSaveFunction
-
-from railrl.launchers.arglauncher import run_variants
-import railrl.misc.hyperparameter as hyp
-
-# from railrl.launchers.experiments.ashvin.rfeatures.rfeatures_trainer import TimePredictionTrainer
-
-from torchvision.utils import save_image
-
-def encoder_wrapped_td3bc_experiment(variant):
-    representation_size = 128
-    output_classes = 20
-
-    model_class = variant.get('model_class', TimestepPredictionModel)
-    model = model_class(
-        representation_size,
-        # decoder_output_activation=decoder_activation,
-        output_classes=output_classes,
-        **variant['model_kwargs'],
-    )
-    # model = torch.nn.DataParallel(model)
-
-    model_path = variant.get("model_path")
-    # model = load_local_or_remote_file(model_path)
-    state_dict = torch.load(model_path)
-    model.load_state_dict(state_dict)
-    model.to(ptu.device)
-    model.eval()
-
-    traj = np.load(variant.get("desired_trajectory"), allow_pickle=True)[0]
-
-    goal_image = traj["observations"][-1]["image_observation"]
-    goal_image = goal_image.reshape(1, 3, 500, 300).transpose([0, 1, 3, 2]) / 255.0
-    # goal_image = goal_image[:, ::-1, :, :].copy() # flip bgr
-    goal_image = goal_image[:, :, :240, 60:500]
-    goal_image_pt = ptu.from_numpy(goal_image)
-    save_image(goal_image_pt.data.cpu(), 'demos/goal.png', nrow=1)
-    goal_latent = model.encode(goal_image_pt).detach().cpu().numpy().flatten()
-
-    initial_image = traj["observations"][0]["image_observation"]
-    initial_image = initial_image.reshape(1, 3, 500, 300).transpose([0, 1, 3, 2]) / 255.0
-    # initial_image = initial_image[:, ::-1, :, :].copy() # flip bgr
-    initial_image = initial_image[:, :, :240, 60:500]
-    initial_image_pt = ptu.from_numpy(initial_image)
-    save_image(initial_image_pt.data.cpu(), 'demos/initial.png', nrow=1)
-    initial_latent = model.encode(initial_image_pt).detach().cpu().numpy().flatten()
-
-    # Move these to td3_bc and bc_v3 (or at least type for reward_params)
-    reward_params = dict(
-        goal_latent=goal_latent,
-        initial_latent=initial_latent,
-        type=variant.get("reward_params_type"),
-    )
-
-    config_params = variant.get("config_params")
-
-    env = variant['env_class'](**variant['env_kwargs'])
-    env = ImageEnv(env,
-        recompute_reward=False,
-        transpose=True,
-        image_length=450000,
-        reward_type="image_distance",
-        # init_camera=sawyer_pusher_camera_upright_v2,
-    )
-    env = EncoderWrappedEnv(env, model, reward_params, config_params)
-
-    expl_env = env # variant['env_class'](**variant['env_kwargs'])
-    eval_env = env # variant['env_class'](**variant['env_kwargs'])
-
-    observation_key = 'latent_observation'
-    desired_goal_key = 'latent_desired_goal'
-    achieved_goal_key = desired_goal_key.replace("desired", "achieved")
-    es = GaussianAndEpislonStrategy(
-        action_space=expl_env.action_space,
-        max_sigma=.2,
-        min_sigma=.2,  # constant sigma
-        epsilon=.3,
-    )
-    obs_dim = expl_env.observation_space.spaces['observation'].low.size
-    goal_dim = expl_env.observation_space.spaces['desired_goal'].low.size
-    action_dim = expl_env.action_space.low.size
-    qf1 = FlattenMlp(
-        input_size=obs_dim + goal_dim + action_dim,
-        output_size=1,
-        # output_activation=TorchMaxClamp(0.0),
-        **variant['qf_kwargs']
-    )
-    qf2 = FlattenMlp(
-        input_size=obs_dim + goal_dim + action_dim,
-        output_size=1,
-        # output_activation=TorchMaxClamp(0.0),
-        **variant['qf_kwargs']
-    )
-    target_qf1 = FlattenMlp(
-        input_size=obs_dim + goal_dim + action_dim,
-        output_size=1,
-        # output_activation=TorchMaxClamp(0.0),
-        **variant['qf_kwargs']
-    )
-    target_qf2 = FlattenMlp(
-        input_size=obs_dim + goal_dim + action_dim,
-        output_size=1,
-        # output_activation=TorchMaxClamp(0.0),
-        **variant['qf_kwargs']
-    )
-    policy = TanhMlpPolicy(
-        input_size=obs_dim + goal_dim,
-        output_size=action_dim,
-        **variant['policy_kwargs']
-    )
-    target_policy = TanhMlpPolicy(
-        input_size=obs_dim + goal_dim,
-        output_size=action_dim,
-        **variant['policy_kwargs']
-    )
-    expl_policy = PolicyWrappedWithExplorationStrategy(
-        exploration_strategy=es,
-        policy=policy,
-    )
-    replay_buffer = ObsDictRelabelingBuffer(
-        env=eval_env,
-        observation_key=observation_key,
-        desired_goal_key=desired_goal_key,
-        achieved_goal_key=achieved_goal_key,
-        **variant['replay_buffer_kwargs']
-    )
-    demo_train_buffer = ObsDictRelabelingBuffer(
-        env=env,
-        observation_key=observation_key,
-        desired_goal_key=desired_goal_key,
-        achieved_goal_key=achieved_goal_key,
-        **variant['replay_buffer_kwargs']
-    )
-    demo_test_buffer = ObsDictRelabelingBuffer(
-        env=env,
-        observation_key=observation_key,
-        desired_goal_key=desired_goal_key,
-        achieved_goal_key=achieved_goal_key,
-        **variant['replay_buffer_kwargs']
-    )
-    td3bc_trainer = TD3BCTrainer(
-        env=env,
-        policy=policy,
-        qf1=qf1,
-        qf2=qf2,
-        replay_buffer=replay_buffer,
-        demo_train_buffer=demo_train_buffer,
-        demo_test_buffer=demo_test_buffer,
-        target_qf1=target_qf1,
-        target_qf2=target_qf2,
-        target_policy=target_policy,
-        **variant['trainer_kwargs']
-    )
-    trainer = HERTrainer(td3bc_trainer)
-    eval_path_collector = GoalConditionedPathCollector(
-        eval_env,
-        policy,
-        observation_key=observation_key,
-        desired_goal_key=desired_goal_key,
-    )
-    expl_path_collector = GoalConditionedPathCollector(
-        expl_env,
-        expl_policy,
-        observation_key=observation_key,
-        desired_goal_key=desired_goal_key,
-    )
-    algorithm = TorchBatchRLAlgorithm(
-        trainer=trainer,
-        exploration_env=expl_env,
-        evaluation_env=eval_env,
-        exploration_data_collector=expl_path_collector,
-        evaluation_data_collector=eval_path_collector,
-        replay_buffer=replay_buffer,
-        **variant['algo_kwargs']
-    )
-
-    if variant.get("save_video", True):
-        video_func = VideoSaveFunction(
-            env,
-            variant,
-        )
-        algorithm.post_train_funcs.append(video_func)
-
-    algorithm.to(ptu.device)
-
-    td3bc_trainer.load_demos()
-    td3bc_trainer.pretrain_policy_with_bc()
-    td3bc_trainer.pretrain_q_with_bc_data()
-
-    algorithm.train()
diff --git a/railrl/launchers/launcher_util.py b/railrl/launchers/launcher_util.py
index 73aff79..1a2df0b 100644
--- a/railrl/launchers/launcher_util.py
+++ b/railrl/launchers/launcher_util.py
@@ -145,8 +145,8 @@ def run_experiment(
         import doodad.mode
         import doodad.ssh
     except ImportError:
-        print("Doodad not set up! Running experiment mode:", mode)
-        # mode = 'here_no_doodad'
+        print("Doodad not set up! Running experiment here.")
+        mode = 'here_no_doodad'
     global ec2_okayed
     global gpu_ec2_okayed
     global target_mount
diff --git a/railrl/misc/plot_util.py b/railrl/misc/plot_util.py
index eeb523a..b98f616 100644
--- a/railrl/misc/plot_util.py
+++ b/railrl/misc/plot_util.py
@@ -34,7 +34,7 @@ def load_exps(dirnames, filter_fn=true_fn, suppress_output=False, progress_filen
         if progress_filename == "progress.csv":
             return core.load_exps_data(dirnames)
         else:
-            return core.load_exps_data(dirnames, data_filename=progress_filename)
+            return core.load_exps_data(dirnames, progress_filename=progress_filename)
 
     if suppress_output:
         with suppress_stdout():
@@ -155,8 +155,6 @@ def comparison(exps, key, vary = ["expdir"], f=true_fn, smooth=identity_fn, figs
                         k_new = remap_keys[k]
                         vals.append(d[k_new])
                     else:
-                        print("not found key", k)
-                        print(d.keys())
                         error_key_not_found_in_logs
                 y = reduce_op(vals)
             else:
diff --git a/railrl/torch/data_management/normalizer.py b/railrl/torch/data_management/normalizer.py
index fc19342..745fb1e 100644
--- a/railrl/torch/data_management/normalizer.py
+++ b/railrl/torch/data_management/normalizer.py
@@ -14,8 +14,8 @@ class TorchNormalizer(Normalizer):
             self.synchronize()
         if clip_range is None:
             clip_range = self.default_clip_range
-        mean = ptu.np_to_var(self.mean, requires_grad=False)
-        std = ptu.np_to_var(self.std, requires_grad=False)
+        mean = ptu.from_numpy(self.mean)
+        std = ptu.from_numpy(self.std)
         if v.dim() == 2:
             # Unsqueeze along the batch use automatic broadcasting
             mean = mean.unsqueeze(0)
@@ -25,8 +25,8 @@ class TorchNormalizer(Normalizer):
     def denormalize(self, v):
         if not self.synchronized:
             self.synchronize()
-        mean = ptu.np_to_var(self.mean, requires_grad=False)
-        std = ptu.np_to_var(self.std, requires_grad=False)
+        mean = ptu.from_numpy(self.mean)
+        std = ptu.from_numpy(self.std)
         if v.dim() == 2:
             mean = mean.unsqueeze(0)
             std = std.unsqueeze(0)
diff --git a/railrl/torch/grill/common.py b/railrl/torch/grill/common.py
index 1db4482..d85d555 100644
--- a/railrl/torch/grill/common.py
+++ b/railrl/torch/grill/common.py
@@ -194,6 +194,7 @@ def generate_vae_dataset(variant):
     n_random_steps = variant.get('n_random_steps', 100)
     vae_dataset_specific_env_kwargs = variant.get('vae_dataset_specific_env_kwargs', None)
     save_file_prefix = variant.get('save_file_prefix', None)
+    save_directory = variant.get('save_directory', None)
     non_presampled_goal_img_is_garbage = variant.get('non_presampled_goal_img_is_garbage', None)
 
     conditional_vae_dataset = variant.get('conditional_vae_dataset', False)
@@ -213,15 +214,39 @@ def generate_vae_dataset(variant):
     from railrl.data_management.dataset  import (
         TrajectoryDataset, ImageObservationDataset, InitialObservationDataset,
         EnvironmentDataset, ConditionalDynamicsDataset, InitialObservationNumpyDataset,
-        InfiniteBatchLoader,
-    )
+        InfiniteBatchLoader,)
+    import datetime
 
     info = {}
     if dataset_path is not None:
+
         dataset = load_local_or_remote_file(dataset_path)
         dataset = dataset.item()
         N = dataset['observations'].shape[0] * dataset['observations'].shape[1]
         n_random_steps = dataset['observations'].shape[1]
+
+    # if dataset_path is not None:
+    #     all_obs = []
+    #     all_env = []
+    #     for d in dataset_path:
+    #         dataset_i = load_local_or_remote_file(d).item()
+    #         obs = dataset_i['observations']
+    #         env = dataset_i['env']
+    #         episode_length = obs.shape[1]
+    #         obs = obs.reshape(-1, obs.shape[2])
+    #         env = np.repeat(env, episode_length, axis=0)
+    #         all_obs.append(obs)
+    #         all_env.append(env)
+    #     dataset = {}
+    #     dataset['observations'] = np.concatenate(all_obs)
+    #     dataset['observations'] = dataset['observations'].reshape(dataset['observations'].shape[0], 1, -1)
+    #     #dataset['actions'] = np.concatenate([load_local_or_remote_file(d).item()['actions'] for d in dataset_path])
+    #     dataset['env'] = np.concatenate(all_env)
+    #         #     for d in dataset_path:
+    #         # dataset = load_local_or_remote_file(d)
+    #         # dataset = dataset.item()
+    #     N = dataset['observations'].shape[0]
+    #     n_random_steps = 1#dataset['observations'].shape[1]
     else:
         if env_kwargs is None:
             env_kwargs = {}
@@ -277,13 +302,14 @@ def generate_vae_dataset(variant):
                 policy.to(ptu.device)
             if random_rollout_data:
                 from railrl.exploration_strategies.ou_strategy import OUStrategy
-                policy = OUStrategy(env.action_space)
+                policy = OUStrategy(env.action_space, max_sigma=0.5)
 
             if save_trajectories:
+                imlength = imsize * imsize * num_channels
                 dataset = {
-                    'observations': np.zeros((N // n_random_steps, n_random_steps, imsize * imsize * num_channels), dtype=np.uint8),
+                    'observations': np.zeros((N // n_random_steps, n_random_steps, imlength), dtype=np.uint8),
                     'actions': np.zeros((N // n_random_steps, n_random_steps, env.action_space.shape[0]), dtype=np.float),
-                    'env': np.zeros((N // n_random_steps, imsize * imsize * num_channels), dtype=np.uint8),
+                    'env': np.zeros((N // n_random_steps, imlength), dtype=np.uint8),
                     }
             else:
                 dataset = np.zeros((N, imsize * imsize * num_channels), dtype=np.uint8)
@@ -310,11 +336,25 @@ def generate_vae_dataset(variant):
                         env.reset()
                         policy.reset()
                         env_img = env._get_obs()['image_observation']
+                        curr_goal = env.sample_goal()['state_desired_goal']
                         if random_rollout_data_set_to_goal:
                             env.set_to_goal(env.get_goal())
                     obs = env._get_obs()
-                    u = policy.get_action_from_raw_action(env.action_space.sample())
+                
+                    actions_hardcoded = False
+                    if actions_hardcoded:
+                        if i % 20 == 0:
+                            curr_goal = env.sample_goal()['state_desired_goal']
+                        #print("current goal", curr_goal)
+                        a = (curr_goal - obs['state_observation'])[:2]
+                        u = a / np.linalg.norm(a)
+                        #u = np.clip(obs['state_desired_goal'] - obs['state_observation'], -1, 1)[:2]
+                        u = policy.get_action_from_raw_action(u)
+                    else:
+                        u = policy.get_action_from_raw_action(env.action_space.sample())
                     env.step(u)
+
+
                 elif oracle_dataset_using_set_to_goal:
                     print(i)
                     goal = env.sample_goal()
@@ -341,9 +381,14 @@ def generate_vae_dataset(variant):
                     cv2.imshow('img', img)
                     cv2.waitKey(1)
                     # radius = input('waiting...')
+            if save_directory:
+                save_file_prefix = save_file_prefix or str(datetime.datetime.now())
+                filename = save_directory + save_file_prefix + ".npy"
+                np.save(filename, dataset)
+            else:
+                np.save(filename, dataset)
             print("done making training data", filename, time.time() - now)
-            np.save(filename, dataset)
-            np.save(filename[:-4] + 'labels.npy', np.array(labels))
+            #np.save(filename[:-4] + 'labels.npy', np.array(labels))
 
     info['train_labels'] = []
     info['test_labels'] = []
@@ -450,15 +495,15 @@ def generate_vae_dataset(variant):
         test_dataset = InfiniteBatchLoader(test_data_loader)
     else:
         n = int(N * test_p)
-        train_dataset = ImageObservationDataset(dataset[:n, :])
-        test_dataset = ImageObservationDataset(dataset[n:, :])
+        train_dataset = ImageObservationDataset(dataset['observations'][:n, :].reshape(-1, imlength))
+        test_dataset = ImageObservationDataset(dataset['observations'][n:, :].reshape(-1, imlength))
     return train_dataset, test_dataset, info
 
 def get_envs(variant):
     from multiworld.core.image_env import ImageEnv
     from railrl.envs.vae_wrappers import VAEWrappedEnv, ConditionalVAEWrappedEnv
     from railrl.misc.asset_loader import load_local_or_remote_file
-    from railrl.torch.vae.conditional_conv_vae import CVAE, ConditionalConvVAE
+    from railrl.torch.vae.conditional_conv_vae import CVAE, DeltaCVAE
 
     render = variant.get('render', False)
     vae_path = variant.get("vae_path", None)
@@ -486,6 +531,7 @@ def get_envs(variant):
                 init_camera=init_camera,
                 transpose=True,
                 normalize=True,
+                non_presampled_goal_img_is_garbage=True,
             )
         if presample_goals:
             """
diff --git a/railrl/torch/grill/cvae_experiments.py b/railrl/torch/grill/cvae_experiments.py
index 8422a01..850bc8a 100644
--- a/railrl/torch/grill/cvae_experiments.py
+++ b/railrl/torch/grill/cvae_experiments.py
@@ -25,6 +25,12 @@ def grill_her_td3_offpolicy_online_vae_full_experiment(variant):
     train_vae_and_update_variant(variant)
     grill_her_td3_experiment_offpolicy_online_vae(variant['grill_variant'])
 
+def grill_her_planning_offpolicy_online_vae_full_experiment(variant):
+    variant['grill_variant']['save_vae_data'] = True
+    full_experiment_variant_preprocess(variant)
+    train_vae_and_update_variant(variant)
+    grill_her_planning_experiment_offpolicy_online_vae(variant['grill_variant'])
+
 def grill_her_td3_experiment_offpolicy_online_vae(variant):
     import railrl.samplers.rollout_functions as rf
     import railrl.torch.pytorch_util as ptu
@@ -94,13 +100,14 @@ def grill_her_td3_experiment_offpolicy_online_vae(variant):
         hidden_sizes=hidden_sizes,
         # **variant['policy_kwargs']
     )
+    es = get_exploration_strategy(variant, env)
 
-    es = GaussianAndEpislonStrategy(
-        action_space=env.action_space,
-        max_sigma=.2,
-        min_sigma=.2,  # constant sigma
-        epsilon=.3,
-    )
+    # es = GaussianAndEpislonStrategy(
+    #     action_space=env.action_space,
+    #     max_sigma=.2,
+    #     min_sigma=.2,  # constant sigma
+    #     epsilon=.3,
+    # )
     expl_policy = PolicyWrappedWithExplorationStrategy(
         exploration_strategy=es,
         policy=policy,
@@ -154,7 +161,6 @@ def grill_her_td3_experiment_offpolicy_online_vae(variant):
         observation_key=observation_key,
         desired_goal_key=desired_goal_key,
     )
-
     algorithm = OnlineVaeOffpolicyAlgorithm(
         trainer=trainer,
         exploration_env=env,
@@ -180,6 +186,175 @@ def grill_her_td3_experiment_offpolicy_online_vae(variant):
 
     algorithm.to(ptu.device)
     vae.to(ptu.device)
-
     algorithm.pretrain()
     algorithm.train()
+
+def grill_her_planning_experiment_offpolicy_online_vae(variant):
+    import railrl.samplers.rollout_functions as rf
+    import railrl.torch.pytorch_util as ptu
+    import railrl.samplers.rollout_functions as rf
+    from railrl.data_management.online_vae_replay_buffer import \
+        OnlineVaeRelabelingBuffer
+    from railrl.torch.networks import FlattenMlp, TanhMlpPolicy, MlpPolicy
+    from railrl.torch.sac.policies import TanhGaussianPolicy
+    from railrl.torch.vae.vae_trainer import ConvVAETrainer
+    from railrl.torch.td3.td3 import TD3
+    from railrl.torch.sac.sap import SAPTrainer
+    from railrl.exploration_strategies.base import (
+        PolicyWrappedWithExplorationStrategy
+    )
+    from railrl.exploration_strategies.base import \
+        PolicyWrappedWithPlanner
+    from railrl.exploration_strategies.gaussian_and_epislon import \
+        GaussianAndEpislonStrategy
+    from railrl.torch.vae.online_vae_offpolicy_algorithm import OnlineVaeOffpolicyAlgorithm
+
+    grill_preprocess_variant(variant)
+    env = get_envs(variant)
+
+    uniform_dataset_fn = variant.get('generate_uniform_dataset_fn', None)
+    if uniform_dataset_fn:
+        uniform_dataset=uniform_dataset_fn(
+            **variant['generate_uniform_dataset_kwargs']
+        )
+    else:
+        uniform_dataset=None
+
+    observation_key = variant.get('observation_key', 'latent_observation')
+    desired_goal_key = variant.get('desired_goal_key', 'latent_desired_goal')
+    achieved_goal_key = desired_goal_key.replace("desired", "achieved")
+    obs_dim = (
+            env.observation_space.spaces[observation_key].low.size
+            + env.observation_space.spaces[desired_goal_key].low.size
+    )
+    state_dim = env.observation_space.spaces[observation_key].low.size
+    action_dim = env.action_space.low.size
+    hidden_sizes = variant.get('hidden_sizes', [400, 300])
+
+    qf1 = FlattenMlp(
+        input_size=obs_dim + action_dim,
+        output_size=1,
+        hidden_sizes=hidden_sizes,
+    )
+    qf2 = FlattenMlp(
+        input_size=obs_dim + action_dim,
+        output_size=1,
+        hidden_sizes=hidden_sizes,
+    )
+    target_qf1 = FlattenMlp(
+        input_size=obs_dim + action_dim,
+        output_size=1,
+        hidden_sizes=hidden_sizes,
+    )
+    target_qf2 = FlattenMlp(
+        input_size=obs_dim + action_dim,
+        output_size=1,
+        hidden_sizes=hidden_sizes,
+    )
+    actor_policy = TanhGaussianPolicy(
+        obs_dim=obs_dim,
+        action_dim=action_dim,
+        hidden_sizes=hidden_sizes,
+    )
+
+    planner_policy = TanhGaussianPolicy(
+        obs_dim=obs_dim,
+        action_dim=state_dim,
+        hidden_sizes=hidden_sizes,
+    )
+
+    model = FlattenMlp(
+        input_size=state_dim + action_dim,
+        output_size=state_dim,
+        hidden_sizes=[128, 128, 128, 128, 128,],
+    )
+
+    eval_policy = PolicyWrappedWithPlanner(
+        MakeDeterministic(actor_policy),
+        MakeDeterministic(planner_policy),
+        state_dim)
+
+    expl_policy = PolicyWrappedWithPlanner(
+        actor_policy,
+        planner_policy,
+        state_dim)
+
+    trainer = SAPTrainer(
+        env=env,
+        actor_policy=actor_policy,
+        planner_policy=planner_policy,
+        model=model,
+        qf1=qf1,
+        qf2=qf2,
+        target_qf1=target_qf1,
+        target_qf2=target_qf2,
+        state_dim=state_dim,
+        act_dim=action_dim,
+        **variant['twin_sac_trainer_kwargs']
+    )
+
+    trainer = HERTrainer(trainer)
+    vae = env.vae
+    replay_buffer_class = variant.get("replay_buffer_class", OnlineVaeRelabelingBuffer)
+    replay_buffer = replay_buffer_class(
+        vae=env.vae,
+        env=env,
+        observation_key=observation_key,
+        desired_goal_key=desired_goal_key,
+        achieved_goal_key=achieved_goal_key,
+
+        **variant['replay_buffer_kwargs']
+    )
+    replay_buffer.representation_size = vae.representation_size
+
+    vae_trainer_class = variant.get("vae_trainer_class", ConvVAETrainer)
+    vae_trainer = vae_trainer_class(
+        env.vae,
+        **variant['online_vae_trainer_kwargs']
+    )
+    assert 'vae_training_schedule' not in variant, "Just put it in algo_kwargs"
+    max_path_length = variant['max_path_length']
+
+    eval_path_collector = VAEWrappedEnvPathCollector(
+        variant['evaluation_goal_sampling_mode'],
+        env,
+        eval_policy,
+        max_path_length,
+        observation_key=observation_key,
+        desired_goal_key=desired_goal_key,
+    )
+    expl_path_collector = VAEWrappedEnvPathCollector(
+        variant['exploration_goal_sampling_mode'],
+        env,
+        expl_policy,
+        max_path_length,
+        observation_key=observation_key,
+        desired_goal_key=desired_goal_key,
+    )
+    algorithm = OnlineVaeOffpolicyAlgorithm(
+        trainer=trainer,
+        exploration_env=env,
+        evaluation_env=env,
+        exploration_data_collector=expl_path_collector,
+        evaluation_data_collector=eval_path_collector,
+        replay_buffer=replay_buffer,
+        vae=vae,
+        vae_trainer=vae_trainer,
+        uniform_dataset=uniform_dataset,
+        max_path_length=max_path_length,
+        **variant['algo_kwargs']
+    )
+
+    if variant.get("save_video", True):
+        video_func = VideoSaveFunction(
+            env,
+            variant,
+        )
+        algorithm.post_train_funcs.append(video_func)
+    if variant['custom_goal_sampler'] == 'replay_buffer':
+        env.custom_goal_sampler = replay_buffer.sample_buffer_goals
+
+    algorithm.to(ptu.device)
+    vae.to(ptu.device)
+    algorithm.pretrain()
+    algorithm.train()
\ No newline at end of file
diff --git a/railrl/torch/grill/launcher.py b/railrl/torch/grill/launcher.py
index 317171e..8be8214 100644
--- a/railrl/torch/grill/launcher.py
+++ b/railrl/torch/grill/launcher.py
@@ -1,6 +1,8 @@
 import os.path as osp
 import time
 
+import sys
+sys.path.remove('/opt/ros/kinetic/lib/python2.7/dist-packages')
 import cv2
 import numpy as np
 
diff --git a/railrl/torch/grill/video_gen.py b/railrl/torch/grill/video_gen.py
index 3f50735..e63760a 100644
--- a/railrl/torch/grill/video_gen.py
+++ b/railrl/torch/grill/video_gen.py
@@ -1,4 +1,5 @@
 import argparse
+import pickle as pkl
 import json
 import os
 import os.path as osp
@@ -24,19 +25,13 @@ import scipy.misc
 from multiworld.core.image_env import ImageEnv
 from railrl.core import logger
 from railrl.envs.vae_wrappers import temporary_mode
-import pickle
 
 class VideoSaveFunction:
     def __init__(self, env, variant):
         self.logdir = logger.get_snapshot_dir()
         self.save_period = variant.get('save_video_period', 50)
-        self.dump_video_kwargs = variant.get("dump_video_kwargs", dict())
+        self.dump_video_kwargs = variant.get("self.dump_video_kwargs", dict())
         self.dump_video_kwargs['imsize'] = env.imsize
-        self.dump_video_kwargs.setdefault("rows", 2)
-        self.dump_video_kwargs.setdefault("columns", 5)
-        self.dump_video_kwargs.setdefault("unnormalize", True)
-        self.exploration_goal_image_key = self.dump_video_kwargs.pop("exploration_goal_image_key", "decoded_goal_image")
-        self.evaluation_goal_image_key = self.dump_video_kwargs.pop("evaluation_goal_image_key", "image_desired_goal")
 
     def __call__(self, algo, epoch):
         expl_data_collector = algo.expl_data_collector
@@ -46,7 +41,9 @@ class VideoSaveFunction:
             dump_paths(algo.expl_env,
                 filename,
                 expl_paths,
-                self.exploration_goal_image_key,
+                "decoded_goal_image",
+                rows=2,
+                columns=5, # 5
                 **self.dump_video_kwargs,
             )
 
@@ -57,7 +54,9 @@ class VideoSaveFunction:
             dump_paths(algo.eval_env,
                 filename,
                 eval_paths,
-                self.evaluation_goal_image_key,
+                "image_desired_goal",
+                rows=2,
+                columns=5, # 5
                 **self.dump_video_kwargs,
             )
 
@@ -70,17 +69,19 @@ def add_border(img, pad_length, pad_color, imsize=84):
     img2[pad_length:-pad_length, pad_length:-pad_length, :] = img
     return img2
 
-def get_image(imgs, imwidth, imheight, pad_length=1, pad_color=255, unnormalize=True):
-    if len(imgs[0].shape) == 1:
-        for i in range(len(imgs)):
-            imgs[i] = imgs[i].reshape(-1, imwidth, imheight).transpose(2, 1, 0)
-    img = np.concatenate(imgs)
-    if unnormalize:
-        img = np.uint8(255 * img)
+
+def get_image(goal, obs, recon_obs, imsize=84, pad_length=1, pad_color=255):
+    if len(goal.shape) == 1:
+        goal = goal.reshape(-1, imsize, imsize).transpose(2, 1, 0)
+        obs = obs.reshape(-1, imsize, imsize).transpose(2,1,0)
+        recon_obs = recon_obs.reshape(-1, imsize, imsize).transpose(2,1,0)
+    img = np.concatenate((goal, obs, recon_obs))
+    img = np.uint8(255 * img)
     if pad_length > 0:
         img = add_border(img, pad_length, pad_color)
     return img
 
+
 def dump_video(
         env,
         policy,
@@ -128,14 +129,13 @@ def dump_video(
             else:
                 recon = d['image_observation']
             l.append(
-                get_image([
+                get_image(
                     d['image_desired_goal'], # d['decoded_goal_image'], # d['image_desired_goal'],
                     d['image_observation'],
-                    recon,],
-                    imwidth=imsize,
-                    imheight=imsize,
+                    recon,
                     pad_length=pad_length,
                     pad_color=pad_color,
+                    imsize=imsize,
                 )
             )
         frames += l
@@ -189,25 +189,12 @@ def dump_paths(
         dirname_to_save_images=None,
         subdirname="rollouts",
         imsize=84,
-        imwidth=None,
-        imheight=None,
-        num_imgs=3, # how many vertical images we stack per rollout
-        dump_pickle=False,
-        unnormalize=True,
 ):
     # num_channels = env.vae.input_channels
     num_channels = 1 if env.grayscale else 3
     frames = []
-
-    imwidth = imwidth or imsize # 500
-    imheight = imheight or imsize # 300
-    num_gaps = num_imgs - 1 # 2
-
-    H = num_imgs * imheight # imsize
-    W = imwidth # imsize
-
-    # H = 3 * imsize
-    # W = imsize
+    H = 3 * imsize
+    W = imsize
     rows = min(rows, int(len(paths) / columns))
     N = rows * columns
     is_vae_env = isinstance(env, VAEWrappedEnv)
@@ -224,19 +211,14 @@ def dump_paths(
                 recon = np.clip(env._reconstruct_img(d['image_observation']), 0, 1)
             else:
                 recon = d['image_observation']
-            imgs = [
-                d[goal_image_key], # d['image_desired_goal'],
-                d['image_observation'],
-                recon,
-            ][:num_imgs]
             l.append(
                 get_image(
-                    imgs,
-                    imwidth,
-                    imheight,
+                    d[goal_image_key], # d['image_desired_goal'],
+                    d['image_observation'],
+                    recon,
                     pad_length=pad_length,
                     pad_color=pad_color,
-                    unnormalize=unnormalize,
+                    imsize=imsize,
                 )
             )
         frames += l
@@ -257,10 +239,10 @@ def dump_paths(
 
     frames = np.array(frames, dtype=np.uint8)
     path_length = frames.size // (
-            N * (H + num_gaps*pad_length) * (W + num_gaps*pad_length) * num_channels
+            N * (H + 2*pad_length) * (W + 2*pad_length) * num_channels
     )
     frames = np.array(frames, dtype=np.uint8).reshape(
-        (N, path_length, H + num_gaps * pad_length, W + num_gaps * pad_length, num_channels)
+        (N, path_length, H + 2 * pad_length, W + 2 * pad_length, num_channels)
     )
     f1 = []
     for k1 in range(columns):
@@ -268,13 +250,12 @@ def dump_paths(
         for k2 in range(rows):
             k = k1 * rows + k2
             f2.append(frames[k:k+1, :, :, :, :].reshape(
-                (path_length, H + num_gaps * pad_length, W + num_gaps * pad_length, num_channels)
+                (path_length, H + 2 * pad_length, W + 2 * pad_length, num_channels)
             ))
         f1.append(np.concatenate(f2, axis=1))
     outputdata = np.concatenate(f1, axis=2)
     skvideo.io.vwrite(filename, outputdata)
+    offpolicy_data = filename[:-4] + ".p"
+    pkl.dump(paths, open(offpolicy_data, "wb"))
     print("Saved video to ", filename)
 
-    if dump_pickle:
-        pickle_filename = filename[:-4] + ".p"
-        pickle.dump(paths, open(pickle_filename, "wb"))
diff --git a/railrl/torch/networks.py b/railrl/torch/networks.py
index 6b59e53..d51a964 100644
--- a/railrl/torch/networks.py
+++ b/railrl/torch/networks.py
@@ -17,12 +17,6 @@ from railrl.torch.modules import SelfOuterProductLinear, LayerNorm
 
 import numpy as np
 
-class TorchMaxClamp:
-    def __init__(self, max_value):
-        self.max_value = max_value
-
-    def __call__(self, x):
-        return torch.clamp(x, max=self.max_value)
 
 class PretrainedCNN(PyTorchModule):
     # Uses a pretrained CNN architecture from torchvision
diff --git a/railrl/torch/sac/ensemble_planning.py b/railrl/torch/sac/ensemble_planning.py
new file mode 100644
index 0000000..fd21240
--- /dev/null
+++ b/railrl/torch/sac/ensemble_planning.py
@@ -0,0 +1,330 @@
+from collections import OrderedDict
+from torch.distributions import Normal
+import numpy as np
+import torch
+import torch.optim as optim
+from torch import nn as nn
+
+import railrl.torch.pytorch_util as ptu
+from railrl.misc.eval_util import create_stats_ordered_dict
+from railrl.torch.core import np_to_pytorch_batch
+from railrl.torch.torch_rl_algorithm import TorchTrainer
+
+
+class SACTrainer(TorchTrainer):
+    def __init__(
+            self,
+            env,
+            policy_ensemble,
+            planner_policy,
+            q_ensemble,
+            q_target_ensemble,
+
+            discount=0.99,
+            reward_scale=1.0,
+
+            policy_lr=1e-3,
+            qf_lr=1e-3,
+            optimizer_class=optim.Adam,
+
+            plotter=None,
+            render_eval_paths=False,
+
+            use_automatic_entropy_tuning=True,
+            target_entropy=None,
+    ):
+        super().__init__()
+        self.env = env
+        self.policy_ensemble = policy_ensemble
+        self.expl_policy = expl_policy
+        self.planner_policy = planner_policy
+        self.eval_ensemble = eval_ensemble
+        self.expl_ensemble = expl_ensemble
+
+        self.use_automatic_entropy_tuning = use_automatic_entropy_tuning
+        if self.use_automatic_entropy_tuning:
+            if target_entropy:
+                self.target_entropy = target_entropy
+            else:
+                self.target_entropy = -np.prod(self.env.action_space.shape).item()  # heuristic value from Tuomas
+            self.eval_log_alpha = ptu.zeros(1, requires_grad=True)
+            self.expl_log_alpha = ptu.zeros(1, requires_grad=True)
+            self.eval_alpha_optimizer = optimizer_class(
+                [self.eval_log_alpha],
+                lr=policy_lr,
+            )
+            self.expl_alpha_optimizer = optimizer_class(
+                [self.expl_log_alpha],
+                lr=policy_lr,
+            )
+        self.plotter = plotter
+        self.render_eval_paths = render_eval_paths
+
+        self.qf_criterion = nn.MSELoss()
+
+        self.eval_policy_optimizer = optimizer_class(
+            self.eval_policy.parameters(),
+            lr=policy_lr,
+        )
+        self.expl_policy_optimizer = optimizer_class(
+            self.expl_policy.parameters(),
+            lr=policy_lr,
+        )
+        self.eval_ensemble_optimizer = [optimizer_class(q.parameters(), lr=qf_lr,) \
+            for q in self.eval_ensemble]
+        self.expl_ensemble_optimizer = [optimizer_class(q.parameters(), lr=qf_lr,) \
+            for q in self.expl_ensemble]
+        self.planner_policy_optimizer = optimizer_class(
+            self.planner_policy.parameters(),
+            lr=policy_lr,
+        )
+
+        self.discount = discount
+        self.reward_scale = reward_scale
+        self.eval_statistics = OrderedDict()
+        self._n_train_steps_total = 0
+        self._need_to_update_eval_statistics = True
+
+    def train_from_torch(self, batch):
+        rewards = batch['rewards']
+        terminals = batch['terminals']
+        obs = batch['observations']
+        actions = batch['actions']
+        next_obs = batch['next_observations']
+
+        """
+        Policy and Alpha Loss
+        """
+        new_obs_actions, policy_mean, policy_log_std, log_pi, *_ = self.eval_policy(
+            obs, reparameterize=True, return_log_prob=True,
+        )
+        if self.use_automatic_entropy_tuning:
+            eval_alpha_loss = -(self.eval_log_alpha * (log_pi + self.target_entropy).detach()).mean()
+            self.eval_alpha_optimizer.zero_grad()
+            eval_alpha_loss.backward()
+            self.eval_alpha_optimizer.step()
+            eval_alpha = self.eval_log_alpha.exp()
+
+        eval_q_new_actions = torch.cat([q(obs, new_obs_actions) for q in self.eval_ensemble], dim=1)
+
+
+        #x = torch.min(eval_q_new_actions,dim=1)
+        #import ipdb; ipdb.set_trace()
+        eval_policy_loss = (eval_alpha*log_pi - torch.mean(eval_q_new_actions, dim=1).reshape(-1, 1)).mean()
+
+            #TODO: CHANGE BACK TO MEAN
+        """
+        QF Loss
+        """
+        q_index = self._n_train_steps_total % len(self.eval_ensemble_optimizer)
+        q_train, q_optimizer = self.eval_ensemble[q_index], self.eval_ensemble_optimizer[q_index]
+        eval_q_pred = q_train(obs, actions)
+
+        var_pre = torch.std(torch.cat([q(obs, actions) for q in self.eval_ensemble], dim=1), dim=1).reshape(-1, 1).detach()
+
+        
+        # Make sure policy accounts for squashing functions like tanh correctly!
+        new_next_actions, _, _, new_log_pi, *_ = self.eval_policy(
+            next_obs, reparameterize=True, return_log_prob=True,
+        )
+        
+        eval_predictions = torch.cat([self.eval_ensemble[i](next_obs, new_next_actions) \
+            for i in range(len(self.eval_ensemble)) if i != q_index], dim=1)
+        
+        target_q_values = torch.mean(eval_predictions, dim=1).reshape(-1, 1) \
+        - eval_alpha * new_log_pi
+
+        q_variance = torch.std(eval_predictions, dim=1).reshape(-1, 1)
+
+        q_target = self.reward_scale * rewards + (1. - terminals) * self.discount * target_q_values
+        eval_loss = self.qf_criterion(eval_q_pred, q_target.detach())
+
+        """
+        Update networks
+        """
+        q_optimizer.zero_grad()
+        eval_loss.backward()
+        q_optimizer.step()
+
+        self.eval_policy_optimizer.zero_grad()
+        eval_policy_loss.backward()
+        self.eval_policy_optimizer.step()
+
+        var_post = torch.std(torch.cat([q(obs, actions) for q in self.eval_ensemble], dim=1), dim=1).reshape(-1, 1)
+        delta_var = torch.abs(var_post - var_pre)
+
+        """
+        Train Exploration Networks
+        """
+
+
+        """
+        Policy and Alpha Loss
+        """
+        new_obs_actions, policy_mean, policy_log_std, log_pi, *_ = self.expl_policy(
+            obs, reparameterize=True, return_log_prob=True,
+        )
+        if self.use_automatic_entropy_tuning:
+            expl_alpha_loss = -(self.expl_log_alpha * (log_pi + self.target_entropy).detach()).mean()
+            self.expl_alpha_optimizer.zero_grad()
+            expl_alpha_loss.backward()
+            self.expl_alpha_optimizer.step()
+            expl_alpha = self.expl_log_alpha.exp()
+
+
+        expl_q_new_actions = torch.cat([q(obs, new_obs_actions) for q in self.expl_ensemble], dim=1)
+        expl_policy_loss = (expl_alpha*log_pi - torch.mean(expl_q_new_actions,dim=1).reshape(-1, 1)).mean()
+
+        """
+        QF Loss
+        """
+        q_train, q_optimizer = self.expl_ensemble[q_index], self.expl_ensemble_optimizer[q_index]
+        expl_q_pred = q_train(obs, actions)
+        
+        # Make sure policy accounts for squashing functions like tanh correctly!
+        new_next_actions, _, _, new_log_pi, *_ = self.expl_policy(
+            next_obs, reparameterize=True, return_log_prob=True,
+        )
+        
+        expl_predictions = torch.cat([self.expl_ensemble[i](next_obs, new_next_actions) \
+            for i in range(len(self.expl_ensemble)) if i != q_index], dim=1)
+        
+        target_q_values = torch.mean(expl_predictions, dim=1).reshape(-1, 1) \
+        - expl_alpha * new_log_pi
+
+        q_target = self.reward_scale * (rewards) + (1. - terminals) * self.discount * (target_q_values)
+        expl_loss = self.qf_criterion(expl_q_pred, q_target.detach())
+
+        """
+        Update networks
+        """
+        q_optimizer.zero_grad()
+        expl_loss.backward()
+        q_optimizer.step()
+
+        self.expl_policy_optimizer.zero_grad()
+        expl_policy_loss.backward()
+        self.expl_policy_optimizer.step()
+
+
+
+        midpoints = self.planner_policy(obs)
+
+        # if self.use_automatic_entropy_tuning:
+        #     planner_alpha_loss = -(self.log_planner_alpha * (log_pi + self.target_entropy).detach()).mean()
+        #     self.planner_alpha_optimizer.zero_grad()
+        #     planner_alpha_loss.backward()
+        #     self.planner_alpha_optimizer.step()
+        #     planner_alpha = self.log_planner_alpha.exp()
+        # else:
+        #     alpha_loss = 0
+        #     alpha = 1
+
+        curr_states = obs[:, :2]
+        goal_states = obs[:, 2:]
+
+
+        a_obs = torch.cat([curr_states, midpoints], dim=1)
+        b_obs = torch.cat([midpoints, goal_states], dim=1)
+
+        a_acts, _, _, a_log_pi, *_ = self.eval_policy(
+            a_obs, reparameterize=True, return_log_prob=True,
+        )
+        b_acts, _, _, b_log_pi, *_ = self.eval_policy(
+            b_obs, reparameterize=True, return_log_prob=True,
+        )
+        
+        dist_a = - torch.mean(torch.cat([q(a_obs, a_acts) for q in self.eval_ensemble], dim=1), dim=1,).reshape(-1, 1)
+        dist_b = - torch.mean(torch.cat([q(b_obs, b_acts) for q in self.eval_ensemble], dim=1), dim=1).reshape(-1, 1)
+        planner_policy_loss = (dist_b).mean()
+
+        self.planner_policy_optimizer.zero_grad()
+        planner_policy_loss.backward()
+        self.planner_policy_optimizer.step()
+        """
+        Save some statistics for eval
+        """
+        if self._need_to_update_eval_statistics:
+            self._need_to_update_eval_statistics = False
+            """
+            Eval should set this to None.
+            This way, these statistics are only computed for one batch.
+            """
+            eval_policy_loss = (eval_alpha*log_pi - torch.mean(eval_q_new_actions,dim=1).reshape(-1, 1)).mean()
+            expl_policy_loss = (expl_alpha*log_pi - torch.mean(expl_q_new_actions,dim=1).reshape(-1, 1)).mean()
+            self.eval_statistics['Eval Q_i Loss'.format(q_index)] = np.mean(ptu.get_numpy(eval_loss))
+            self.eval_statistics['Expl Q_i Loss'.format(q_index)] = np.mean(ptu.get_numpy(expl_loss))
+            #self.eval_statistics['QF2 Loss'] = np.mean(ptu.get_numpy(qf2_loss))
+            self.eval_statistics['Eval Policy Loss'] = np.mean(ptu.get_numpy(
+                eval_policy_loss
+            ))
+            self.eval_statistics['Expl Policy Loss'] = np.mean(ptu.get_numpy(
+                expl_policy_loss
+            ))
+
+            self.eval_statistics.update(create_stats_ordered_dict(
+                'Eval Ensemble Mean',
+                np.mean(ptu.get_numpy(torch.mean(eval_q_new_actions,dim=1))),
+            ))
+            self.eval_statistics.update(create_stats_ordered_dict(
+                'Eval Ensemble Std',
+                np.mean(ptu.get_numpy(torch.std(eval_q_new_actions,dim=1))),
+            ))
+
+            self.eval_statistics.update(create_stats_ordered_dict(
+                'Expl Ensemble Mean',
+                np.mean(ptu.get_numpy(torch.mean(expl_q_new_actions,dim=1))),
+            ))
+            self.eval_statistics.update(create_stats_ordered_dict(
+                'Expl Ensemble Std',
+                np.mean(ptu.get_numpy(torch.std(expl_q_new_actions,dim=1))),
+            ))
+
+            # self.eval_statistics.update(create_stats_ordered_dict(
+            #     'Q2 Predictions',
+            #     ptu.get_numpy(q2_pred),
+            # ))
+            # self.eval_statistics.update(create_stats_ordered_dict(
+            #     'Q Targets',
+            #     ptu.get_numpy(q_target),
+            # ))
+            # self.eval_statistics.update(create_stats_ordered_dict(
+            #     'Log Pis',
+            #     ptu.get_numpy(log_pi),
+            # ))
+            # self.eval_statistics.update(create_stats_ordered_dict(
+            #     'Policy mu',
+            #     ptu.get_numpy(policy_mean),
+            # ))
+            # self.eval_statistics.update(create_stats_ordered_dict(
+            #     'Policy log std',
+            #     ptu.get_numpy(policy_log_std),
+            # ))
+            # if self.use_automatic_entropy_tuning:
+            #     self.eval_statistics['Alpha'] = alpha.item()
+            #     self.eval_statistics['Alpha Loss'] = alpha_loss.item()
+        self._n_train_steps_total += 1
+
+    def get_diagnostics(self):
+        stats = super().get_diagnostics()
+        stats.update(self.eval_statistics)
+        return stats
+
+    def end_epoch(self, epoch):
+        self._need_to_update_eval_statistics = True
+
+    @property
+    def networks(self):
+        return [
+            self.eval_policy,
+            self.expl_policy,
+            self.planner_policy,] + self.eval_ensemble + self.expl_ensemble
+
+    def get_snapshot(self):
+        return dict(
+            eval_policy=self.eval_policy,
+            expl_policy=self.expl_policy,
+            planning_policy=self.planner_policy,
+            eval_ensemble=self.eval_ensemble,
+            expl_ensemble=self.expl_ensemble,
+        )
\ No newline at end of file
diff --git a/railrl/torch/sac/policies.py b/railrl/torch/sac/policies.py
index 727e3cc..9e0e263 100644
--- a/railrl/torch/sac/policies.py
+++ b/railrl/torch/sac/policies.py
@@ -4,7 +4,7 @@ from torch import nn as nn
 
 from railrl.policies.base import ExplorationPolicy, Policy
 from railrl.torch.core import eval_np
-from railrl.torch.distributions import TanhNormal
+from railrl.torch.distributions import TanhNormal, Normal
 from railrl.torch.networks import Mlp, CNN
 
 LOG_SIG_MAX = 2
@@ -252,6 +252,128 @@ class TanhGaussianPolicy(Mlp, ExplorationPolicy):
             mean_action_log_prob, pre_tanh_value,
         )
 
+class GaussianPolicy(Mlp, ExplorationPolicy):
+    """
+    Usage:
+
+    ```
+    policy = TanhGaussianPolicy(...)
+    action, mean, log_std, _ = policy(obs)
+    action, mean, log_std, _ = policy(obs, deterministic=True)
+    action, mean, log_std, log_prob = policy(obs, return_log_prob=True)
+    ```
+
+    Here, mean and log_std are the mean and log_std of the Gaussian that is
+    sampled from.
+
+    If deterministic is True, action = tanh(mean).
+    If return_log_prob is False (default), log_prob = None
+        This is done because computing the log_prob can be a bit expensive.
+    """
+
+    def __init__(
+            self,
+            hidden_sizes,
+            obs_dim,
+            action_dim,
+            std=None,
+            init_w=1e-3,
+            **kwargs
+    ):
+        super().__init__(
+            hidden_sizes,
+            input_size=obs_dim,
+            output_size=action_dim,
+            init_w=init_w,
+            **kwargs
+        )
+        self.log_std = None
+        self.std = std
+        if std is None:
+            last_hidden_size = obs_dim
+            if len(hidden_sizes) > 0:
+                last_hidden_size = hidden_sizes[-1]
+            self.last_fc_log_std = nn.Linear(last_hidden_size, action_dim)
+            self.last_fc_log_std.weight.data.uniform_(-init_w, init_w)
+            self.last_fc_log_std.bias.data.uniform_(-init_w, init_w)
+        else:
+            self.log_std = np.log(std)
+            assert LOG_SIG_MIN <= self.log_std <= LOG_SIG_MAX
+
+    def get_action(self, obs_np, deterministic=False):
+        actions = self.get_actions(obs_np[None], deterministic=deterministic)
+        return actions[0, :], {}
+
+    def get_actions(self, obs_np, deterministic=False):
+        return eval_np(self, obs_np, deterministic=deterministic)[0]
+
+    def forward(
+            self,
+            obs,
+            reparameterize=True,
+            deterministic=False,
+            return_log_prob=False,
+            return_entropy=False,
+            return_log_prob_of_mean=False,
+    ):
+        """
+        :param obs: Observation
+        :param deterministic: If True, do not sample
+        :param return_log_prob: If True, return a sample and its log probability
+        :param return_entropy: If True, return the true expected log
+        prob. Will not need to be differentiated through, so this can be a
+        number.
+        :param return_log_prob_of_mean: If True, return the true expected log
+        prob. Will not need to be differentiated through, so this can be a
+        number.
+        """
+        h = obs
+        for i, fc in enumerate(self.fcs):
+            h = self.hidden_activation(fc(h))
+        mean = self.last_fc(h)
+        if self.std is None:
+            log_std = self.last_fc_log_std(h)
+            log_std = torch.clamp(log_std, LOG_SIG_MIN, LOG_SIG_MAX)
+            std = torch.exp(log_std)
+        else:
+            std = self.std
+            log_std = self.log_std
+
+        log_prob = None
+        entropy = None
+        mean_action_log_prob = None
+        pre_tanh_value = None
+        if deterministic:
+            action = mean
+        else:
+            normal = Normal(mean, std)
+            if return_log_prob:
+                if reparameterize is True:
+                    action = normal.rsample()
+                else:
+                    action = normal.sample()
+                log_prob = normal.log_prob(action)
+                log_prob = log_prob.sum(dim=1, keepdim=True)
+            else:
+                if reparameterize is True:
+                    action = normal.rsample()
+                else:
+                    action = normal.sample()
+
+        if return_entropy:
+            entropy = log_std + 0.5 + np.log(2 * np.pi) / 2
+            # I'm not sure how to compute the (differential) entropy for a
+            # tanh(Gaussian)
+            entropy = entropy.sum(dim=1, keepdim=True)
+            raise NotImplementedError()
+        if return_log_prob_of_mean:
+            normal = Normal(mean, std)
+            mean_action_log_prob = normal.log_prob(mean)
+            mean_action_log_prob = mean_action_log_prob.sum(dim=1, keepdim=True)
+        return (
+            action, mean, log_std, log_prob, entropy, std,
+            mean_action_log_prob, pre_tanh_value,
+        )
 
 # noinspection PyMethodOverriding
 class TanhCNNGaussianPolicy(CNN, ExplorationPolicy):
diff --git a/railrl/torch/sac/sac_curious.py b/railrl/torch/sac/sac_curious.py
new file mode 100644
index 0000000..839f182
--- /dev/null
+++ b/railrl/torch/sac/sac_curious.py
@@ -0,0 +1,348 @@
+from collections import OrderedDict
+
+import numpy as np
+import torch
+import torch.optim as optim
+from torch import nn as nn
+
+import railrl.torch.pytorch_util as ptu
+from railrl.misc.eval_util import create_stats_ordered_dict
+from railrl.torch.core import np_to_pytorch_batch
+from railrl.torch.torch_rl_algorithm import TorchTrainer
+
+
+class SACTrainer(TorchTrainer):
+    def __init__(
+            self,
+            env,
+            policy,
+            expl_policy,
+            qf1,
+            expl_qf1,
+            qf2,
+            expl_qf2,
+            target_qf1,
+            expl_target_qf1,
+            target_qf2,
+            expl_target_qf2,
+            discount=0.99,
+            reward_scale=1.0,
+
+            policy_lr=1e-3,
+            qf_lr=1e-3,
+            optimizer_class=optim.Adam,
+
+            soft_target_tau=1e-2,
+            target_update_period=1,
+            plotter=None,
+            render_eval_paths=False,
+
+            use_automatic_entropy_tuning=True,
+            target_entropy=None,
+    ):
+        super().__init__()
+        self.env = env
+        self.policy = policy
+        self.expl_policy = expl_policy
+        self.qf1 = qf1
+        self.expl_qf1 = expl_qf1
+        self.qf2 = qf2
+        self.expl_qf2 = expl_qf2
+        self.target_qf1 = target_qf1
+        self.expl_target_qf1 = expl_target_qf1
+        self.target_qf2 = target_qf2
+        self.expl_target_qf2 = expl_target_qf2
+        self.soft_target_tau = soft_target_tau
+        self.target_update_period = target_update_period
+
+        self.use_automatic_entropy_tuning = use_automatic_entropy_tuning
+        if self.use_automatic_entropy_tuning:
+            if target_entropy:
+                self.target_entropy = target_entropy
+            else:
+                self.target_entropy = -np.prod(self.env.action_space.shape).item()  # heuristic value from Tuomas
+            self.log_alpha = ptu.zeros(1, requires_grad=True)
+            self.alpha_optimizer = optimizer_class(
+                [self.log_alpha],
+                lr=policy_lr,
+            )
+
+            self.expl_log_alpha = ptu.zeros(1, requires_grad=True)
+            self.expl_alpha_optimizer = optimizer_class(
+                [self.expl_log_alpha],
+                lr=policy_lr,
+            )
+
+
+        self.plotter = plotter
+        self.render_eval_paths = render_eval_paths
+
+        self.qf_criterion = nn.MSELoss()
+        self.vf_criterion = nn.MSELoss()
+
+        self.policy_optimizer = optimizer_class(
+            self.policy.parameters(),
+            lr=policy_lr,
+        )
+        self.qf1_optimizer = optimizer_class(
+            self.qf1.parameters(),
+            lr=qf_lr,
+        )
+        self.qf2_optimizer = optimizer_class(
+            self.qf2.parameters(),
+            lr=qf_lr,
+        )
+        self.expl_policy_optimizer = optimizer_class(
+            self.expl_policy.parameters(),
+            lr=policy_lr,
+        )
+        self.expl_qf1_optimizer = optimizer_class(
+            self.expl_qf1.parameters(),
+            lr=qf_lr,
+        )
+        self.expl_qf2_optimizer = optimizer_class(
+            self.expl_qf2.parameters(),
+            lr=qf_lr,
+        )
+
+        self.discount = discount
+        self.reward_scale = reward_scale
+        self.eval_statistics = OrderedDict()
+        self._n_train_steps_total = 0
+        self._need_to_update_eval_statistics = True
+
+    def train_from_torch(self, batch):
+        rewards = batch['rewards']
+        terminals = batch['terminals']
+        obs = batch['observations']
+        actions = batch['actions']
+        next_obs = batch['next_observations']
+
+        """
+        Policy and Alpha Loss
+        """
+        new_obs_actions, policy_mean, policy_log_std, log_pi, *_ = self.policy(
+            obs, reparameterize=True, return_log_prob=True,
+        )
+        if self.use_automatic_entropy_tuning:
+            alpha_loss = -(self.log_alpha * (log_pi + self.target_entropy).detach()).mean()
+            self.alpha_optimizer.zero_grad()
+            alpha_loss.backward()
+            self.alpha_optimizer.step()
+            alpha = self.log_alpha.exp()
+        else:
+            alpha_loss = 0
+            alpha = 1
+
+        q_new_actions = torch.min(
+            self.qf1(obs, new_obs_actions),
+            self.qf2(obs, new_obs_actions),
+        )
+        policy_loss = (alpha*log_pi - q_new_actions).mean()
+
+        """
+        QF Loss
+        """
+        q1_pred = self.qf1(obs, actions)
+        q2_pred = self.qf2(obs, actions)
+        # Make sure policy accounts for squashing functions like tanh correctly!
+        new_next_actions, _, _, new_log_pi, *_ = self.policy(
+            next_obs, reparameterize=True, return_log_prob=True,
+        )
+        target_q_values = torch.min(
+            self.target_qf1(next_obs, new_next_actions),
+            self.target_qf2(next_obs, new_next_actions),
+        ) - alpha * new_log_pi
+
+        q_target = self.reward_scale * rewards + (1. - terminals) * self.discount * target_q_values
+        qf1_loss = self.qf_criterion(q1_pred, q_target.detach())
+        qf2_loss = self.qf_criterion(q2_pred, q_target.detach())
+
+        bellman_error_pre = torch.abs(q1_pred.detach() - q_target.detach()) + torch.abs(q2_pred.detach() - q_target.detach())
+        #bellman_error_pre = 0.5 * (qf1_loss + qf2_loss)
+        """
+        Update networks
+        """
+        self.qf1_optimizer.zero_grad()
+        qf1_loss.backward()
+        self.qf1_optimizer.step()
+
+        self.qf2_optimizer.zero_grad()
+        qf2_loss.backward()
+        self.qf2_optimizer.step()
+
+        self.policy_optimizer.zero_grad()
+        policy_loss.backward()
+        self.policy_optimizer.step()
+
+        q1_pred_post = self.qf1(obs, actions)
+        q2_pred_post = self.qf2(obs, actions)
+
+        bellman_error_post = torch.abs(q1_pred_post.detach() - q_target.detach()) + torch.abs(q2_pred_post.detach() - q_target.detach())
+        #bellman_error_post = 0.5 * (self.qf_criterion(q1_pred_post, q_target.detach()) + self.qf_criterion(q2_pred_post, q_target.detach()))
+        delta_be = torch.abs(bellman_error_post - bellman_error_pre)
+        #bellman_error_post = 0.5*(torch.abs(q1_pred_post.detach() - q_target.detach()) + torch.abs(q2_pred.detach() - q_target.detach()))
+
+
+        new_obs_actions, policy_mean, policy_log_std, log_pi, *_ = self.expl_policy(
+            obs, reparameterize=True, return_log_prob=True,
+        )
+        if self.use_automatic_entropy_tuning:
+            expl_alpha_loss = -(self.expl_log_alpha * (log_pi + self.target_entropy).detach()).mean()
+            self.expl_alpha_optimizer.zero_grad()
+            expl_alpha_loss.backward()
+            self.expl_alpha_optimizer.step()
+            expl_alpha = self.expl_log_alpha.exp()
+        else:
+            alpha_loss = 0
+            alpha = 1
+
+        q_new_actions = torch.min(
+            self.expl_qf1(obs, new_obs_actions),
+            self.expl_qf2(obs, new_obs_actions),
+        )
+        expl_policy_loss = (expl_alpha*log_pi - q_new_actions).mean()
+
+        """
+        QF Loss
+        """
+        expl_q1_pred = self.expl_qf1(obs, actions)
+        expl_q2_pred = self.expl_qf2(obs, actions)
+        # Make sure policy accounts for squashing functions like tanh correctly!
+        new_next_actions, _, _, new_log_pi, *_ = self.expl_policy(
+            next_obs, reparameterize=True, return_log_prob=True,
+        )
+        target_q_values = torch.min(
+            self.expl_target_qf1(next_obs, new_next_actions),
+            self.expl_target_qf2(next_obs, new_next_actions),
+        ) - expl_alpha * new_log_pi
+
+        #try change in variance too?
+        expl_q_target = self.reward_scale * (delta_be) + (1. - terminals) * self.discount * target_q_values
+        expl_qf1_loss = self.qf_criterion(expl_q1_pred, expl_q_target.detach())
+        expl_qf2_loss = self.qf_criterion(expl_q2_pred, expl_q_target.detach())
+
+        """
+        Update networks
+        """
+        self.expl_qf1_optimizer.zero_grad()
+        expl_qf1_loss.backward()
+        self.expl_qf1_optimizer.step()
+
+        self.expl_qf2_optimizer.zero_grad()
+        expl_qf2_loss.backward()
+        self.expl_qf2_optimizer.step()
+
+        self.expl_policy_optimizer.zero_grad()
+        expl_policy_loss.backward()
+        self.expl_policy_optimizer.step()
+
+        """
+        Soft Updates
+        """
+        if self._n_train_steps_total % self.target_update_period == 0:
+            ptu.soft_update_from_to(
+                self.qf1, self.target_qf1, self.soft_target_tau
+            )
+            ptu.soft_update_from_to(
+                self.qf2, self.target_qf2, self.soft_target_tau
+            )
+            ptu.soft_update_from_to(
+                self.expl_qf1, self.expl_target_qf1, self.soft_target_tau
+            )
+            ptu.soft_update_from_to(
+                self.expl_qf2, self.expl_target_qf2, self.soft_target_tau
+            )
+
+
+        """
+        Save some statistics for eval
+        """
+        if self._need_to_update_eval_statistics:
+            self._need_to_update_eval_statistics = False
+            """
+            Eval should set this to None.
+            This way, these statistics are only computed for one batch.
+            """
+            policy_loss = (log_pi - q_new_actions).mean()
+
+            self.eval_statistics['QF1 Loss'] = np.mean(ptu.get_numpy(qf1_loss))
+            self.eval_statistics['QF2 Loss'] = np.mean(ptu.get_numpy(qf2_loss))
+            self.eval_statistics['Expl_QF1 Loss'] = np.mean(ptu.get_numpy(expl_qf1_loss))
+            self.eval_statistics['Expl_QF2 Loss'] = np.mean(ptu.get_numpy(expl_qf2_loss))
+            self.eval_statistics['Policy Loss'] = np.mean(ptu.get_numpy(
+                policy_loss
+            ))
+            self.eval_statistics['Expl_Policy Loss'] = np.mean(ptu.get_numpy(
+                expl_policy_loss
+            ))
+            self.eval_statistics.update(create_stats_ordered_dict(
+                'Q1 Predictions',
+                ptu.get_numpy(q1_pred),
+            ))
+            self.eval_statistics.update(create_stats_ordered_dict(
+                'Q2 Predictions',
+                ptu.get_numpy(q2_pred),
+            ))
+            self.eval_statistics.update(create_stats_ordered_dict(
+                'Q Targets',
+                ptu.get_numpy(q_target),
+            ))
+
+
+            self.eval_statistics.update(create_stats_ordered_dict(
+                'Expl Q1 Predictions',
+                ptu.get_numpy(expl_q1_pred),
+            ))
+            self.eval_statistics.update(create_stats_ordered_dict(
+                'Expl Q2 Predictions',
+                ptu.get_numpy(expl_q2_pred),
+            ))
+            self.eval_statistics.update(create_stats_ordered_dict(
+                'Expl Q Targets',
+                ptu.get_numpy(expl_q_target),
+            ))
+
+            self.eval_statistics.update(create_stats_ordered_dict(
+                'Delta BE Bonus',
+                ptu.get_numpy(delta_be),
+            ))
+
+        self._n_train_steps_total += 1
+
+    def get_diagnostics(self):
+        stats = super().get_diagnostics()
+        stats.update(self.eval_statistics)
+        return stats
+
+    def end_epoch(self, epoch):
+        self._need_to_update_eval_statistics = True
+
+    @property
+    def networks(self):
+        return [
+            self.policy,
+            self.qf1,
+            self.qf2,
+            self.target_qf1,
+            self.target_qf2,
+            self.expl_policy,
+            self.expl_qf1,
+            self.expl_qf2,
+            self.expl_target_qf1,
+            self.expl_target_qf2,
+        ]
+
+    def get_snapshot(self):
+        return dict(
+            policy=self.policy,
+            expl_policy=self.expl_policy,
+            qf1=self.qf1,
+            qf2=self.qf2,
+            target_qf1=self.qf1,
+            target_qf2=self.qf2,
+            expl_qf1=self.expl_qf1,
+            expl_qf2=self.expl_qf2,
+            expl_target_qf1=self.expl_target_qf1,
+            expl_target_qf2=self.expl_target_qf2,
+        )
diff --git a/railrl/torch/sac/sac_dist.py b/railrl/torch/sac/sac_dist.py
new file mode 100644
index 0000000..b756ffc
--- /dev/null
+++ b/railrl/torch/sac/sac_dist.py
@@ -0,0 +1,282 @@
+from collections import OrderedDict
+from torch.distributions import Normal
+import numpy as np
+import torch
+import torch.optim as optim
+from torch import nn as nn
+
+import railrl.torch.pytorch_util as ptu
+from railrl.misc.eval_util import create_stats_ordered_dict
+from railrl.torch.core import np_to_pytorch_batch
+from railrl.torch.torch_rl_algorithm import TorchTrainer
+
+
+class ZACTrainer(TorchTrainer):
+    def __init__(
+            self,
+            env,
+            policy,
+            qf1,
+            qf2,
+            target_qf1,
+            target_qf2,
+
+            discount=0.99,
+            reward_scale=1.0,
+
+            policy_lr=1e-3,
+            qf_lr=1e-3,
+            optimizer_class=optim.Adam,
+
+            soft_target_tau=1e-2,
+            target_update_period=1,
+            plotter=None,
+            render_eval_paths=False,
+
+            use_automatic_entropy_tuning=True,
+            target_entropy=None,
+    ):
+        super().__init__()
+        self.env = env
+        self.policy = policy
+        self.qf1 = qf1
+        self.qf2 = qf2
+        self.target_qf1 = target_qf1
+        self.target_qf2 = target_qf2
+        self.soft_target_tau = soft_target_tau
+        self.target_update_period = target_update_period
+
+        self.use_automatic_entropy_tuning = use_automatic_entropy_tuning
+        if self.use_automatic_entropy_tuning:
+            if target_entropy:
+                self.target_entropy = target_entropy
+            else:
+                self.target_entropy = -np.prod(self.env.action_space.shape).item()  # heuristic value from Tuomas
+            self.log_alpha = ptu.zeros(1, requires_grad=True)
+            self.alpha_optimizer = optimizer_class(
+                [self.log_alpha],
+                lr=policy_lr,
+            )
+
+        self.plotter = plotter
+        self.render_eval_paths = render_eval_paths
+
+        self.qf_criterion = nn.MSELoss()
+        self.vf_criterion = nn.MSELoss()
+
+        self.policy_optimizer = optimizer_class(
+            self.policy.parameters(),
+            lr=policy_lr,
+        )
+        self.qf1_optimizer = optimizer_class(
+            self.qf1.parameters(),
+            lr=qf_lr,
+        )
+        self.qf2_optimizer = optimizer_class(
+            self.qf2.parameters(),
+            lr=qf_lr,
+        )
+
+        self.discount = discount
+        self.reward_scale = reward_scale
+        self.eval_statistics = OrderedDict()
+        self._n_train_steps_total = 0
+        self._need_to_update_eval_statistics = True
+
+
+    # def train_eval_networks(self, batch):
+    #     rewards = batch['rewards']
+    #     terminals = batch['terminals']
+    #     obs = batch['observations']
+    #     actions = batch['actions']
+    #     next_obs = batch['next_observations']
+
+    # def train_explor_networks(self, batch):
+    #     rewards = batch['rewards']
+    #     terminals = batch['terminals']
+    #     obs = batch['observations']
+    #     actions = batch['actions']
+    #     next_obs = batch['next_observations']
+
+
+
+    def train_from_torch(self, batch):
+        rewards = batch['rewards']
+        terminals = batch['terminals']
+        obs = batch['observations']
+        actions = batch['actions']
+        next_obs = batch['next_observations']
+
+        """
+        Policy and Alpha Loss
+        """
+        new_obs_actions, policy_mean, policy_log_std, log_pi, *_ = self.policy(
+            obs, reparameterize=True, return_log_prob=True,
+        )
+        if self.use_automatic_entropy_tuning:
+            alpha_loss = -(self.log_alpha * (log_pi + self.target_entropy).detach()).mean()
+            self.alpha_optimizer.zero_grad()
+            alpha_loss.backward()
+            self.alpha_optimizer.step()
+            alpha = self.log_alpha.exp()
+        else:
+            alpha_loss = 0
+            alpha = 1
+
+        q_new_actions = torch.min(
+            self.qf1(obs, new_obs_actions)[:, 0].reshape(-1, 1),
+            self.qf2(obs, new_obs_actions)[:, 0].reshape(-1, 1),
+        )
+        policy_loss = (alpha*log_pi - q_new_actions).mean()
+
+        """
+        QF Loss
+        """
+        q1_pred = self.qf1(obs, actions)
+        q2_pred = self.qf2(obs, actions)
+        # Make sure policy accounts for squashing functions like tanh correctly!
+        new_next_actions, _, _, new_log_pi, *_ = self.policy(
+            next_obs, reparameterize=True, return_log_prob=True,
+        )
+
+        target_q_values = torch.min(
+            self.target_qf1(next_obs, new_next_actions)[:, 0].reshape(-1, 1),
+            self.target_qf2(next_obs, new_next_actions)[:, 0].reshape(-1, 1),
+        ) - alpha * new_log_pi
+
+
+        # target_q_values = 0.5 * (self.sample_value(self.target_qf1(next_obs, new_next_actions)) + \
+        #     self.sample_value(self.target_qf2(next_obs, new_next_actions))) - alpha * new_log_pi
+
+        uncertainty = (0.5 * (
+            self.target_qf1(next_obs, new_next_actions)[:, 1].reshape(-1, 1) +
+            self.target_qf2(next_obs, new_next_actions)[:, 1].reshape(-1, 1))).exp().pow(0.5)
+
+        q_target = self.reward_scale * rewards + (1. - terminals) * self.discount * (target_q_values + uncertainty)
+        #suprise = self.compute_gaussian_log_prob(q_target.detach(), q1_pred.detach()) + self.compute_gaussian_log_prob(q_target.detach(), q2_pred.detach())
+        #q_target -= 0.1 * suprise
+
+        qf1_loss = -1 * self.compute_gaussian_log_prob(q_target.detach(), q1_pred)
+        qf2_loss = -1 * self.compute_gaussian_log_prob(q_target.detach(), q2_pred)
+
+        """
+        Update networks
+        """
+        self.qf1_optimizer.zero_grad()
+        qf1_loss.backward()
+        self.qf1_optimizer.step()
+
+        self.qf2_optimizer.zero_grad()
+        qf2_loss.backward()
+        self.qf2_optimizer.step()
+
+        self.policy_optimizer.zero_grad()
+        policy_loss.backward()
+        self.policy_optimizer.step()
+
+        """
+        Soft Updates
+        """
+        if self._n_train_steps_total % self.target_update_period == 0:
+            ptu.soft_update_from_to(
+                self.qf1, self.target_qf1, self.soft_target_tau
+            )
+            ptu.soft_update_from_to(
+                self.qf2, self.target_qf2, self.soft_target_tau
+            )
+
+        """
+        Save some statistics for eval
+        """
+        if self._need_to_update_eval_statistics:
+            self._need_to_update_eval_statistics = False
+            """
+            Eval should set this to None.
+            This way, these statistics are only computed for one batch.
+            """
+            policy_loss = (log_pi - q_new_actions).mean()
+
+            self.eval_statistics['QF1 Loss'] = np.mean(ptu.get_numpy(qf1_loss))
+            self.eval_statistics['QF2 Loss'] = np.mean(ptu.get_numpy(qf2_loss))
+            self.eval_statistics['Policy Loss'] = np.mean(ptu.get_numpy(
+                policy_loss
+            ))
+            self.eval_statistics.update(create_stats_ordered_dict(
+                'Q1 Predictions',
+                ptu.get_numpy(q1_pred),
+            ))
+            self.eval_statistics.update(create_stats_ordered_dict(
+                'Q2 Predictions',
+                ptu.get_numpy(q2_pred),
+            ))
+            self.eval_statistics.update(create_stats_ordered_dict(
+                'Q Targets',
+                ptu.get_numpy(q_target),
+            ))
+            self.eval_statistics.update(create_stats_ordered_dict(
+                'Log Pis',
+                ptu.get_numpy(log_pi),
+            ))
+            self.eval_statistics.update(create_stats_ordered_dict(
+                'Policy mu',
+                ptu.get_numpy(policy_mean),
+            ))
+            self.eval_statistics.update(create_stats_ordered_dict(
+                'Policy log std',
+                ptu.get_numpy(policy_log_std),
+            ))
+            if self.use_automatic_entropy_tuning:
+                self.eval_statistics['Alpha'] = alpha.item()
+                self.eval_statistics['Alpha Loss'] = alpha_loss.item()
+        self._n_train_steps_total += 1
+
+
+    def kl_divergence(self, latent_distribution_params):
+        mu, logvar = latent_distribution_params
+        return - 0.5 * torch.sum(1 + logvar - mu.pow(2) - logvar.exp(), dim=1).mean()
+
+    def sample_value(self, dist):
+        mu, logvar = dist[:, 0].reshape(-1, 1), dist[:, 1].reshape(-1, 1)
+        stds = (0.5 * logvar).exp()
+        epsilon = ptu.randn(*mu.size())
+        sample = epsilon * stds + mu
+        return sample
+
+    def compute_gaussian_log_prob(self, x, dist):
+        var = dist[:, 1].reshape(-1, 1).exp()
+        mu = dist[:, 0].reshape(-1, 1)
+        dist = Normal(mu, var.pow(0.5))
+        log_probs = dist.log_prob(x)
+        vals = log_probs.sum(dim=1, keepdim=True)
+        return vals.mean()
+
+        # mu, var = dist[:, 0].reshape(-1, 1), dist[:, 1].reshape(-1, 1).exp()
+        # logprob = -(x - mu)**2/(2*var)
+        # return logprob.mean()
+
+    def get_diagnostics(self):
+        stats = super().get_diagnostics()
+        stats.update(self.eval_statistics)
+        return stats
+
+    def end_epoch(self, epoch):
+        self._need_to_update_eval_statistics = True
+
+    @property
+    def networks(self):
+        return [
+            self.policy,
+            self.qf1,
+            self.qf2,
+            self.target_qf1,
+            self.target_qf2,
+        ]
+
+    def get_snapshot(self):
+        return dict(
+            policy=self.policy,
+            qf1=self.qf1,
+            qf2=self.qf2,
+            target_qf1=self.qf1,
+            target_qf2=self.qf2,
+        )
\ No newline at end of file
diff --git a/railrl/torch/sac/sac_ensemble.py b/railrl/torch/sac/sac_ensemble.py
new file mode 100644
index 0000000..4c130f1
--- /dev/null
+++ b/railrl/torch/sac/sac_ensemble.py
@@ -0,0 +1,649 @@
+from collections import OrderedDict
+from torch.distributions import Normal
+import numpy as np
+import torch
+import torch.optim as optim
+from torch import nn as nn
+
+import railrl.torch.pytorch_util as ptu
+from railrl.misc.eval_util import create_stats_ordered_dict
+from railrl.torch.core import np_to_pytorch_batch
+from railrl.torch.torch_rl_algorithm import TorchTrainer
+
+# class SACTrainer(TorchTrainer):
+#     def __init__(
+#             self,
+#             env,
+#             eval_policy,
+#             expl_policy,
+#             averse_policy,
+#             eval_ensemble,
+#             expl_ensemble,
+#             averse_ensemble,
+
+#             discount=0.99,
+#             reward_scale=1.0,
+
+#             policy_lr=1e-3,
+#             qf_lr=1e-3,
+#             optimizer_class=optim.Adam,
+
+#             plotter=None,
+#             render_eval_paths=False,
+
+#             use_automatic_entropy_tuning=True,
+#             target_entropy=None,
+#     ):
+#         super().__init__()
+#         self.env = env
+#         self.eval_policy = eval_policy
+#         self.expl_policy = expl_policy
+#         self.averse_policy = averse_policy
+#         self.eval_ensemble = eval_ensemble
+#         self.expl_ensemble = expl_ensemble
+#         self.averse_ensemble = averse_ensemble
+
+#         self.use_automatic_entropy_tuning = use_automatic_entropy_tuning
+#         if self.use_automatic_entropy_tuning:
+#             if target_entropy:
+#                 self.target_entropy = target_entropy
+#             else:
+#                 self.target_entropy = -np.prod(self.env.action_space.shape).item()  # heuristic value from Tuomas
+#             self.eval_log_alpha = ptu.zeros(1, requires_grad=True)
+#             self.expl_log_alpha = ptu.zeros(1, requires_grad=True)
+#             self.averse_log_alpha = ptu.zeros(1, requires_grad=True)
+#             self.eval_alpha_optimizer = optimizer_class(
+#                 [self.eval_log_alpha],
+#                 lr=policy_lr,
+#             )
+#             self.expl_alpha_optimizer = optimizer_class(
+#                 [self.expl_log_alpha],
+#                 lr=policy_lr,
+#             )
+#             self.averse_alpha_optimizer = optimizer_class(
+#                 [self.averse_log_alpha],
+#                 lr=policy_lr,
+#             )
+#         self.plotter = plotter
+#         self.render_eval_paths = render_eval_paths
+
+#         self.qf_criterion = nn.MSELoss()
+
+#         self.eval_policy_optimizer = optimizer_class(
+#             self.eval_policy.parameters(),
+#             lr=policy_lr,
+#         )
+#         self.expl_policy_optimizer = optimizer_class(
+#             self.expl_policy.parameters(),
+#             lr=policy_lr,
+#         )
+
+#         self.averse_policy_optimizer = optimizer_class(
+#             self.averse_policy.parameters(),
+#             lr=policy_lr,
+#         )
+#         self.eval_ensemble_optimizer = [optimizer_class(q.parameters(), lr=qf_lr,) \
+#             for q in self.eval_ensemble]
+#         self.expl_ensemble_optimizer = [optimizer_class(q.parameters(), lr=qf_lr,) \
+#             for q in self.expl_ensemble]
+#         self.averse_ensemble_optimizer = [optimizer_class(q.parameters(), lr=qf_lr,) \
+#             for q in self.averse_ensemble]
+
+#         self.discount = discount
+#         self.reward_scale = reward_scale
+#         self.eval_statistics = OrderedDict()
+#         self._n_train_steps_total = 0
+#         self._need_to_update_eval_statistics = True
+
+#     def train_from_torch(self, batch):
+#         rewards = batch['rewards']
+#         terminals = batch['terminals']
+#         obs = batch['observations']
+#         actions = batch['actions']
+#         next_obs = batch['next_observations']
+
+#         """
+#         Policy and Alpha Loss
+#         """
+#         new_obs_actions, policy_mean, policy_log_std, log_pi, *_ = self.eval_policy(
+#             obs, reparameterize=True, return_log_prob=True,
+#         )
+#         if self.use_automatic_entropy_tuning:
+#             eval_alpha_loss = -(self.eval_log_alpha * (log_pi + self.target_entropy).detach()).mean()
+#             self.eval_alpha_optimizer.zero_grad()
+#             eval_alpha_loss.backward()
+#             self.eval_alpha_optimizer.step()
+#             eval_alpha = self.eval_log_alpha.exp()
+
+#         eval_q_new_actions = torch.cat([q(obs, new_obs_actions) for q in self.eval_ensemble], dim=1)
+
+
+#         #x = torch.min(eval_q_new_actions,dim=1)
+#         #import ipdb; ipdb.set_trace()
+#         eval_policy_loss = (eval_alpha*log_pi - torch.mean(eval_q_new_actions, dim=1).reshape(-1, 1)).mean()
+
+#             #TODO: CHANGE BACK TO MEAN
+#         """
+#         QF Loss
+#         """
+#         q_index = self._n_train_steps_total % len(self.eval_ensemble_optimizer)
+#         q_train, q_optimizer = self.eval_ensemble[q_index], self.eval_ensemble_optimizer[q_index]
+#         eval_q_pred = q_train(obs, actions)
+
+#         uncertainty = torch.std(torch.cat([q(obs, actions) for q in self.eval_ensemble], dim=1), dim=1).reshape(-1, 1).detach()
+
+        
+#         # Make sure policy accounts for squashing functions like tanh correctly!
+#         new_next_actions, _, _, new_log_pi, *_ = self.eval_policy(
+#             next_obs, reparameterize=True, return_log_prob=True,
+#         )
+        
+#         eval_predictions = torch.cat([self.eval_ensemble[i](next_obs, new_next_actions) \
+#             for i in range(len(self.eval_ensemble)) if i != q_index], dim=1)
+        
+#         target_q_values = torch.mean(eval_predictions, dim=1).reshape(-1, 1) \
+#         - eval_alpha * new_log_pi
+
+#         #q_variance = torch.std(eval_predictions, dim=1).reshape(-1, 1).detach()
+
+#         q_target = self.reward_scale * rewards + (1. - terminals) * self.discount * target_q_values
+#         eval_loss = self.qf_criterion(eval_q_pred, q_target.detach())
+
+#         """
+#         Update networks
+#         """
+#         q_optimizer.zero_grad()
+#         eval_loss.backward()
+#         q_optimizer.step()
+
+#         self.eval_policy_optimizer.zero_grad()
+#         eval_policy_loss.backward()
+#         self.eval_policy_optimizer.step()
+
+#         #var_post = torch.std(torch.cat([q(obs, actions) for q in self.eval_ensemble], dim=1), dim=1).reshape(-1, 1)
+#         #delta_var = torch.abs(var_post - var_pre)
+
+#         """
+#         Train Exploration Networks
+#         """
+
+
+#         """
+#         Policy and Alpha Loss
+#         """
+#         new_obs_actions, policy_mean, policy_log_std, log_pi, *_ = self.expl_policy(
+#             obs, reparameterize=True, return_log_prob=True,
+#         )
+#         if self.use_automatic_entropy_tuning:
+#             expl_alpha_loss = -(self.expl_log_alpha * (log_pi + self.target_entropy).detach()).mean()
+#             self.expl_alpha_optimizer.zero_grad()
+#             expl_alpha_loss.backward()
+#             self.expl_alpha_optimizer.step()
+#             expl_alpha = self.expl_log_alpha.exp()
+
+
+#         expl_q_new_actions = torch.cat([q(obs, new_obs_actions) for q in self.expl_ensemble], dim=1)
+#         expl_policy_loss = (expl_alpha*log_pi - torch.mean(expl_q_new_actions,dim=1).reshape(-1, 1)).mean()
+
+#         """
+#         QF Loss
+#         """
+#         q_train, q_optimizer = self.expl_ensemble[q_index], self.expl_ensemble_optimizer[q_index]
+#         expl_q_pred = q_train(obs, actions)
+        
+#         # Make sure policy accounts for squashing functions like tanh correctly!
+#         new_next_actions, _, _, new_log_pi, *_ = self.expl_policy(
+#             next_obs, reparameterize=True, return_log_prob=True,
+#         )
+        
+#         expl_predictions = torch.cat([self.expl_ensemble[i](next_obs, new_next_actions) \
+#             for i in range(len(self.expl_ensemble)) if i != q_index], dim=1)
+        
+#         target_q_values = torch.mean(expl_predictions, dim=1).reshape(-1, 1) \
+#         - expl_alpha * new_log_pi
+
+#         q_target = self.reward_scale * (rewards + uncertainty) + (1. - terminals) * self.discount * (target_q_values)
+#         expl_loss = self.qf_criterion(expl_q_pred, q_target.detach())
+
+#         """
+#         Update networks
+#         """
+#         q_optimizer.zero_grad()
+#         expl_loss.backward()
+#         q_optimizer.step()
+
+#         self.expl_policy_optimizer.zero_grad()
+#         expl_policy_loss.backward()
+#         self.expl_policy_optimizer.step()
+
+#         """
+#         Train Risk Averse Networks
+#         """
+
+
+#         """
+#         Policy and Alpha Loss
+#         """
+#         new_obs_actions, policy_mean, policy_log_std, log_pi, *_ = self.averse_policy(
+#             obs, reparameterize=True, return_log_prob=True,
+#         )
+#         if self.use_automatic_entropy_tuning:
+#             averse_alpha_loss = -(self.averse_log_alpha * (log_pi + self.target_entropy).detach()).mean()
+#             self.averse_alpha_optimizer.zero_grad()
+#             averse_alpha_loss.backward()
+#             self.averse_alpha_optimizer.step()
+#             averse_alpha = self.averse_log_alpha.exp()
+
+
+#         averse_q_new_actions = torch.cat([q(obs, new_obs_actions) for q in self.averse_ensemble], dim=1)
+#         averse_policy_loss = (averse_alpha*log_pi - torch.mean(averse_q_new_actions,dim=1).reshape(-1, 1)).mean()
+
+#         """
+#         QF Loss
+#         """
+#         q_train, q_optimizer = self.averse_ensemble[q_index], self.averse_ensemble_optimizer[q_index]
+#         averse_q_pred = q_train(obs, actions)
+        
+#         # Make sure policy accounts for squashing functions like tanh correctly!
+#         new_next_actions, _, _, new_log_pi, *_ = self.averse_policy(
+#             next_obs, reparameterize=True, return_log_prob=True,
+#         )
+        
+#         averse_predictions = torch.cat([self.averse_ensemble[i](next_obs, new_next_actions) \
+#             for i in range(len(self.averse_ensemble)) if i != q_index], dim=1)
+        
+#         target_q_values = torch.mean(averse_predictions, dim=1).reshape(-1, 1) \
+#         - averse_alpha * new_log_pi
+
+#         q_target = self.reward_scale * (rewards - uncertainty) + (1. - terminals) * self.discount * (target_q_values)
+#         averse_loss = self.qf_criterion(averse_q_pred, q_target.detach())
+
+#         """
+#         Update networks
+#         """
+#         q_optimizer.zero_grad()
+#         averse_loss.backward()
+#         q_optimizer.step()
+
+#         self.averse_policy_optimizer.zero_grad()
+#         averse_policy_loss.backward()
+#         self.averse_policy_optimizer.step()
+
+#         """
+#         Save some statistics for eval
+#         """
+#         if self._need_to_update_eval_statistics:
+#             self._need_to_update_eval_statistics = False
+#             """
+#             Eval should set this to None.
+#             This way, these statistics are only computed for one batch.
+#             """
+#             eval_policy_loss = (eval_alpha*log_pi - torch.mean(eval_q_new_actions,dim=1).reshape(-1, 1)).mean()
+#             expl_policy_loss = (expl_alpha*log_pi - torch.mean(expl_q_new_actions,dim=1).reshape(-1, 1)).mean()
+#             averse_policy_loss = (averse_alpha*log_pi - torch.mean(averse_q_new_actions,dim=1).reshape(-1, 1)).mean()
+#             self.eval_statistics['Eval Q_i Loss'.format(q_index)] = np.mean(ptu.get_numpy(eval_loss))
+#             self.eval_statistics['Expl Q_i Loss'.format(q_index)] = np.mean(ptu.get_numpy(expl_loss))
+#             self.eval_statistics['Averse Q_i Loss'.format(q_index)] = np.mean(ptu.get_numpy(averse_loss))
+#             self.eval_statistics['Eval Policy Loss'] = np.mean(ptu.get_numpy(
+#                 eval_policy_loss
+#             ))
+#             self.eval_statistics['Expl Policy Loss'] = np.mean(ptu.get_numpy(
+#                 expl_policy_loss
+#             ))
+#             self.eval_statistics['Averse Policy Loss'] = np.mean(ptu.get_numpy(
+#                 averse_policy_loss
+#             ))
+#             self.eval_statistics.update(create_stats_ordered_dict(
+#                 'Eval Ensemble Mean',
+#                 np.mean(ptu.get_numpy(torch.mean(eval_q_new_actions,dim=1))),
+#             ))
+#             self.eval_statistics.update(create_stats_ordered_dict(
+#                 'Eval Ensemble Std',
+#                 np.mean(ptu.get_numpy(torch.std(eval_q_new_actions,dim=1))),
+#             ))
+
+#             self.eval_statistics.update(create_stats_ordered_dict(
+#                 'Expl Ensemble Mean',
+#                 np.mean(ptu.get_numpy(torch.mean(expl_q_new_actions,dim=1))),
+#             ))
+#             self.eval_statistics.update(create_stats_ordered_dict(
+#                 'Expl Ensemble Std',
+#                 np.mean(ptu.get_numpy(torch.std(expl_q_new_actions,dim=1))),
+#             ))
+
+#             self.eval_statistics.update(create_stats_ordered_dict(
+#                 'Averse Ensemble Mean',
+#                 np.mean(ptu.get_numpy(torch.mean(averse_q_new_actions,dim=1))),
+#             ))
+#             self.eval_statistics.update(create_stats_ordered_dict(
+#                 'Averse Ensemble Std',
+#                 np.mean(ptu.get_numpy(torch.std(averse_q_new_actions,dim=1))),
+#             ))
+
+#             # self.eval_statistics.update(create_stats_ordered_dict(
+#             #     'Q2 Predictions',
+#             #     ptu.get_numpy(q2_pred),
+#             # ))
+#             # self.eval_statistics.update(create_stats_ordered_dict(
+#             #     'Q Targets',
+#             #     ptu.get_numpy(q_target),
+#             # ))
+#             # self.eval_statistics.update(create_stats_ordered_dict(
+#             #     'Log Pis',
+#             #     ptu.get_numpy(log_pi),
+#             # ))
+#             # self.eval_statistics.update(create_stats_ordered_dict(
+#             #     'Policy mu',
+#             #     ptu.get_numpy(policy_mean),
+#             # ))
+#             # self.eval_statistics.update(create_stats_ordered_dict(
+#             #     'Policy log std',
+#             #     ptu.get_numpy(policy_log_std),
+#             # ))
+#             # if self.use_automatic_entropy_tuning:
+#             #     self.eval_statistics['Alpha'] = alpha.item()
+#             #     self.eval_statistics['Alpha Loss'] = alpha_loss.item()
+#         self._n_train_steps_total += 1
+
+#     def get_diagnostics(self):
+#         stats = super().get_diagnostics()
+#         stats.update(self.eval_statistics)
+#         return stats
+
+#     def end_epoch(self, epoch):
+#         self._need_to_update_eval_statistics = True
+
+#     @property
+#     def networks(self):
+#         return [
+#             self.eval_policy,
+#             self.expl_policy,
+#             self.averse_policy] + self.eval_ensemble + self.expl_ensemble + self.averse_ensemble
+
+#     def get_snapshot(self):
+#         return dict(
+#             eval_policy=self.eval_policy,
+#             expl_policy=self.expl_policy,
+#             averse_policy=self.averse_policy,
+#             eval_ensemble=self.eval_ensemble,
+#             expl_ensemble=self.expl_ensemble,
+#             averse_ensemble=self.averse_ensemble
+#         )
+
+
+
+class SACTrainer(TorchTrainer):
+    def __init__(
+            self,
+            env,
+            eval_policy,
+            expl_policy,
+            eval_ensemble,
+            expl_ensemble,
+
+            discount=0.99,
+            reward_scale=1.0,
+
+            policy_lr=1e-3,
+            qf_lr=1e-3,
+            optimizer_class=optim.Adam,
+
+            plotter=None,
+            render_eval_paths=False,
+
+            use_automatic_entropy_tuning=True,
+            target_entropy=None,
+    ):
+        super().__init__()
+        self.env = env
+        self.eval_policy = eval_policy
+        self.expl_policy = expl_policy
+        self.eval_ensemble = eval_ensemble
+        self.expl_ensemble = expl_ensemble
+
+        self.use_automatic_entropy_tuning = use_automatic_entropy_tuning
+        if self.use_automatic_entropy_tuning:
+            if target_entropy:
+                self.target_entropy = target_entropy
+            else:
+                self.target_entropy = -np.prod(self.env.action_space.shape).item()  # heuristic value from Tuomas
+            self.eval_log_alpha = ptu.zeros(1, requires_grad=True)
+            self.expl_log_alpha = ptu.zeros(1, requires_grad=True)
+            self.eval_alpha_optimizer = optimizer_class(
+                [self.eval_log_alpha],
+                lr=policy_lr,
+            )
+            self.expl_alpha_optimizer = optimizer_class(
+                [self.expl_log_alpha],
+                lr=policy_lr,
+            )
+        self.plotter = plotter
+        self.render_eval_paths = render_eval_paths
+
+        self.qf_criterion = nn.MSELoss()
+
+        self.eval_policy_optimizer = optimizer_class(
+            self.eval_policy.parameters(),
+            lr=policy_lr,
+        )
+        self.expl_policy_optimizer = optimizer_class(
+            self.expl_policy.parameters(),
+            lr=policy_lr,
+        )
+        self.eval_ensemble_optimizer = [optimizer_class(q.parameters(), lr=qf_lr,) \
+            for q in self.eval_ensemble]
+        self.expl_ensemble_optimizer = [optimizer_class(q.parameters(), lr=qf_lr,) \
+            for q in self.expl_ensemble]
+
+        self.discount = discount
+        self.reward_scale = reward_scale
+        self.eval_statistics = OrderedDict()
+        self._n_train_steps_total = 0
+        self._need_to_update_eval_statistics = True
+
+    def train_from_torch(self, batch):
+        rewards = batch['rewards']
+        terminals = batch['terminals']
+        obs = batch['observations']
+        actions = batch['actions']
+        next_obs = batch['next_observations']
+
+        """
+        Policy and Alpha Loss
+        """
+        new_obs_actions, policy_mean, policy_log_std, log_pi, *_ = self.eval_policy(
+            obs, reparameterize=True, return_log_prob=True,
+        )
+        if self.use_automatic_entropy_tuning:
+            eval_alpha_loss = -(self.eval_log_alpha * (log_pi + self.target_entropy).detach()).mean()
+            self.eval_alpha_optimizer.zero_grad()
+            eval_alpha_loss.backward()
+            self.eval_alpha_optimizer.step()
+            eval_alpha = self.eval_log_alpha.exp()
+
+        eval_q_new_actions = torch.cat([q(obs, new_obs_actions) for q in self.eval_ensemble], dim=1)
+
+
+        #x = torch.min(eval_q_new_actions,dim=1)
+        #import ipdb; ipdb.set_trace()
+        eval_policy_loss = (eval_alpha*log_pi - torch.mean(eval_q_new_actions, dim=1).reshape(-1, 1)).mean()
+
+            #TODO: CHANGE BACK TO MEAN
+        """
+        QF Loss
+        """
+        q_index = self._n_train_steps_total % len(self.eval_ensemble_optimizer)
+        q_train, q_optimizer = self.eval_ensemble[q_index], self.eval_ensemble_optimizer[q_index]
+        eval_q_pred = q_train(obs, actions)
+
+        var_pre = torch.std(torch.cat([q(obs, actions) for q in self.eval_ensemble], dim=1), dim=1).reshape(-1, 1).detach()
+
+        
+        # Make sure policy accounts for squashing functions like tanh correctly!
+        new_next_actions, _, _, new_log_pi, *_ = self.eval_policy(
+            next_obs, reparameterize=True, return_log_prob=True,
+        )
+        
+        eval_predictions = torch.cat([self.eval_ensemble[i](next_obs, new_next_actions) \
+            for i in range(len(self.eval_ensemble)) if i != q_index], dim=1)
+        
+        target_q_values = torch.mean(eval_predictions, dim=1).reshape(-1, 1) \
+        - eval_alpha * new_log_pi
+
+        q_variance = torch.std(eval_predictions, dim=1).reshape(-1, 1)
+
+        q_target = self.reward_scale * rewards + (1. - terminals) * self.discount * target_q_values
+        eval_loss = self.qf_criterion(eval_q_pred, q_target.detach())
+
+        """
+        Update networks
+        """
+        q_optimizer.zero_grad()
+        eval_loss.backward()
+        q_optimizer.step()
+
+        self.eval_policy_optimizer.zero_grad()
+        eval_policy_loss.backward()
+        self.eval_policy_optimizer.step()
+
+        var_post = torch.std(torch.cat([q(obs, actions) for q in self.eval_ensemble], dim=1), dim=1).reshape(-1, 1)
+        delta_var = torch.abs(var_post - var_pre)
+
+        """
+        Train Exploration Networks
+        """
+
+
+        """
+        Policy and Alpha Loss
+        """
+        new_obs_actions, policy_mean, policy_log_std, log_pi, *_ = self.expl_policy(
+            obs, reparameterize=True, return_log_prob=True,
+        )
+        if self.use_automatic_entropy_tuning:
+            expl_alpha_loss = -(self.expl_log_alpha * (log_pi + self.target_entropy).detach()).mean()
+            self.expl_alpha_optimizer.zero_grad()
+            expl_alpha_loss.backward()
+            self.expl_alpha_optimizer.step()
+            expl_alpha = self.expl_log_alpha.exp()
+
+
+        expl_q_new_actions = torch.cat([q(obs, new_obs_actions) for q in self.expl_ensemble], dim=1)
+        expl_policy_loss = (expl_alpha*log_pi - torch.mean(expl_q_new_actions,dim=1).reshape(-1, 1)).mean()
+
+        """
+        QF Loss
+        """
+        q_train, q_optimizer = self.expl_ensemble[q_index], self.expl_ensemble_optimizer[q_index]
+        expl_q_pred = q_train(obs, actions)
+        
+        # Make sure policy accounts for squashing functions like tanh correctly!
+        new_next_actions, _, _, new_log_pi, *_ = self.expl_policy(
+            next_obs, reparameterize=True, return_log_prob=True,
+        )
+        
+        expl_predictions = torch.cat([self.expl_ensemble[i](next_obs, new_next_actions) \
+            for i in range(len(self.expl_ensemble)) if i != q_index], dim=1)
+        
+        target_q_values = torch.mean(expl_predictions, dim=1).reshape(-1, 1) \
+        - expl_alpha * new_log_pi
+
+        q_target = self.reward_scale * (rewards + var_pre) + (1. - terminals) * self.discount * (target_q_values)
+        expl_loss = self.qf_criterion(expl_q_pred, q_target.detach())
+
+        """
+        Update networks
+        """
+        q_optimizer.zero_grad()
+        expl_loss.backward()
+        q_optimizer.step()
+
+        self.expl_policy_optimizer.zero_grad()
+        expl_policy_loss.backward()
+        self.expl_policy_optimizer.step()
+
+        """
+        Save some statistics for eval
+        """
+        if self._need_to_update_eval_statistics:
+            self._need_to_update_eval_statistics = False
+            """
+            Eval should set this to None.
+            This way, these statistics are only computed for one batch.
+            """
+            eval_policy_loss = (eval_alpha*log_pi - torch.mean(eval_q_new_actions,dim=1).reshape(-1, 1)).mean()
+            expl_policy_loss = (expl_alpha*log_pi - torch.mean(expl_q_new_actions,dim=1).reshape(-1, 1)).mean()
+            self.eval_statistics['Eval Q_i Loss'.format(q_index)] = np.mean(ptu.get_numpy(eval_loss))
+            self.eval_statistics['Expl Q_i Loss'.format(q_index)] = np.mean(ptu.get_numpy(expl_loss))
+            #self.eval_statistics['QF2 Loss'] = np.mean(ptu.get_numpy(qf2_loss))
+            self.eval_statistics['Eval Policy Loss'] = np.mean(ptu.get_numpy(
+                eval_policy_loss
+            ))
+            self.eval_statistics['Expl Policy Loss'] = np.mean(ptu.get_numpy(
+                expl_policy_loss
+            ))
+
+            self.eval_statistics.update(create_stats_ordered_dict(
+                'Eval Ensemble Mean',
+                np.mean(ptu.get_numpy(torch.mean(eval_q_new_actions,dim=1))),
+            ))
+            self.eval_statistics.update(create_stats_ordered_dict(
+                'Eval Ensemble Std',
+                np.mean(ptu.get_numpy(torch.std(eval_q_new_actions,dim=1))),
+            ))
+
+            self.eval_statistics.update(create_stats_ordered_dict(
+                'Expl Ensemble Mean',
+                np.mean(ptu.get_numpy(torch.mean(expl_q_new_actions,dim=1))),
+            ))
+            self.eval_statistics.update(create_stats_ordered_dict(
+                'Expl Ensemble Std',
+                np.mean(ptu.get_numpy(torch.std(expl_q_new_actions,dim=1))),
+            ))
+
+            # self.eval_statistics.update(create_stats_ordered_dict(
+            #     'Q2 Predictions',
+            #     ptu.get_numpy(q2_pred),
+            # ))
+            # self.eval_statistics.update(create_stats_ordered_dict(
+            #     'Q Targets',
+            #     ptu.get_numpy(q_target),
+            # ))
+            # self.eval_statistics.update(create_stats_ordered_dict(
+            #     'Log Pis',
+            #     ptu.get_numpy(log_pi),
+            # ))
+            # self.eval_statistics.update(create_stats_ordered_dict(
+            #     'Policy mu',
+            #     ptu.get_numpy(policy_mean),
+            # ))
+            # self.eval_statistics.update(create_stats_ordered_dict(
+            #     'Policy log std',
+            #     ptu.get_numpy(policy_log_std),
+            # ))
+            # if self.use_automatic_entropy_tuning:
+            #     self.eval_statistics['Alpha'] = alpha.item()
+            #     self.eval_statistics['Alpha Loss'] = alpha_loss.item()
+        self._n_train_steps_total += 1
+
+    def get_diagnostics(self):
+        stats = super().get_diagnostics()
+        stats.update(self.eval_statistics)
+        return stats
+
+    def end_epoch(self, epoch):
+        self._need_to_update_eval_statistics = True
+
+    @property
+    def networks(self):
+        return [
+            self.eval_policy,
+            self.expl_policy] + self.eval_ensemble + self.expl_ensemble
+
+    def get_snapshot(self):
+        return dict(
+            eval_policy=self.eval_policy,
+            expl_policy=self.expl_policy,
+            eval_ensemble=self.eval_ensemble,
+            expl_ensemble=self.expl_ensemble,
+        )
diff --git a/railrl/torch/sac/sap.py b/railrl/torch/sac/sap.py
new file mode 100644
index 0000000..ae81099
--- /dev/null
+++ b/railrl/torch/sac/sap.py
@@ -0,0 +1,673 @@
+from collections import OrderedDict
+
+import numpy as np
+import torch
+import torch.optim as optim
+from torch import nn as nn
+from railrl.torch.data_management.normalizer import TorchNormalizer
+import railrl.torch.pytorch_util as ptu
+from railrl.misc.eval_util import create_stats_ordered_dict
+from railrl.torch.core import np_to_pytorch_batch
+from railrl.torch.torch_rl_algorithm import TorchTrainer
+
+
+class SAPTrainer(TorchTrainer):
+    def __init__(
+            self,
+            env,
+            actor_policy,
+            planner_policy,
+            model,
+            qf1,
+            qf2,
+            target_qf1,
+            target_qf2,
+
+            state_dim,
+            act_dim,
+
+            discount=0.99,
+            reward_scale=1.0,
+
+            policy_lr=1e-3,
+            model_lr=1e-3,
+            qf_lr=1e-3,
+            optimizer_class=optim.Adam,
+
+            soft_target_tau=1e-2,
+            target_update_period=1,
+            plotter=None,
+            render_eval_paths=False,
+
+            use_automatic_entropy_tuning=True,
+            target_entropy=None,
+    ):
+        super().__init__()
+        self.env = env
+        self.actor_policy = actor_policy
+        self.planner_policy = planner_policy
+        self.model = model
+        self.qf1 = qf1
+        self.qf2 = qf2
+        self.target_qf1 = target_qf1
+        self.target_qf2 = target_qf2
+        self.soft_target_tau = soft_target_tau
+        self.target_update_period = target_update_period
+        self.state_dim = state_dim
+        self.act_dim = act_dim
+        self.k = 5
+
+        self.use_automatic_entropy_tuning = use_automatic_entropy_tuning
+        if self.use_automatic_entropy_tuning:
+            if target_entropy:
+                self.target_entropy = target_entropy
+            else:
+                self.actor_target_entropy = -np.prod(act_dim).item()  # heuristic value from Tuomas
+                self.planner_target_entropy = -np.prod(state_dim).item()
+            self.log_actor_alpha = ptu.zeros(1, requires_grad=True)
+            self.actor_alpha_optimizer = optimizer_class(
+                [self.log_actor_alpha],
+                lr=policy_lr,
+            )
+            self.log_planner_alpha = ptu.zeros(1, requires_grad=True)
+            self.planner_alpha_optimizer = optimizer_class(
+                [self.log_planner_alpha],
+                lr=policy_lr,
+            )
+
+        self.plotter = plotter
+        self.render_eval_paths = render_eval_paths
+
+        self.prediction_criterion = nn.MSELoss()
+
+        self.policy_optimizer = optimizer_class(
+            list(self.actor_policy.parameters()) + list(self.planner_policy.parameters()),
+            lr=policy_lr,
+        )
+
+        self.actor_optimizer = optimizer_class(
+            self.actor_policy.parameters(),
+            lr=policy_lr,
+        )
+
+        self.planner_optimizer = optimizer_class(
+            self.planner_policy.parameters(),
+            lr=policy_lr,
+        )
+
+        self.model_optimizer = optimizer_class(
+            self.model.parameters(),
+            lr=model_lr,
+        )
+
+        self.qf1_optimizer = optimizer_class(
+            self.qf1.parameters(),
+            lr=qf_lr,
+        )
+        self.qf2_optimizer = optimizer_class(
+            self.qf2.parameters(),
+            lr=qf_lr,
+        )
+
+        self.discount = discount
+        self.reward_scale = reward_scale
+        self.eval_statistics = OrderedDict()
+        self._n_train_steps_total = 0
+        self._need_to_update_eval_statistics = True
+
+        self.obs_normalizer = TorchNormalizer(self.state_dim)
+        self.action_normalizer = TorchNormalizer(self.act_dim)
+        self.delta_normalizer = TorchNormalizer(self.state_dim)
+
+    def predict(self, observations, actions):
+        norm_obs = self.obs_normalizer.normalize(observations)
+        norm_acts = self.action_normalizer.normalize(actions)
+        delta_predicted_normalized = self.model(norm_obs, norm_acts)
+        delta_predicted = self.delta_normalizer.denormalize(delta_predicted_normalized)
+        next_obs_predicted = observations + delta_predicted
+        return next_obs_predicted
+
+    def train_model(self, observations, actions, next_observations):        
+        #Update Normalizer
+        self.obs_normalizer.update(ptu.get_numpy(observations))
+        self.action_normalizer.update(ptu.get_numpy(actions))
+        self.delta_normalizer.update(ptu.get_numpy(next_observations) - ptu.get_numpy(observations))
+
+        #Predict next state
+        norm_obs = self.obs_normalizer.normalize(observations)
+        norm_acts = self.action_normalizer.normalize(actions)
+        delta_predicted = self.model(norm_obs, norm_acts)
+
+        #Calcualte loss
+        obs_delta = self.delta_normalizer.normalize(next_observations - observations)
+        model_loss = self.prediction_criterion(delta_predicted, obs_delta)
+        np_model_loss = ptu.get_numpy(model_loss)
+
+        #Train Model
+        self.model_optimizer.zero_grad()
+        model_loss.backward()
+        self.model_optimizer.step()
+
+        """
+        Update Eval Statistics
+        """
+        if self._need_to_update_eval_statistics:
+            self.eval_statistics['Model Loss'] = np_model_loss
+
+    def train_policies(self, obs, actions, next_obs):
+        #Extract State Information
+        curr_state = obs[:, :self.state_dim]
+        goal_state = obs[:, self.state_dim:]
+
+        #Evaluate Policies
+        subgoal, planner_policy_mean, policy_log_std, planner_log_pi, *_ = self.planner_policy(
+            obs, reparameterize=True, return_log_prob=True)
+        abstract_obs = torch.cat([curr_state, subgoal], dim=1)
+        actor_action, actor_policy_mean, policy_log_std, actor_log_pi, *_ = self.actor_policy(
+                abstract_obs, reparameterize=True, return_log_prob=True)
+
+        #Update Alpha Value
+        if self.use_automatic_entropy_tuning:
+            planner_alpha_loss = -(self.log_planner_alpha * (planner_log_pi + self.planner_target_entropy).detach()).mean()
+            actor_alpha_loss = -(self.log_actor_alpha * (actor_log_pi + self.actor_target_entropy).detach()).mean()
+            self.planner_alpha_optimizer.zero_grad()
+            self.actor_alpha_optimizer.zero_grad()
+            planner_alpha_loss.backward()
+            actor_alpha_loss.backward()
+            self.planner_alpha_optimizer.step()
+            self.actor_alpha_optimizer.step()
+            planner_alpha = self.log_planner_alpha.exp()
+            actor_alpha = self.log_actor_alpha.exp()
+        else:
+            planner_alpha_loss = 0
+            planner_alpha = 1
+            actor_alpha_loss = 0
+            actor_alpha = 1
+
+
+        #Calculate Policy Loss
+        policy_loss = planner_alpha*planner_log_pi
+
+        for i in range(self.k + 1):
+            abstract_obs = torch.cat([curr_state, subgoal], dim=1)
+            actor_action, policy_mean, policy_log_std, actor_log_pi, *_ = self.actor_policy(
+                abstract_obs, reparameterize=True, return_log_prob=True)
+            entropy = actor_alpha*actor_log_pi
+
+            if i == self.k:
+                curr_obs = torch.cat([curr_state, goal_state], dim=1)
+                q_to_go = torch.min(
+                    self.qf1(curr_obs, actor_action),
+                    self.qf2(curr_obs, actor_action))
+                policy_loss = policy_loss - self.discount**i * (q_to_go - entropy)
+            else:
+                curr_state = self.predict(curr_state, actor_action)
+                rew = - torch.norm(curr_state - goal_state, dim=1, keepdim=True)
+                # ptu.from_numpy(self.env.compute_rewards(actor_action,
+                #     {'state_achieved_goal': ptu.get_numpy(curr_state),
+                #     'state_desired_goal': ptu.get_numpy(goal_state)}))
+                #rew = distance to goal
+                policy_loss = policy_loss - self.discount**i * (rew - entropy)
+
+        #Reformat Loss
+        policy_loss = policy_loss.mean()
+        np_policy_loss = ptu.get_numpy(policy_loss)
+
+        #Train Policies
+        self.policy_optimizer.zero_grad()
+        policy_loss.backward()
+        self.policy_optimizer.step()
+
+        """
+        Update Eval Statistics
+        """
+        if self._need_to_update_eval_statistics:
+            self.eval_statistics['Policy Loss'] = np_policy_loss
+            self.eval_statistics.update(create_stats_ordered_dict(
+                'Planner Log Pis',
+                ptu.get_numpy(planner_log_pi),
+            ))
+            self.eval_statistics.update(create_stats_ordered_dict(
+                'Planner Policy mu',
+                ptu.get_numpy(planner_policy_mean),
+            ))
+            self.eval_statistics.update(create_stats_ordered_dict(
+                'Actor Log Pis',
+                ptu.get_numpy(actor_log_pi),
+            ))
+            self.eval_statistics.update(create_stats_ordered_dict(
+                'Actor Policy mu',
+                ptu.get_numpy(actor_policy_mean),
+            ))
+            if self.use_automatic_entropy_tuning:
+                self.eval_statistics['Actor Alpha'] = actor_alpha.item()
+                self.eval_statistics['Actor Alpha Loss'] = actor_alpha_loss.item()
+                self.eval_statistics['Planner Alpha'] = planner_alpha.item()
+                self.eval_statistics['Planner Alpha Loss'] = planner_alpha_loss.item()
+
+    # def train_policies(self, obs, actions, next_obs):
+    #     import copy
+    #     #Extract State Information
+    #     curr_state = obs[:, :self.state_dim]
+    #     goal_state = obs[:, self.state_dim:]
+
+    #     #Evaluate Policies
+    #     subgoal, planner_policy_mean, policy_log_std, planner_log_pi, *_ = self.planner_policy(
+    #         obs, reparameterize=True, return_log_prob=True)
+    #     abstract_obs = torch.cat([curr_state, subgoal], dim=1)
+    #     actor_action, actor_policy_mean, policy_log_std, actor_log_pi, *_ = self.actor_policy(
+    #             abstract_obs, reparameterize=True, return_log_prob=True)
+
+    #     #Update Alpha Value
+    #     if self.use_automatic_entropy_tuning:
+    #         planner_alpha_loss = -(self.log_planner_alpha * (planner_log_pi + self.planner_target_entropy).detach()).mean()
+    #         actor_alpha_loss = -(self.log_actor_alpha * (actor_log_pi + self.actor_target_entropy).detach()).mean()
+    #         self.planner_alpha_optimizer.zero_grad()
+    #         self.actor_alpha_optimizer.zero_grad()
+    #         planner_alpha_loss.backward()
+    #         actor_alpha_loss.backward()
+    #         self.planner_alpha_optimizer.step()
+    #         self.actor_alpha_optimizer.step()
+    #         planner_alpha = self.log_planner_alpha.exp()
+    #         actor_alpha = self.log_actor_alpha.exp()
+    #     else:
+    #         planner_alpha_loss = 0
+    #         planner_alpha = 1
+    #         actor_alpha_loss = 0
+    #         actor_alpha = 1
+
+    #     #Calculate Actor Loss
+    #     # q_value = torch.min(
+    #     #     self.qf1(obs, actor_action),
+    #     #     self.qf2(obs, actor_action),
+    #     # )
+    #     #actor_loss = actor_alpha*actor_log_pi - q_value
+
+    #     #Calculate Planner Loss
+
+    #     actor_policy_copy = copy.deepcopy(self.actor_policy)
+    #     planner_loss = planner_alpha*planner_log_pi
+
+    #     for i in range(self.k + 1):
+    #         curr_state = self.predict(curr_state, actor_action)
+    #         abstract_obs = torch.cat([curr_state, subgoal], dim=1)
+    #         actor_action, policy_mean, policy_log_std, actor_log_pi, *_ = actor_policy_copy(
+    #             abstract_obs, reparameterize=True, return_log_prob=True)
+    #         entropy = actor_alpha*actor_log_pi
+
+    #         if i == self.k:
+    #             curr_obs = torch.cat([curr_state, goal_state], dim=1)
+    #             q_to_go = torch.min(
+    #                 self.qf1(curr_obs, actor_action),
+    #                 self.qf2(curr_obs, actor_action))
+    #             planner_loss = planner_loss - self.discount**i * (q_to_go - entropy)
+    #         else:
+    #             rew = - torch.norm(curr_state - goal_state, dim=1, keepdim=True)
+    #             planner_loss = planner_loss - self.discount**i * (rew - entropy)
+
+    #     #Reformat Losses
+    #     #actor_loss = actor_loss.mean()
+    #     planner_loss = planner_loss.mean()
+    #     np_actor_loss = ptu.get_numpy(planner_loss)
+    #     np_planner_loss = ptu.get_numpy(planner_loss)
+
+    #     #Train Policies
+    #     self.policy_optimizer.zero_grad()
+    #     planner_loss.backward()
+    #     self.planner_optimizer.step()
+
+    #     # self.actor_optimizer.zero_grad()
+    #     # actor_loss.backward()
+    #     # self.actor_optimizer.step()
+
+    #     """
+    #     Update Eval Statistics
+    #     """
+    #     if self._need_to_update_eval_statistics:
+    #         self.eval_statistics['Actor Policy Loss'] = np_actor_loss
+    #         self.eval_statistics['Planner Policy Loss'] = np_planner_loss
+    #         self.eval_statistics.update(create_stats_ordered_dict(
+    #             'Planner Log Pis',
+    #             ptu.get_numpy(planner_log_pi),
+    #         ))
+    #         self.eval_statistics.update(create_stats_ordered_dict(
+    #             'Planner Policy mu',
+    #             ptu.get_numpy(planner_policy_mean),
+    #         ))
+    #         self.eval_statistics.update(create_stats_ordered_dict(
+    #             'Actor Log Pis',
+    #             ptu.get_numpy(actor_log_pi),
+    #         ))
+    #         self.eval_statistics.update(create_stats_ordered_dict(
+    #             'Actor Policy mu',
+    #             ptu.get_numpy(actor_policy_mean),
+    #         ))
+    #         if self.use_automatic_entropy_tuning:
+    #             self.eval_statistics['Actor Alpha'] = actor_alpha.item()
+    #             self.eval_statistics['Actor Alpha Loss'] = actor_alpha_loss.item()
+    #             self.eval_statistics['Planner Alpha'] = planner_alpha.item()
+    #             self.eval_statistics['Planner Alpha Loss'] = planner_alpha_loss.item()
+
+    def train_q_networks(self, obs, actions, next_obs, rewards, terminals):
+        #Extract Information
+        curr_state = next_obs[:, :self.state_dim]
+        goal_state = next_obs[:, self.state_dim:]
+        planner_alpha = self.log_planner_alpha.exp()
+        actor_alpha = self.log_actor_alpha.exp()
+        rewards = - torch.norm(curr_state - goal_state, dim=1, keepdim=True)
+
+        #Get Prediction
+        q1_pred = self.qf1(obs, actions)
+        q2_pred = self.qf2(obs, actions)
+
+        #Calculate Target Value
+        subgoal, planner_policy_mean, policy_log_std, planner_log_pi, *_ = self.planner_policy(
+            next_obs, reparameterize=True, return_log_prob=True)
+        target_q_values = 0
+
+
+        for i in range(self.k + 1):
+            abstract_obs = torch.cat([curr_state, subgoal], dim=1)
+            actor_action, actor_policy_mean, policy_log_std, actor_log_pi, *_ = self.actor_policy(
+                abstract_obs, reparameterize=True, return_log_prob=True)
+            entropy = actor_alpha*actor_log_pi + planner_alpha * planner_log_pi
+
+            if i == self.k:
+                curr_obs = torch.cat([curr_state, goal_state], dim=1)
+                q_to_go = torch.min(
+                    self.target_qf1(curr_obs, actor_action),
+                    self.target_qf2(curr_obs, actor_action))
+                target_q_values = target_q_values + self.discount**i * (q_to_go - entropy)
+            else:
+                curr_state = self.predict(curr_state, actor_action)
+                rew = - torch.norm(curr_state - goal_state, dim=1, keepdim=True)
+                target_q_values = target_q_values + self.discount**i * (rew - entropy)
+
+        q_target = self.reward_scale * rewards + (1. - terminals) * self.discount * target_q_values
+
+        #Calculate Losses
+        qf1_loss = self.prediction_criterion(q1_pred, q_target.detach())
+        qf2_loss = self.prediction_criterion(q2_pred, q_target.detach())
+        np_q1_loss = ptu.get_numpy(qf1_loss)
+        np_q2_loss = ptu.get_numpy(qf2_loss)
+
+        #Update Q Networks
+        self.qf1_optimizer.zero_grad()
+        qf1_loss.backward()
+        self.qf1_optimizer.step()
+
+        self.qf2_optimizer.zero_grad()
+        qf2_loss.backward()
+        self.qf2_optimizer.step()
+
+        """
+        Soft Updates
+        """
+        if self._n_train_steps_total % self.target_update_period == 0:
+            ptu.soft_update_from_to(self.qf1, self.target_qf1, self.soft_target_tau)
+            ptu.soft_update_from_to(self.qf2, self.target_qf2, self.soft_target_tau)
+
+        """
+        Update Eval Statistics
+        """
+        if self._need_to_update_eval_statistics:
+            self.eval_statistics['QF1 Loss'] = np_q1_loss
+            self.eval_statistics['QF2 Loss'] = np_q2_loss
+            self.eval_statistics.update(create_stats_ordered_dict(
+                'Q1 Predictions',
+                ptu.get_numpy(q1_pred),
+            ))
+            self.eval_statistics.update(create_stats_ordered_dict(
+                'Q2 Predictions',
+                ptu.get_numpy(q2_pred),
+            ))
+            self.eval_statistics.update(create_stats_ordered_dict(
+                'Q Targets',
+                ptu.get_numpy(q_target),
+            ))
+
+    def train_from_torch(self, batch):
+        rewards = batch['rewards']
+        terminals = batch['terminals']
+        obs = batch['observations']
+        actions = batch['actions']
+        next_obs = batch['next_observations']
+
+        """
+        Train Forward Model
+        """
+        self.train_model(obs[:, :self.state_dim],
+            actions, next_obs[:, :self.state_dim])
+
+        #Train Policies
+        self.train_policies(obs, actions, next_obs)
+
+        #Train Q Networks
+        self.train_q_networks(obs, actions, next_obs, rewards, terminals)
+
+        #Update Completed
+        self._need_to_update_eval_statistics = False
+        self._n_train_steps_total += 1
+
+    # #def train_from_torch(self, batch):
+    #     rewards = batch['rewards']
+    #     terminals = batch['terminals']
+    #     obs = batch['observations']
+    #     actions = batch['actions']
+    #     next_obs = batch['next_observations']
+
+    #     """
+    #     Train Model
+    #     """
+    #     model_loss = self.train_model(obs[:, :self.state_dim],
+    #         actions, next_obs[:, :self.state_dim])
+
+    #     """
+    #     Policy and Alpha Loss For Actor
+    #     """
+    #     curr_state = obs[:, :self.state_dim]
+    #     goal_state = obs[:, 2self.state_dim:]
+    #     subgoal, policy_mean, policy_log_std, planner_log_pi, *_ = self.planner_policy(
+    #         obs, reparameterize=True, return_log_prob=True)
+
+    #     actor_entropy = []
+    #     policy_loss = 0
+    #     for i in range(self.k):
+    #         abstract_obs = torch.cat([curr_state, subgoal], dim=1)
+    #         act, policy_mean, policy_log_std, actor_log_pi, *_ = self.actor_policy(
+    #             abstract_obs, reparameterize=True, return_log_prob=True)
+    #         actor_entropy.append(actor_log_pi)
+
+    #         if i == self.k - 1:
+    #             curr_obs = torch.cat([curr_state, goal_state], dim=1)
+    #             q_to_go = torch.min(
+    #                 self.qf1(curr_obs, act),
+    #                 self.qf2(curr_obs, act))
+    #             policy_loss = policy_loss - self.discount**i * q_to_go
+    #         else:
+    #             curr_state = self.predict(curr_state, act)
+    #             rew = ptu.from_numpy(self.env.compute_rewards(act,
+    #                 {'state_achieved_goal': ptu.get_numpy(curr_state),
+    #                 'state_desired_goal': ptu.get_numpy(goal_state)})).detach() #?????
+    #             policy_loss = policy_loss - self.discount**i * rew
+
+    #     if self.use_automatic_entropy_tuning:
+    #         planner_alpha_loss = -(self.log_planner_alpha * (planner_log_pi + self.planner_target_entropy).detach()).mean()
+    #         actor_log_pi = torch.mean(torch.cat(actor_entropy, dim=1), dim=1)
+    #         actor_alpha_loss = -(self.log_actor_alpha * (actor_log_pi + self.actor_target_entropy).detach()).mean()
+    #         self.planner_alpha_optimizer.zero_grad()
+    #         self.actor_alpha_optimizer.zero_grad()
+    #         planner_alpha_loss.backward()
+    #         actor_alpha_loss.backward()
+    #         self.planner_alpha_optimizer.step()
+    #         self.actor_alpha_optimizer.step()
+    #         planner_alpha = self.log_planner_alpha.exp()
+    #         actor_alpha = self.log_actor_alpha.exp()
+    #     else:
+    #         planner_alpha_loss = 0
+    #         planner_alpha = 1
+    #         actor_alpha_loss = 0
+    #         actor_alpha = 1
+
+    #     for i in range(k):
+    #         policy_loss = policy_loss + self.discount**i * actor_alpha*actor_entropy[i]
+    #     policy_loss = policy_loss + planner_alpha * planner_log_pi
+
+    #     """
+    #     QF Loss
+    #     """
+        
+    #     q1_pred = self.qf1(obs, actions)
+    #     q2_pred = self.qf2(obs, actions)
+    #     # Make sure policy accounts for squashing functions like tanh correctly!
+
+    #     curr_state = next_obs[:, :self.state_dim]
+    #     goal_state = next_obs[:, self.state_dim:]
+    #     subgoal, planner_policy_mean, policy_log_std, planner_log_pi, *_ = self.planner_policy(
+    #         next_obs, reparameterize=True, return_log_prob=True,
+    #     )
+
+    #     target_q_values = - planner_alpha * planner_log_pi
+    #     for i in range(self.k):
+    #         abstract_obs = torch.cat([curr_state, subgoal], dim=1)
+    #         act, actor_policy_mean, policy_log_std, actor_log_pi, *_ = self.actor_policy(
+    #             abstract_obs, reparameterize=True, return_log_prob=True)
+    #         entropy = actor_alpha*actor_log_pi
+
+    #         if i == self.k - 1:
+    #             curr_obs = torch.cat([curr_state, goal_state], dim=1)
+    #             q_to_go = torch.min(
+    #                 self.target_qf1(curr_obs, act),
+    #                 self.target_qf2(curr_obs, act))
+    #             target_q_values = target_q_values + self.discount**i * (q_to_go - entropy)
+    #         else:
+    #             curr_state = self.predict(curr_state, act)
+
+    #             rew = ptu.from_numpy(self.env.compute_rewards(act,
+    #                 {'state_achieved_goal': ptu.get_numpy(curr_state),
+    #                 'state_desired_goal': ptu.get_numpy(goal_state)}))
+    #             target_q_values = target_q_values + self.discount**i * (rew - entropy)
+
+    #     q_target = self.reward_scale * rewards + (1. - terminals) * self.discount * target_q_values
+    #     qf1_loss = self.prediction_criterion(q1_pred, q_target.detach())
+    #     qf2_loss = self.prediction_criterion(q2_pred, q_target.detach())
+
+    #     """
+    #     Update networks
+    #     """
+    #     performance_loss = ptu.get_numpy(policy_loss.mean())
+
+    #     self.policy_optimizer.zero_grad()
+    #     policy_loss.mean().backward()
+    #     self.policy_optimizer.step()
+
+    #     self.qf1_optimizer.zero_grad()
+    #     qf1_loss.backward()
+    #     self.qf1_optimizer.step()
+
+    #     self.qf2_optimizer.zero_grad()
+    #     qf2_loss.backward()
+    #     self.qf2_optimizer.step()
+
+
+    #     """
+    #     Soft Updates
+    #     """
+    #     if self._n_train_steps_total % self.target_update_period == 0:
+    #         ptu.soft_update_from_to(
+    #             self.qf1, self.target_qf1, self.soft_target_tau
+    #         )
+    #         ptu.soft_update_from_to(
+    #             self.qf2, self.target_qf2, self.soft_target_tau
+    #         )
+
+    #     """
+    #     Save some statistics for eval
+    #     """
+    #     if self._need_to_update_eval_statistics:
+    #         self._need_to_update_eval_statistics = False
+    #         """
+    #         Eval should set this to None.
+    #         This way, these statistics are only computed for one batch.
+    #         """
+    #         self.eval_statistics['QF1 Loss'] = np.mean(ptu.get_numpy(qf1_loss))
+    #         self.eval_statistics['QF2 Loss'] = np.mean(ptu.get_numpy(qf2_loss))
+    #         self.eval_statistics['Model Loss'] = np.mean(ptu.get_numpy(model_loss))
+    #         self.eval_statistics['Policy Loss'] = np.mean(
+    #             performance_loss)
+
+    #         # self.eval_statistics['Actor Policy Loss'] = np.mean(ptu.get_numpy(
+    #         #     actor_policy_loss
+    #         # ))
+    #         # self.eval_statistics['Planner Policy Loss'] = np.mean(ptu.get_numpy(
+    #         #     planner_policy_loss
+    #         # ))
+    #         self.eval_statistics.update(create_stats_ordered_dict(
+    #             'Q1 Predictions',
+    #             ptu.get_numpy(q1_pred),
+    #         ))
+    #         self.eval_statistics.update(create_stats_ordered_dict(
+    #             'Q2 Predictions',
+    #             ptu.get_numpy(q2_pred),
+    #         ))
+    #         self.eval_statistics.update(create_stats_ordered_dict(
+    #             'Q Targets',
+    #             ptu.get_numpy(q_target),
+    #         ))
+    #         self.eval_statistics.update(create_stats_ordered_dict(
+    #             'Planner Log Pis',
+    #             ptu.get_numpy(planner_log_pi),
+    #         ))
+    #         self.eval_statistics.update(create_stats_ordered_dict(
+    #             'Planner Policy mu',
+    #             ptu.get_numpy(planner_policy_mean),
+    #         ))
+    #         self.eval_statistics.update(create_stats_ordered_dict(
+    #             'Actor Log Pis',
+    #             ptu.get_numpy(actor_log_pi),
+    #         ))
+    #         self.eval_statistics.update(create_stats_ordered_dict(
+    #             'Actor Policy mu',
+    #             ptu.get_numpy(actor_policy_mean),
+    #         ))
+    #         # self.eval_statistics.update(create_stats_ordered_dict(
+    #         #     'Policy log std',
+    #         #     ptu.get_numpy(policy_log_std),
+    #         # ))
+    #         if self.use_automatic_entropy_tuning:
+    #             self.eval_statistics['Actor Alpha'] = actor_alpha.item()
+    #             self.eval_statistics['Actor Alpha Loss'] = actor_alpha_loss.item()
+    #             self.eval_statistics['Planner Alpha'] = planner_alpha.item()
+    #             self.eval_statistics['Planner Alpha Loss'] = planner_alpha_loss.item()
+    #     self._n_train_steps_total += 1
+
+    def get_diagnostics(self):
+        stats = super().get_diagnostics()
+        stats.update(self.eval_statistics)
+        return stats
+
+    def end_epoch(self, epoch):
+        self._need_to_update_eval_statistics = True
+
+    @property
+    def networks(self):
+        return [
+            self.actor_policy,
+            self.planner_policy,
+            self.model,
+            self.qf1,
+            self.qf2,
+            self.target_qf1,
+            self.target_qf2,
+        ]
+
+    def get_snapshot(self):
+        return dict(
+            actor_policy=self.actor_policy,
+            planner_policy=self.planner_policy,
+            model=self.model,
+            qf1=self.qf1,
+            qf2=self.qf2,
+            target_qf1=self.target_qf1,
+            target_qf2=self.target_qf2,
+        )
diff --git a/railrl/torch/sac/sap_joined.py b/railrl/torch/sac/sap_joined.py
new file mode 100644
index 0000000..ac2346c
--- /dev/null
+++ b/railrl/torch/sac/sap_joined.py
@@ -0,0 +1,255 @@
+from collections import OrderedDict
+
+import numpy as np
+import torch
+import torch.optim as optim
+from torch import nn as nn
+
+import railrl.torch.pytorch_util as ptu
+from railrl.misc.eval_util import create_stats_ordered_dict
+from railrl.torch.core import np_to_pytorch_batch
+from railrl.torch.torch_rl_algorithm import TorchTrainer
+
+
+class SAPTrainer(TorchTrainer):
+    def __init__(
+            self,
+            env,
+            policy,
+            qf1,
+            qf2,
+            target_qf1,
+            target_qf2,
+
+            act_dim,
+            obs_dim,
+
+            discount=0.99,
+            reward_scale=1.0,
+
+            policy_lr=1e-3,
+            qf_lr=1e-3,
+            optimizer_class=optim.Adam,
+
+            soft_target_tau=1e-2,
+            target_update_period=1,
+            plotter=None,
+            render_eval_paths=False,
+
+            use_automatic_entropy_tuning=True,
+            target_entropy=None,
+    ):
+        super().__init__()
+        self.env = env
+        self.policy = policy
+        self.qf1 = qf1
+        self.qf2 = qf2
+        self.target_qf1 = target_qf1
+        self.target_qf2 = target_qf2
+        self.act_dim = act_dim
+        self.obs_dim = obs_dim
+        self.soft_target_tau = soft_target_tau
+        self.target_update_period = target_update_period
+
+        self.use_automatic_entropy_tuning = use_automatic_entropy_tuning
+        if self.use_automatic_entropy_tuning:
+            if target_entropy:
+                self.target_entropy = target_entropy
+            else:
+                self.target_entropy = -np.prod(self.env.action_space.shape).item()  # heuristic value from Tuomas
+            self.log_alpha = ptu.zeros(1, requires_grad=True)
+            self.alpha_optimizer = optimizer_class(
+                [self.log_alpha],
+                lr=policy_lr,
+            )
+
+        self.plotter = plotter
+        self.render_eval_paths = render_eval_paths
+
+        self.qf_criterion = nn.MSELoss()
+        self.vf_criterion = nn.MSELoss()
+
+        self.policy_optimizer = optimizer_class(
+            self.policy.parameters(),
+            lr=policy_lr,
+        )
+        self.qf1_optimizer = optimizer_class(
+            self.qf1.parameters(),
+            lr=qf_lr,
+        )
+        self.qf2_optimizer = optimizer_class(
+            self.qf2.parameters(),
+            lr=qf_lr,
+        )
+
+        self.discount = discount
+        self.reward_scale = reward_scale
+        self.eval_statistics = OrderedDict()
+        self._n_train_steps_total = 0
+        self._need_to_update_eval_statistics = True
+
+    def train_from_torch(self, batch):
+        rewards = batch['rewards']
+        terminals = batch['terminals']
+        obs = batch['observations']
+        low_actions = batch['actions']
+        next_obs = batch['next_observations']
+
+        """
+        Policy and Alpha Loss
+        """
+        new_obs_actions, policy_mean, policy_log_std, log_pi, *_ = self.policy(
+            obs, reparameterize=True, return_log_prob=True,
+        )
+        if self.use_automatic_entropy_tuning:
+            alpha_loss = -(self.log_alpha * (log_pi + self.target_entropy).detach()).mean()
+            self.alpha_optimizer.zero_grad()
+            alpha_loss.backward()
+            self.alpha_optimizer.step()
+            alpha = self.log_alpha.exp()
+        else:
+            alpha_loss = 0
+            alpha = 1
+
+        q_new_actions = torch.min(
+            self.qf1(obs, new_obs_actions),
+            self.qf2(obs, new_obs_actions),
+        )
+        policy_loss = (alpha*log_pi - q_new_actions).mean()
+
+        """
+        QF Loss
+        """
+        midpoints = new_obs_actions[:, self.act_dim:].detach()
+        actions = torch.cat([low_actions, midpoints], dim=1)
+
+        q1_pred = self.qf1(obs, actions)
+        q2_pred = self.qf2(obs, actions)
+        # Make sure policy accounts for squashing functions like tanh correctly!
+        curr_states = next_obs[:, :self.obs_dim]
+        goal_states = next_obs[:, self.obs_dim:]
+
+        s_m_obs = torch.cat([curr_states, midpoints], dim=1)
+        m_g_obs = torch.cat([midpoints, goal_states], dim=1)
+
+        s_m_acts, _, _, s_m_log_pi, *_ = self.policy(
+            s_m_obs, reparameterize=True, return_log_prob=True,
+        )
+        m_g_acts, _, _, m_g_log_pi, *_ = self.policy(
+            m_g_obs, reparameterize=True, return_log_prob=True,
+        )
+
+        target_a = torch.min(
+            self.target_qf1(s_m_obs, s_m_acts),
+            self.target_qf2(s_m_obs, s_m_acts),
+        ) - alpha * s_m_log_pi
+
+        target_b = torch.min(
+            self.target_qf1(m_g_obs, m_g_acts),
+            self.target_qf2(m_g_obs, m_g_acts),
+        ) - alpha * m_g_log_pi
+
+        #target_q_values = target_a + target_b + torch.norm(target_a - target_b)
+        target_q_values = self.discount * target_a + torch.exp(-target_a * np.log(self.discount)) * target_b + torch.norm(target_a - target_b)
+
+        q_target = self.reward_scale * rewards + (1. - terminals) * target_q_values
+        qf1_loss = self.qf_criterion(q1_pred, q_target.detach())
+        qf2_loss = self.qf_criterion(q2_pred, q_target.detach())
+
+        """
+        Update networks
+        """
+        self.qf1_optimizer.zero_grad()
+        qf1_loss.backward()
+        self.qf1_optimizer.step()
+
+        self.qf2_optimizer.zero_grad()
+        qf2_loss.backward()
+        self.qf2_optimizer.step()
+
+        self.policy_optimizer.zero_grad()
+        policy_loss.backward()
+        self.policy_optimizer.step()
+
+        """
+        Soft Updates
+        """
+        if self._n_train_steps_total % self.target_update_period == 0:
+            ptu.soft_update_from_to(
+                self.qf1, self.target_qf1, self.soft_target_tau
+            )
+            ptu.soft_update_from_to(
+                self.qf2, self.target_qf2, self.soft_target_tau
+            )
+
+        """
+        Save some statistics for eval
+        """
+        if self._need_to_update_eval_statistics:
+            self._need_to_update_eval_statistics = False
+            """
+            Eval should set this to None.
+            This way, these statistics are only computed for one batch.
+            """
+            policy_loss = (log_pi - q_new_actions).mean()
+
+            self.eval_statistics['QF1 Loss'] = np.mean(ptu.get_numpy(qf1_loss))
+            self.eval_statistics['QF2 Loss'] = np.mean(ptu.get_numpy(qf2_loss))
+            self.eval_statistics['Policy Loss'] = np.mean(ptu.get_numpy(
+                policy_loss
+            ))
+            self.eval_statistics.update(create_stats_ordered_dict(
+                'Q1 Predictions',
+                ptu.get_numpy(q1_pred),
+            ))
+            self.eval_statistics.update(create_stats_ordered_dict(
+                'Q2 Predictions',
+                ptu.get_numpy(q2_pred),
+            ))
+            self.eval_statistics.update(create_stats_ordered_dict(
+                'Q Targets',
+                ptu.get_numpy(q_target),
+            ))
+            self.eval_statistics.update(create_stats_ordered_dict(
+                'Log Pis',
+                ptu.get_numpy(log_pi),
+            ))
+            self.eval_statistics.update(create_stats_ordered_dict(
+                'Policy mu',
+                ptu.get_numpy(policy_mean),
+            ))
+            self.eval_statistics.update(create_stats_ordered_dict(
+                'Policy log std',
+                ptu.get_numpy(policy_log_std),
+            ))
+            if self.use_automatic_entropy_tuning:
+                self.eval_statistics['Alpha'] = alpha.item()
+                self.eval_statistics['Alpha Loss'] = alpha_loss.item()
+        self._n_train_steps_total += 1
+
+    def get_diagnostics(self):
+        stats = super().get_diagnostics()
+        stats.update(self.eval_statistics)
+        return stats
+
+    def end_epoch(self, epoch):
+        self._need_to_update_eval_statistics = True
+
+    @property
+    def networks(self):
+        return [
+            self.policy,
+            self.qf1,
+            self.qf2,
+            self.target_qf1,
+            self.target_qf2,
+        ]
+
+    def get_snapshot(self):
+        return dict(
+            policy=self.policy,
+            qf1=self.qf1,
+            qf2=self.qf2,
+            target_qf1=self.qf1,
+            target_qf2=self.qf2,
+        )
\ No newline at end of file
diff --git a/railrl/torch/sac/sap_seperate.py b/railrl/torch/sac/sap_seperate.py
new file mode 100644
index 0000000..8f91fa3
--- /dev/null
+++ b/railrl/torch/sac/sap_seperate.py
@@ -0,0 +1,358 @@
+from collections import OrderedDict
+
+import numpy as np
+import torch
+import torch.optim as optim
+from torch import nn as nn
+
+import railrl.torch.pytorch_util as ptu
+from railrl.misc.eval_util import create_stats_ordered_dict
+from railrl.torch.core import np_to_pytorch_batch
+from railrl.torch.torch_rl_algorithm import TorchTrainer
+
+
+class SAPTrainer(TorchTrainer):
+    def __init__(
+            self,
+            env,
+            actor_policy,
+            planner_policy,
+            qf1,
+            qf2,
+            #qp1,
+            #qp2,
+            target_qf1,
+            target_qf2,
+            #target_qp1,
+            #target_qp2,
+
+            act_dim,
+            obs_dim,
+
+            discount=0.99,
+            reward_scale=1.0,
+
+            policy_lr=1e-3,
+            qf_lr=1e-3,
+            optimizer_class=optim.Adam,
+
+            soft_target_tau=1e-2,
+            target_update_period=1,
+            plotter=None,
+            render_eval_paths=False,
+
+            use_automatic_entropy_tuning=True,
+            target_entropy=None,
+    ):
+        super().__init__()
+        self.env = env
+        self.actor_policy = actor_policy
+        self.planner_policy = planner_policy
+        self.qf1 = qf1
+        self.qf2 = qf2
+        self.target_qf1 = target_qf1
+        self.target_qf2 = target_qf2
+        self.soft_target_tau = soft_target_tau
+        self.target_update_period = target_update_period
+        self.act_dim = act_dim
+        self.obs_dim = obs_dim
+
+        self.use_automatic_entropy_tuning = use_automatic_entropy_tuning
+        if self.use_automatic_entropy_tuning:
+            if target_entropy:
+                self.target_entropy = target_entropy
+            else:
+                self.actor_target_entropy = -np.prod(self.env.action_space.shape).item()  # heuristic value from Tuomas
+                self.planner_target_entropy = -np.prod(obs_dim).item()
+            self.log_actor_alpha = ptu.zeros(1, requires_grad=True)
+            self.actor_alpha_optimizer = optimizer_class(
+                [self.log_actor_alpha],
+                lr=policy_lr,
+            )
+            self.log_planner_alpha = ptu.zeros(1, requires_grad=True)
+            self.planner_alpha_optimizer = optimizer_class(
+                [self.log_planner_alpha],
+                lr=policy_lr,
+            )
+
+        self.plotter = plotter
+        self.render_eval_paths = render_eval_paths
+
+        self.qf_criterion = nn.MSELoss()
+        self.vf_criterion = nn.MSELoss()
+
+        self.policy_optimizer = optimizer_class(
+            list(self.actor_policy.parameters()) + list(self.planner_policy.parameters()),
+            lr=policy_lr,
+        )
+        # self.planner_policy_optimizer = optimizer_class(
+        #     self.planner_policy.parameters(),
+        #     lr=policy_lr,
+        # )
+        self.qf1_optimizer = optimizer_class(
+            self.qf1.parameters(),
+            lr=qf_lr,
+        )
+        self.qf2_optimizer = optimizer_class(
+            self.qf2.parameters(),
+            lr=qf_lr,
+        )
+
+        self.discount = discount
+        self.reward_scale = reward_scale
+        self.eval_statistics = OrderedDict()
+        self._n_train_steps_total = 0
+        self._need_to_update_eval_statistics = True
+
+    # def get_agent_action(self, obs):
+    #     instructions, policy_mean, policy_log_std, planner_log_pi, *_ = self.planner_policy(
+    #         obs, reparameterize=True, return_log_prob=True,
+    #     )
+    #     actions, policy_mean, policy_log_std, actor_log_pi, *_ = self.actor_policy(
+    #         instructions, reparameterize=True, return_log_prob=True,
+    #     )
+    #     return actions, 
+
+    def train_from_torch(self, batch):
+        rewards = batch['rewards']
+        terminals = batch['terminals']
+        obs = batch['observations']
+        actions = batch['actions']
+        next_obs = batch['next_observations']
+
+        """
+        Policy and Alpha Loss For Actor
+        """
+
+        instructions, policy_mean, policy_log_std, planner_log_pi, *_ = self.planner_policy(
+            obs, reparameterize=True, return_log_prob=True,
+        )
+        new_obs_actions, policy_mean, policy_log_std, actor_log_pi, *_ = self.actor_policy(
+            instructions, reparameterize=True, return_log_prob=True,
+        )
+
+        if self.use_automatic_entropy_tuning:
+            planner_alpha_loss = -(self.log_planner_alpha * (planner_log_pi + self.planner_target_entropy).detach()).mean()
+            actor_alpha_loss = -(self.log_actor_alpha * (actor_log_pi + self.actor_target_entropy).detach()).mean()
+            self.planner_alpha_optimizer.zero_grad()
+            self.actor_alpha_optimizer.zero_grad()
+            planner_alpha_loss.backward()
+            actor_alpha_loss.backward()
+            self.planner_alpha_optimizer.step()
+            self.actor_alpha_optimizer.step()
+            planner_alpha = self.log_planner_alpha.exp()
+            actor_alpha = self.log_actor_alpha.exp()
+        else:
+            planner_alpha_loss = 0
+            planner_alpha = 1
+            actor_alpha_loss = 0
+            actor_alpha = 1
+
+        q_new_actions = torch.min(
+            self.qf1(obs, new_obs_actions),
+            self.qf2(obs, new_obs_actions),
+        )
+        entropy = actor_alpha*actor_log_pi + planner_alpha * planner_log_pi
+        policy_loss = (entropy - q_new_actions).mean()
+
+
+
+        # """
+        # Policy and Alpha Loss For Planner
+        # """
+
+
+        # midpoints, policy_mean, policy_log_std, log_prob, *_ = self.planner_policy(
+        #     obs, reparameterize=True, return_log_prob=True,
+        # )
+        # #midpoints = self.planner_policy(obs)
+
+        # if self.use_automatic_entropy_tuning:
+        #     planner_alpha_loss = -(self.log_planner_alpha * (log_prob + self.target_entropy).detach()).mean()
+        #     self.planner_alpha_optimizer.zero_grad()
+        #     planner_alpha_loss.backward()
+        #     self.planner_alpha_optimizer.step()
+        #     planner_alpha = self.log_planner_alpha.exp()
+        # else:
+        #     planner_alpha_loss = 0
+        #     planner_alpha = 1
+
+        # curr_states = obs[:, :self.obs_dim]
+        # goal_states = obs[:, self.obs_dim:]
+
+        # a_obs = torch.cat([curr_states, midpoints], dim=1)
+        # b_obs = torch.cat([midpoints, goal_states], dim=1)
+
+        # a_acts, _, _, a_log_pi, *_ = self.actor_policy(
+        #     a_obs, reparameterize=True, return_log_prob=True,
+        # )
+        # b_acts, _, _, b_log_pi, *_ = self.actor_policy(
+        #     b_obs, reparameterize=True, return_log_prob=True,
+        # )
+        # a = [self.qf1(a_obs, a_acts),  self.qf2(a_obs, a_acts), self.target_qf1(a_obs, a_acts), self.target_qf2(a_obs, a_acts)]
+        # b = [self.qf1(b_obs, b_acts), self.qf2(b_obs, b_acts), self.target_qf1(b_obs, b_acts), self.target_qf2(b_obs, b_acts)]
+        # c = [self.qf1(obs, a_acts),  self.qf2(obs, a_acts), self.target_qf1(obs, a_acts), self.target_qf2(obs, a_acts)]
+        # a_funcs = torch.cat(a, dim=1)
+        # b_funcs = torch.cat(b, dim=1)
+        # c_funcs = torch.cat(c, dim=1)
+
+        # entropy = planner_alpha * log_prob
+
+        # #ADD DISAGREEMENT, LIKE 
+        # planner_policy_loss = (entropy - torch.mean(c_funcs).reshape(-1, 1)).mean()
+        #target = 1/2 * (curr_states + goal_states)
+        #planner_policy_loss = ((target - midpoints)**2).mean()
+        #planner_policy_loss = (- self.target_qf1(a_obs, a_acts) - self.target_qf1(b_obs, b_acts) + (self.target_qf1(a_obs, a_acts) - self.target_qf1(b_obs, b_acts) **2)).mean()
+
+
+        """
+        QF Loss
+        """
+        
+        q1_pred = self.qf1(obs, actions)
+        q2_pred = self.qf2(obs, actions)
+        # Make sure policy accounts for squashing functions like tanh correctly!
+
+
+        instructions, planner_policy_mean, policy_log_std, planner_log_pi, *_ = self.planner_policy(
+            next_obs, reparameterize=True, return_log_prob=True,
+        )
+        new_next_actions, actor_policy_mean, policy_log_std, actor_log_pi, *_ = self.actor_policy(
+            instructions, reparameterize=True, return_log_prob=True,
+        )
+
+
+        # new_next_actions, _, _, new_log_pi, *_ = self.actor_policy(
+        #     next_obs, reparameterize=True, return_log_prob=True,
+        # )
+        
+        target_q_values = torch.min(
+            self.target_qf1(next_obs, new_next_actions),
+            self.target_qf2(next_obs, new_next_actions),
+        ) - planner_alpha * planner_log_pi - actor_alpha * actor_log_pi
+
+        q_target = self.reward_scale * rewards + (1. - terminals) * self.discount * target_q_values
+        qf1_loss = self.qf_criterion(q1_pred, q_target.detach())
+        qf2_loss = self.qf_criterion(q2_pred, q_target.detach())
+
+        """
+        Update networks
+        """
+
+        self.policy_optimizer.zero_grad()
+        policy_loss.backward()
+        self.policy_optimizer.step()
+
+        self.qf1_optimizer.zero_grad()
+        qf1_loss.backward()
+        self.qf1_optimizer.step()
+
+        self.qf2_optimizer.zero_grad()
+        qf2_loss.backward()
+        self.qf2_optimizer.step()
+
+        # self.actor_policy_optimizer.zero_grad()
+        # actor_policy_loss.backward()
+        # self.actor_policy_optimizer.step()
+
+        """
+        Soft Updates
+        """
+        if self._n_train_steps_total % self.target_update_period == 0:
+            ptu.soft_update_from_to(
+                self.qf1, self.target_qf1, self.soft_target_tau
+            )
+            ptu.soft_update_from_to(
+                self.qf2, self.target_qf2, self.soft_target_tau
+            )
+
+        """
+        Save some statistics for eval
+        """
+        if self._need_to_update_eval_statistics:
+            self._need_to_update_eval_statistics = False
+            """
+            Eval should set this to None.
+            This way, these statistics are only computed for one batch.
+            """
+            policy_loss = (entropy - q_new_actions).mean()
+
+            self.eval_statistics['QF1 Loss'] = np.mean(ptu.get_numpy(qf1_loss))
+            self.eval_statistics['QF2 Loss'] = np.mean(ptu.get_numpy(qf2_loss))
+            self.eval_statistics['Policy Loss'] = np.mean(ptu.get_numpy(
+                policy_loss
+            ))
+
+            # self.eval_statistics['Actor Policy Loss'] = np.mean(ptu.get_numpy(
+            #     actor_policy_loss
+            # ))
+            # self.eval_statistics['Planner Policy Loss'] = np.mean(ptu.get_numpy(
+            #     planner_policy_loss
+            # ))
+            self.eval_statistics.update(create_stats_ordered_dict(
+                'Q1 Predictions',
+                ptu.get_numpy(q1_pred),
+            ))
+            self.eval_statistics.update(create_stats_ordered_dict(
+                'Q2 Predictions',
+                ptu.get_numpy(q2_pred),
+            ))
+            self.eval_statistics.update(create_stats_ordered_dict(
+                'Q Targets',
+                ptu.get_numpy(q_target),
+            ))
+            self.eval_statistics.update(create_stats_ordered_dict(
+                'Planner Log Pis',
+                ptu.get_numpy(planner_log_pi),
+            ))
+            self.eval_statistics.update(create_stats_ordered_dict(
+                'Planner Policy mu',
+                ptu.get_numpy(planner_policy_mean),
+            ))
+            self.eval_statistics.update(create_stats_ordered_dict(
+                'Actor Log Pis',
+                ptu.get_numpy(actor_log_pi),
+            ))
+            self.eval_statistics.update(create_stats_ordered_dict(
+                'Actor Policy mu',
+                ptu.get_numpy(actor_policy_mean),
+            ))
+            # self.eval_statistics.update(create_stats_ordered_dict(
+            #     'Policy log std',
+            #     ptu.get_numpy(policy_log_std),
+            # ))
+            if self.use_automatic_entropy_tuning:
+                self.eval_statistics['Actor Alpha'] = actor_alpha.item()
+                self.eval_statistics['Actor Alpha Loss'] = actor_alpha_loss.item()
+                self.eval_statistics['Planner Alpha'] = planner_alpha.item()
+                self.eval_statistics['Planner Alpha Loss'] = planner_alpha_loss.item()
+        self._n_train_steps_total += 1
+
+    def get_diagnostics(self):
+        stats = super().get_diagnostics()
+        stats.update(self.eval_statistics)
+        return stats
+
+    def end_epoch(self, epoch):
+        self._need_to_update_eval_statistics = True
+
+    @property
+    def networks(self):
+        return [
+            self.actor_policy,
+            self.planner_policy,
+            self.qf1,
+            self.qf2,
+            self.target_qf1,
+            self.target_qf2,
+        ]
+
+    def get_snapshot(self):
+        return dict(
+            actor_policy=self.actor_policy,
+            planner_policy=self.planner_policy,
+            qf1=self.qf1,
+            qf2=self.qf2,
+            target_qf1=self.target_qf1,
+            target_qf2=self.target_qf2,
+        )
\ No newline at end of file
diff --git a/railrl/torch/vae/conditional_conv_vae.py b/railrl/torch/vae/conditional_conv_vae.py
index 05bd794..f20927e 100644
--- a/railrl/torch/vae/conditional_conv_vae.py
+++ b/railrl/torch/vae/conditional_conv_vae.py
@@ -1,332 +1,16 @@
 import torch
+import numpy as np
 import torch.utils.data
 from torch import nn
 from torch.nn import functional as F
 from railrl.pythonplusplus import identity
+from railrl.torch.data_management.normalizer import TorchNormalizer
 from railrl.torch import pytorch_util as ptu
-import numpy as np
+from railrl.torch.networks import FlattenMlp, TanhMlpPolicy, MlpPolicy
 from railrl.torch.networks import CNN, TwoHeadDCNN, DCNN
 from railrl.torch.vae.vae_base import compute_bernoulli_log_prob, compute_gaussian_log_prob, GaussianLatentVAE
 from railrl.torch.vae.conv_vae import ConvVAE
 
-class ConditionalConvVAE(GaussianLatentVAE):
-    def __init__(
-            self,
-            representation_size,
-            architecture,
-
-            encoder_class=CNN,
-            decoder_class=DCNN,
-            decoder_output_activation=identity,
-            decoder_distribution='bernoulli',
-
-            input_channels=1,
-            imsize=48,
-            init_w=1e-3,
-            min_variance=1e-3,
-            hidden_init=ptu.fanin_init,
-            reconstruction_channels=3,
-            base_depth=32,
-            weight_init_gain=1.0,
-    ):
-        """
-        :param representation_size:
-        :param conv_args:
-        must be a dictionary specifying the following:
-            kernel_sizes
-            n_channels
-            strides
-        :param conv_kwargs:
-        a dictionary specifying the following:
-            hidden_sizes
-            batch_norm
-        :param deconv_args:
-        must be a dictionary specifying the following:
-            hidden_sizes
-            deconv_input_width
-            deconv_input_height
-            deconv_input_channels
-            deconv_output_kernel_size
-            deconv_output_strides
-            deconv_output_channels
-            kernel_sizes
-            n_channels
-            strides
-        :param deconv_kwargs:
-            batch_norm
-        :param encoder_class:
-        :param decoder_class:
-        :param decoder_output_activation:
-        :param decoder_distribution:
-        :param input_channels:
-        :param imsize:
-        :param init_w:
-        :param min_variance:
-        :param hidden_init:
-        """
-        super().__init__(representation_size)
-        if min_variance is None:
-            self.log_min_variance = None
-        else:
-            self.log_min_variance = float(np.log(min_variance))
-        self.input_channels = 6
-        self.imsize = imsize
-        self.imlength = self.imsize*self.imsize*self.input_channels
-        self.hidden_init = hidden_init
-        self.init_w = init_w
-        self.architecture = architecture
-        self.reconstruction_channels = reconstruction_channels
-        self.decoder_output_activation = decoder_output_activation
-
-        conv_args, conv_kwargs, deconv_args, deconv_kwargs = \
-            architecture['conv_args'], architecture['conv_kwargs'], \
-            architecture['deconv_args'], architecture['deconv_kwargs']
-        conv_output_size=deconv_args['deconv_input_width']*\
-                         deconv_args['deconv_input_height']*\
-                         deconv_args['deconv_input_channels']
-
-        # self.encoder=encoder_class(
-        #     **conv_args,
-        #     paddings=np.zeros(len(conv_args['kernel_sizes']), dtype=np.int64),
-        #     input_height=self.imsize,
-        #     input_width=self.imsize,
-        #     input_channels=self.input_channels,
-        #     output_size=conv_output_size,
-        #     init_w=init_w,
-        #     hidden_init=hidden_init,
-        #     **conv_kwargs)
-
-        # self.decoder = decoder_class(
-        #     **deconv_args,
-        #     fc_input_size=representation_size,
-        #     init_w=init_w,
-        #     output_activation=decoder_output_activation,
-        #     paddings=np.zeros(len(deconv_args['kernel_sizes']), dtype=np.int64),
-        #     hidden_init=hidden_init,
-        #     **deconv_kwargs)
-
-        self.relu = nn.LeakyReLU()
-        self.gain = weight_init_gain
-        self.init_w = init_w
-
-        self.base_depth = base_depth
-        self.epoch = 0
-        self.decoder_distribution=decoder_distribution
-        self.representation_size = representation_size
-
-        self._create_layers()
-
-    def _create_layers(self):
-        self.conv1a = nn.Conv2d(3, self.base_depth, 3, stride=3)
-        nn.init.xavier_uniform_(self.conv1a.weight, gain=self.gain)
-        self.conv2a = nn.Conv2d(self.base_depth, self.base_depth * 2 , 3, stride=3)
-        nn.init.xavier_uniform_(self.conv2a.weight, gain=self.gain)
-
-        self.conv1b = nn.Conv2d(3, self.base_depth, 3, stride=3)
-        nn.init.xavier_uniform_(self.conv1b.weight, gain=self.gain)
-        self.conv2b = nn.Conv2d(self.base_depth, self.base_depth * 2 , 3, stride=3)
-        nn.init.xavier_uniform_(self.conv2b.weight, gain=self.gain)
-
-        self.conv3 = nn.Conv2d(2 * self.base_depth* 2, self.base_depth * 4, 3, stride=2) # fusion
-        nn.init.xavier_uniform_(self.conv3.weight, gain=self.gain)
-
-        self.fc1 = nn.Linear(self.base_depth*4*2*2, self.representation_size)
-        self.fc2 = nn.Linear(self.base_depth*4*2*2, self.representation_size)
-
-        self.fc1.weight.data.uniform_(-self.init_w, self.init_w)
-        self.fc1.bias.data.uniform_(-self.init_w, self.init_w)
-
-        self.fc2.weight.data.uniform_(-self.init_w, self.init_w)
-        self.fc2.bias.data.uniform_(-self.init_w, self.init_w)
-
-        self.deconv_fc1 = nn.Linear(self.representation_size, 2*2*self.base_depth*4)
-        self.deconv_fc1.weight.data.uniform_(-self.init_w, self.init_w)
-        self.deconv_fc1.bias.data.uniform_(-self.init_w, self.init_w)
-
-        self.dconv1 = nn.ConvTranspose2d(self.base_depth*4, self.base_depth*4, 5, stride=3)
-        nn.init.xavier_uniform_(self.dconv1.weight, gain=self.gain)
-        self.dconv2 = nn.ConvTranspose2d(2*self.base_depth*4, self.base_depth*2, 6, stride=2) # skip connection
-        nn.init.xavier_uniform_(self.dconv2.weight, gain=self.gain)
-
-        self.dconv3 = nn.ConvTranspose2d(2*self.base_depth*2, 3, 10, stride=2)
-        nn.init.xavier_uniform_(self.dconv3.weight, gain=self.gain)
-
-        self.up1 = nn.UpsamplingNearest2d(scale_factor=4)
-        self.up2 = nn.UpsamplingNearest2d(scale_factor=4)
-
-
-    def forward(self, input):
-        """
-        :param input:
-        :return: reconstructed input, obs_distribution_params, latent_distribution_params
-        """
-        input = input.view(-1, self.input_channels, self.imsize, self.imsize)
-        # import pdb; pdb.set_trace()
-        x = input[:, :3, :, :]
-        x0 = input[:, 3:, :, :]
-
-        a1 = self.conv1a(x)
-        a2 = self.conv2a(self.relu(a1))
-        b1 = self.conv1b(x0)
-        b2 = self.conv2b(self.relu(b1)) # 32 x 18 x 18
-
-        h2 = torch.cat((a2, b2), dim=1)
-        h3 = self.conv3(self.relu(h2))
-
-        # hlayers = [a1, h2, h3, ]
-        # for l in hlayers:
-        #     print(l.shape)
-        h = h3.view(h3.size()[0], -1)
-
-        ### encode
-        mu = self.fc1(h)
-        if self.log_min_variance is None:
-            logvar = self.fc2(h)
-        else:
-            logvar = self.log_min_variance + torch.abs(self.fc2(h))
-
-        latent_distribution_params = (mu, logvar)
-
-        ### reparameterize
-
-        latents = self.reparameterize(latent_distribution_params)
-
-        dh0 = self.deconv_fc1(latents)
-        dh0 = self.relu(dh0.view(-1, self.base_depth*4, 2, 2))
-        ### decode
-
-        dh1 = self.dconv1(dh0)
-        dh1 = torch.cat((dh1, self.up2(dh0)), dim=1)
-
-        dh2 = self.dconv2(self.relu(dh1))
-
-        # fusion
-        f = torch.cat((dh2, self.up1(b2)), dim=1)
-
-        dh3 = self.dconv3(self.relu(f))
-
-        # print(dh3.shape)
-
-        """
-        f3 = torch.cat((dh3, b2), dim=1)
-        dh4 = self.dconv4(self.relu(f3))
-        dh5 = self.dconv5(self.relu(dh4))
-        """
-
-        # dlayers = [dh1, dh2, dh3, dh4, dh5]
-        # for l in dlayers:
-        #     print(l.shape)
-
-        decoded = self.decoder_output_activation(dh3)
-
-        decoded = decoded.view(-1, self.imsize*self.imsize*self.reconstruction_channels)
-        # if self.decoder_distribution == 'bernoulli': # assume bernoulli
-        reconstructions, obs_distribution_params = decoded, [decoded]
-
-        return reconstructions, obs_distribution_params, latent_distribution_params
-
-    def encode(self, input):
-        input = input.view(-1, self.input_channels, self.imsize, self.imsize)
-        x = input[:, :3, :, :]
-        x0 = input[:, 3:, :, :]
-        a1 = self.conv1a(x)
-        a2 = self.conv2a(self.relu(a1))
-        b1 = self.conv1b(x0)
-        b2 = self.conv2b(self.relu(b1)) # 32 x 18 x 18
-
-
-
-        h2 = torch.cat((a2, b2), dim=1)
-        h3 = self.conv3(self.relu(h2))
-
-        # hlayers = [h2, h3, h4, h5]
-        # for l in hlayers:
-        #     print(l.shape)
-        h = h3.view(h3.size()[0], -1)
-
-        ### encode
-        mu = self.fc1(h)
-        if self.log_min_variance is None:
-            logvar = self.fc2(h)
-        else:
-            logvar = self.log_min_variance + torch.abs(self.fc2(h))
-
-        return (mu, logvar)
-
-
-    def decode(self, latents, data):
-        data = data.view(-1, self.input_channels, self.imsize, self.imsize)
-        x0 = data[:, 3:, :, :]
-
-        b1 = self.conv1b(x0)
-        b2 = self.conv2b(self.relu(b1)) # 32 x 18 x 18
-
-        # newer arch
-        dh0 = self.deconv_fc1(latents)
-        dh0 = self.relu(dh0.view(-1, self.base_depth*4, 2, 2))
-        ### decode
-
-        dh1 = self.dconv1(dh0)
-        dh1 = torch.cat((dh1, self.up2(dh0)), dim=1)
-
-        dh2 = self.dconv2(self.relu(dh1))
-
-        # fusion
-        if dh2.shape != self.up1(b2).shape:
-            import pdb; pdb.set_trace()
-        f = torch.cat((dh2, self.up1(b2)), dim=1)
-
-        dh3 = self.dconv3(self.relu(f))
-
-
-        # older arch
-        # dh0 = self.deconv_fc1(latents)
-        # dh0 = self.relu(dh0.view(-1, self.base_depth*4, 2, 2))
-        # dh1 = self.dconv1(dh0)
-        # dh1 = torch.cat((dh1, self.up2(dh0)), dim=1)
-        # dh2 = self.dconv2(self.relu(dh1))
-
-        # f = torch.cat((dh2, self.up1(b2)), dim=1)
-
-        # dh3 = self.dconv3(self.relu(f))
-
-
-
-        #f3 = torch.cat((dh3, b2), dim=1)
-        #dh4 = self.dconv4(self.relu(f3))
-        #dh5 = self.dconv5(self.relu(dh4))
-
-        #decoded = self.decoder_output_activation(dh5)
-        decoded = self.decoder_output_activation(dh3)
-        decoded = decoded.view(-1, self.imsize*self.imsize*self.reconstruction_channels)
-        if self.decoder_distribution == 'bernoulli':
-            return decoded, [decoded]
-        elif self.decoder_distribution == 'gaussian_identity_variance':
-            return torch.clamp(decoded, 0, 1), [torch.clamp(decoded, 0, 1), torch.ones_like(decoded)]
-        else:
-            raise NotImplementedError('Distribution {} not supported'.format(self.decoder_distribution))
-
-    def logprob(self, inputs, obs_distribution_params):
-        if self.decoder_distribution == 'bernoulli':
-            inputs = inputs.view(
-                -1, self.input_channels, self.imsize, self.imsize
-            )
-            length = self.reconstruction_channels * self.imsize * self.imsize
-            x = inputs[:, :self.reconstruction_channels, :, :].view(-1, length)
-            # x = x.narrow(start=0, length=length,
-                 # dim=1).contiguous().view(-1, length)
-            reconstruction_x = obs_distribution_params[0]
-            log_prob = compute_bernoulli_log_prob(x, reconstruction_x) * (self.imsize*self.imsize*self.reconstruction_channels)
-            return log_prob
-        if self.decoder_distribution == 'gaussian_identity_variance':
-            inputs = inputs.narrow(start=0, length=self.imlength // 2,
-                                   dim=1).contiguous().view(-1, self.imlength // 2)
-            log_prob = -1*F.mse_loss(inputs, obs_distribution_params[0],reduction='elementwise_mean')
-            return log_prob
-        else:
-            raise NotImplementedError('Distribution {} not supported'.format(self.decoder_distribution))
-
-
 class CVAE(GaussianLatentVAE):
     def __init__(
             self,
@@ -620,100 +304,7 @@ class DeltaCVAE(CVAE):
             raise NotImplementedError('Distribution {} not supported'.format(self.decoder_distribution))
 
 
-class ACE(CVAE):
-    def __init__(
-            self,
-            representation_size,
-            architecture,
-            encoder_class=CNN,
-            decoder_class=DCNN,
-            decoder_output_activation=identity,
-            decoder_distribution='bernoulli',
-            num_labels = 0,
-            input_channels=1,
-            imsize=48,
-            init_w=1e-3,
-            min_variance=1e-3,
-            add_labels_to_latents=False,
-            hidden_init=nn.init.xavier_uniform_,
-            reconstruction_channels=3,
-            base_depth=32,
-            weight_init_gain=1.0,
-        ):
-        super().__init__(
-            representation_size,
-            architecture,
-            encoder_class,
-            decoder_class,
-            decoder_output_activation,
-            decoder_distribution,
-            num_labels,
-            input_channels,
-            imsize,
-            init_w,
-            min_variance,
-            add_labels_to_latents,
-            hidden_init,
-            reconstruction_channels,
-            base_depth,
-            weight_init_gain)
-
-        self.CVAE = CVAE(
-            representation_size,
-            architecture,
-            encoder_class,
-            decoder_class,
-            decoder_output_activation,
-            decoder_distribution,
-            num_labels,
-            input_channels,
-            imsize,
-            init_w,
-            min_variance,
-            add_labels_to_latents,
-            hidden_init,
-            reconstruction_channels,
-            base_depth,
-            weight_init_gain)
-
-        self.adversary = ConvVAE(
-            representation_size[0],
-            architecture,
-            encoder_class,
-            decoder_class,
-            decoder_output_activation,
-            decoder_distribution,
-            input_channels,
-            imsize,
-            init_w,
-            min_variance,
-            hidden_init)
-
-    def forward(self, x_t, x_0):
-        """
-        :param input:
-        :return: reconstructed input, obs_distribution_params, latent_distribution_params
-        """
-        latent_distribution_params = self.CVAE.encode(x_t, x_0)
-        latents = self.reparameterize(latent_distribution_params)
-        reconstructions, obs_distribution_params = self.CVAE.decode(latents)
-        return reconstructions, obs_distribution_params, latent_distribution_params
-
-    def encode(self, x_t, x_0, distrib=True):
-        return self.CVAE.encode(x_t, x_0, distrib)
-
-    def decode(self, latents):
-        return self.CVAE.decode(latents)
-
-    def logprob(self, inputs, obs_distribution_params):
-        return self.CVAE.logprob(inputs, obs_distribution_params)
-
-    def sample_prior(self, batch_size, x_0):
-        return self.CVAE.sample_prior(batch_size, x_0)
-
-
-
-class CDVAE(CVAE):#CHANGE SO ONLY SEES Z -> DYNAMICS INDPENEDENT OF X_0
+class CDVAE(CVAE):
     "Conditional Dynamics VAE"
     def __init__(
             self,
@@ -757,41 +348,61 @@ class CDVAE(CVAE):#CHANGE SO ONLY SEES Z -> DYNAMICS INDPENEDENT OF X_0
         )
 
         self.action_dim = action_dim
-        self.dynamics_type = dynamics_type
+        self.dynamics_type = 'nonlinear' #dynamics_type
 
-        self.globally_linear = nn.Linear(self.latent_sizes + self.action_dim, self.latent_sizes)
+        self.globally_linear = nn.Linear(self.representation_size + self.action_dim, self.representation_size)
 
-        self.locally_linear_f1 = nn.Linear(self.latent_sizes, 400)
-        self.locally_linear_f2 = nn.Linear(400, (self.latent_sizes + self.action_dim) * self.latent_sizes)
+        self.locally_linear_f1 = nn.Linear(self.representation_size, 400)
+        self.locally_linear_f2 = nn.Linear(400, (self.representation_size + self.action_dim) * self.representation_size)
 
-        self.nonlinear_dynamics_f1 = nn.Linear(self.latent_sizes + self.action_dim, 400)
-        self.nonlinear_dynamics_f2 = nn.Linear(400, self.latent_sizes)
+        self.nonlinear_dynamics_model = FlattenMlp(
+            input_size=self.representation_size + self.action_dim,
+            output_size=self.representation_size,
+            hidden_sizes=[128, 128, 128,],
+        )
 
-        self.bn_d = nn.BatchNorm1d(400)
-        self.do_d = nn.Dropout(0.2)
+        self.d_mu = nn.Linear(self.representation_size + self.latent_sizes[1], self.latent_sizes[0])
+        self.d_var = nn.Linear(self.representation_size + self.latent_sizes[1], self.latent_sizes[0])
 
-        nn.init.xavier_uniform_(self.globally_linear.weight, gain=self.gain)
+        self.action_normalizer = TorchNormalizer(self.action_dim)
 
+        nn.init.xavier_uniform_(self.globally_linear.weight, gain=self.gain)
         nn.init.xavier_uniform_(self.locally_linear_f1.weight, gain=self.gain)
         self.locally_linear_f1.bias.data.uniform_(-init_w, init_w)
-
         nn.init.xavier_uniform_(self.locally_linear_f2.weight, gain=self.gain)
         self.locally_linear_f2.bias.data.uniform_(-init_w, init_w)
 
-        nn.init.xavier_uniform_(self.nonlinear_dynamics_f1.weight, gain=self.gain)
-        self.nonlinear_dynamics_f1.bias.data.uniform_(-init_w, init_w)
+        nn.init.xavier_uniform_(self.d_mu.weight, gain=self.gain)
+        self.d_mu.bias.data.uniform_(-init_w, init_w)
+        nn.init.xavier_uniform_(self.d_var.weight, gain=self.gain)
+        self.d_var.bias.data.uniform_(-init_w, init_w)
 
-        nn.init.xavier_uniform_(self.nonlinear_dynamics_f2.weight, gain=self.gain)
-        self.nonlinear_dynamics_f2.bias.data.uniform_(-init_w, init_w)
+    def forward(self, x_t, context, actions):
+        """
+        :param input:
+        :return: reconstructed input, obs_distribution_params, latent_distribution_params
+        """
+        latent_dist_params = self.encode(x_t, context)
+        latents = self.reparameterize(latent_dist_params)
+        obs_recon, obs_dist_params = self.decode(latents)
 
+        mu = torch.cat([latent_dist_params[0], latent_dist_params[2]], dim=1)
+        pred_dist_params = self.nonlinear_dynamics(mu, actions)
+        pred = self.reparameterize(pred_dist_params)
+        pred_recon, pred_obs_dist_params = self.decode(pred)
 
-    def process_dynamics(self, latents, actions):
+        obs_recon, obs_dist_params = self.decode(latents)
+        return obs_recon, obs_dist_params, latent_dist_params, pred_recon, pred_obs_dist_params, pred_dist_params
+
+    def process_dynamics(self, latents, actions, distrib=True):
+        self.action_normalizer.update(ptu.get_numpy(actions))
+        actions = self.action_normalizer.normalize(actions)
         if self.dynamics_type == 'global':
             return self.global_linear_dynamics(latents, actions)
         if self.dynamics_type == 'local':
             return self.local_linear_dynamics(latents, actions)
         if self.dynamics_type == 'nonlinear':
-            return self.nonlinear_dynamics(latents, actions)
+            return self.nonlinear_dynamics(latents, actions, distrib)
 
     def global_linear_dynamics(self, latents, actions):
         action_obs_pair = torch.cat([latents, actions], dim=1)
@@ -808,10 +419,20 @@ class CDVAE(CVAE):#CHANGE SO ONLY SEES Z -> DYNAMICS INDPENEDENT OF X_0
             z_prime[i] = torch.matmul(dynamics[i], action_obs_pair[i])
         return z_prime
 
-    def nonlinear_dynamics(self, latents, actions):
-        action_obs_pair = torch.cat([latents, actions], dim=1)
-        z_prime = self.nonlinear_dynamics_f2(self.bn_d(self.relu(self.nonlinear_dynamics_f1(action_obs_pair))))
-        return z_prime
+    def nonlinear_dynamics(self, latents, actions, distrib=True):
+        z_c = latents[:, self.latent_sizes[0]:]
+        latent = self.nonlinear_dynamics_model(latents, actions)
+        cond_latents = torch.cat([latent, z_c], dim=1)
+        mu = self.d_mu(cond_latents)
+
+        if not distrib: return torch.cat([mu, z_c], dim=1)
+
+        if self.log_min_variance is None:
+            logvar = self.d_var(cond_latents)
+        else:
+            logvar = self.log_min_variance + torch.abs(self.d_var(cond_latents))
+        return (mu, logvar, z_c)
+
 
 class DeltaDynamicsCVAE(DeltaCVAE):#CHANGE SO ONLY SEES Z -> DYNAMICS INDPENEDENT OF X_0
     "Conditional Dynamics VAE"
@@ -912,103 +533,3 @@ class DeltaDynamicsCVAE(DeltaCVAE):#CHANGE SO ONLY SEES Z -> DYNAMICS INDPENEDEN
         action_obs_pair = torch.cat([latents, actions], dim=1)
         z_prime = self.nonlinear_dynamics_f2(self.bn_d(self.relu(self.nonlinear_dynamics_f1(action_obs_pair))))
         return z_prime
-
-class CADVAE(CVAE):
-    "Conditioned Adversarial Dynamics VAE"
-
-    def __init__(
-            self,
-            representation_size,
-            architecture,
-            action_dim,
-            dynamics_type=None,
-            encoder_class=CNN,
-            decoder_class=DCNN,
-            decoder_output_activation=identity,
-            decoder_distribution='bernoulli',
-            input_channels=1,
-            num_labels=0,
-            imsize=48,
-            init_w=1e-3,
-            min_variance=1e-3,
-            hidden_init=nn.init.xavier_uniform_,
-            add_labels_to_latents=False,
-            reconstruction_channels=3,
-            base_depth=32,
-            weight_init_gain=1.0,
-    ):
-        super().__init__(
-            representation_size,
-            architecture,
-            encoder_class,
-            decoder_class,
-            decoder_output_activation,
-            decoder_distribution,
-            num_labels,
-            input_channels,
-            imsize,
-            init_w,
-            min_variance,
-            add_labels_to_latents,
-            hidden_init,
-            reconstruction_channels,
-            base_depth,
-            weight_init_gain)
-
-        self.CDVAE = CDVAE(
-            representation_size,
-            architecture,
-            action_dim,
-            dynamics_type,
-            encoder_class,
-            decoder_class,
-            decoder_output_activation,
-            decoder_distribution,
-            input_channels,
-            num_labels,
-            imsize,
-            init_w,
-            min_variance,
-            hidden_init,
-            add_labels_to_latents,
-            reconstruction_channels,
-            base_depth,
-            weight_init_gain)
-
-        self.adversary = ConvVAE(
-            representation_size[0],
-            architecture,
-            encoder_class,
-            decoder_class,
-            decoder_output_activation,
-            decoder_distribution,
-            input_channels,
-            imsize,
-            init_w,
-            min_variance,
-            hidden_init)
-
-    def forward(self, x_t, x_0):
-        """
-        :param input:
-        :return: reconstructed input, obs_distribution_params, latent_distribution_params
-        """
-        latent_distribution_params = self.CDVAE.encode(x_t, x_0)
-        latents = self.CDVAE.reparameterize(latent_distribution_params)
-        reconstructions, obs_distribution_params = self.CDVAE.decode(latents)
-        return reconstructions, obs_distribution_params, latent_distribution_params
-
-    def encode(self, x_t, x_0, distrib=True):
-        return self.CDVAE.encode(x_t, x_0, distrib)
-
-    def decode(self, latents, conditioning=False):
-        return self.CDVAE.decode(latents, conditioning)
-
-    def logprob(self, inputs, obs_distribution_params):
-        return self.CDVAE.logprob(inputs, obs_distribution_params)
-
-    def sample_prior(self, batch_size, x_0):
-        return self.CDVAE.sample_prior(batch_size, x_0)
-
-    def process_dynamics(self, latents, actions):
-        return self.CDVAE.process_dynamics(latents, actions)
diff --git a/railrl/torch/vae/conditional_vae_trainer.py b/railrl/torch/vae/conditional_vae_trainer.py
index fb520b3..a932544 100644
--- a/railrl/torch/vae/conditional_vae_trainer.py
+++ b/railrl/torch/vae/conditional_vae_trainer.py
@@ -302,6 +302,7 @@ class DeltaCVAETrainer(ConditionalConvVAETrainer):
             log_interval=0,
             beta=0.5,
             beta_schedule=None,
+            context_weight=1,
             lr=None,
             do_scatterplot=False,
             normalize=False,
@@ -344,6 +345,7 @@ class DeltaCVAETrainer(ConditionalConvVAETrainer):
             start_skew_epoch,
             weight_decay,
         )
+        self.context_weight = context_weight
         self.optimizer = optim.Adam(self.model.parameters(),
             lr=self.lr,
             weight_decay=weight_decay,
@@ -358,7 +360,7 @@ class DeltaCVAETrainer(ConditionalConvVAETrainer):
         log_prob = self.model.logprob(batch["x_t"], obs_distribution_params)
         env_log_prob = self.model.logprob(batch["env"], env_distribution_params)
         kle = self.model.kl_divergence(latent_distribution_params)
-        loss = -1 * (log_prob + env_log_prob) + beta * kle
+        loss = -1 * (log_prob + self.context_weight * env_log_prob) + beta * kle
 
         self.eval_statistics['epoch'] = epoch
         self.eval_statistics['beta'] = beta
@@ -441,46 +443,41 @@ class DeltaCVAETrainer(ConditionalConvVAETrainer):
         save_dir = osp.join(self.log_dir, 's%d.png' % epoch)
         save_image(comparison.data.cpu(), save_dir, nrow=8)
 
-
 class CDVAETrainer(CVAETrainer):
 
-    def state_linearity_loss(self, x_t, x_next, env, actions):
-        latent_obs = self.model.encode(x_t, env, distrib=False)
-        latent_next_obs = self.model.encode(x_next, env, distrib=False)
-        predicted_latent = self.model.process_dynamics(latent_obs, actions)
-        return torch.norm(predicted_latent - latent_next_obs) ** 2 / self.batch_size
-
-    def state_distance_loss(self, x_t, x_next, env):
-        latent_obs = self.model.encode(x_t, env, distrib=False)
-        latent_next_obs = self.model.encode(x_next, env, distrib=False)
-        return torch.norm(latent_obs - latent_next_obs) ** 2 / self.batch_size
+    # def process_dynamics(self, x_t, x_next, env, actions):
+    #     latent_obs = self.model.encode(x_t, env, distrib=False)
+    #     predicted_latent = self.model.process_dynamics(latent_obs, actions)
+    #     self.model.decode(predicted_latent, env)
+    #     return torch.norm(predicted_latent - latent_next_obs) ** 2 / self.batch_size
 
     def compute_loss(self, epoch, batch, test=False):
         prefix = "test/" if test else "train/"
         beta = float(self.beta_schedule.get_value(epoch))
-        reconstructions, obs_distribution_params, latent_distribution_params = self.model(batch["x_t"], batch["env"])
-        log_prob = self.model.logprob(batch["x_t"], obs_distribution_params)
-        kle = self.model.kl_divergence(latent_distribution_params)
-        state_distance_loss = self.state_distance_loss(batch["x_t"], batch["x_next"], batch["env"])
-        dynamics_loss = self.state_linearity_loss(
-            batch["x_t"], batch["x_next"], batch["env"], batch["actions"]
-        )
-
-        loss = -1 * log_prob + beta * kle + self.linearity_weight * dynamics_loss + self.distance_weight * state_distance_loss
+        obs_recon, obs_dist_params, latent_dist_params,\
+            pred_recon, pred_obs_dist_params, pred_dist_params = self.model(batch["x_t"], batch["env"], batch["actions"])
+        log_prob = self.model.logprob(batch["x_t"], obs_dist_params)
+        pred_log_prob = self.model.logprob(batch["x_next"], pred_obs_dist_params)
+        kle = self.model.kl_divergence(latent_dist_params)
+        pred_kle = self.model.kl_divergence(pred_dist_params)
+        loss = -1 * (log_prob + pred_log_prob) + beta * (kle + pred_kle)
+        # + self.linearity_weight * dynamics_loss + self.distance_weight * state_distance_loss
 
         self.eval_statistics['epoch'] = epoch
         self.eval_statistics['beta'] = beta
         self.eval_statistics[prefix + "losses"].append(loss.item())
         self.eval_statistics[prefix + "log_probs"].append(log_prob.item())
+        self.eval_statistics[prefix + "pred_log_probs"].append(pred_log_prob.item())
         self.eval_statistics[prefix + "kles"].append(kle.item())
-        self.eval_statistics[prefix + "dynamics_loss"].append(dynamics_loss.item())
-        self.eval_statistics[prefix + "distance_loss"].append(state_distance_loss.item())
+        self.eval_statistics[prefix + "pred_kles"].append(pred_kle.item())
+        #self.eval_statistics[prefix + "dynamics_loss"].append(dynamics_loss.item())
+        #self.eval_statistics[prefix + "distance_loss"].append(state_distance_loss.item())
 
-        encoder_mean = self.model.get_encoding_from_latent_distribution_params(latent_distribution_params)
+        encoder_mean = self.model.get_encoding_from_latent_distribution_params(latent_dist_params)
         z_data = ptu.get_numpy(encoder_mean.cpu())
         for i in range(len(z_data)):
             self.eval_data[prefix + "zs"].append(z_data[i, :])
-        self.eval_data[prefix + "last_batch"] = (batch, reconstructions)
+        self.eval_data[prefix + "last_batch"] = (batch, obs_recon)
 
         return loss
 
@@ -497,7 +494,7 @@ class CDVAETrainer(CVAETrainer):
         all_imgs.append(pred_curr.view(3, size, size).transpose(1, 2))
 
         for i in range(n - 1):
-            latent_state = self.model.process_dynamics(latent_state.reshape(1, -1), act[i].reshape(1, -1))
+            latent_state = self.model.process_dynamics(latent_state.reshape(1, -1), act[i].reshape(1, -1), distrib=False)
             pred_curr = self.model.decode(latent_state)[0]
             all_imgs.append(pred_curr.view(3, size, size).transpose(1, 2))
 
@@ -540,6 +537,104 @@ class CDVAETrainer(CVAETrainer):
         save_dir = osp.join(self.log_dir, 'r%d.png' % epoch)
         save_image(comparison.data.cpu(), save_dir, nrow=n)
 
+# class CDVAETrainer(CVAETrainer):
+
+#     def state_linearity_loss(self, x_t, x_next, env, actions):
+#         latent_obs = self.model.encode(x_t, env, distrib=False)
+#         latent_next_obs = self.model.encode(x_next, env, distrib=False)
+#         predicted_latent = self.model.process_dynamics(latent_obs, actions)
+#         return torch.norm(predicted_latent - latent_next_obs) ** 2 / self.batch_size
+
+#     def state_distance_loss(self, x_t, x_next, env):
+#         latent_obs = self.model.encode(x_t, env, distrib=False)
+#         latent_next_obs = self.model.encode(x_next, env, distrib=False)
+#         return torch.norm(latent_obs - latent_next_obs) ** 2 / self.batch_size
+
+#     def compute_loss(self, epoch, batch, test=False):
+#         prefix = "test/" if test else "train/"
+#         beta = float(self.beta_schedule.get_value(epoch))
+#         reconstructions, obs_distribution_params, latent_distribution_params = self.model(batch["x_t"], batch["env"])
+#         log_prob = self.model.logprob(batch["x_t"], obs_distribution_params)
+#         kle = self.model.kl_divergence(latent_distribution_params)
+#         state_distance_loss = self.state_distance_loss(batch["x_t"], batch["x_next"], batch["env"])
+#         dynamics_loss = self.state_linearity_loss(
+#             batch["x_t"], batch["x_next"], batch["env"], batch["actions"]
+#         )
+
+#         loss = -1 * log_prob + beta * kle + self.linearity_weight * dynamics_loss + self.distance_weight * state_distance_loss
+
+#         self.eval_statistics['epoch'] = epoch
+#         self.eval_statistics['beta'] = beta
+#         self.eval_statistics[prefix + "losses"].append(loss.item())
+#         self.eval_statistics[prefix + "log_probs"].append(log_prob.item())
+#         self.eval_statistics[prefix + "kles"].append(kle.item())
+#         self.eval_statistics[prefix + "dynamics_loss"].append(dynamics_loss.item())
+#         self.eval_statistics[prefix + "distance_loss"].append(state_distance_loss.item())
+
+#         encoder_mean = self.model.get_encoding_from_latent_distribution_params(latent_distribution_params)
+#         z_data = ptu.get_numpy(encoder_mean.cpu())
+#         for i in range(len(z_data)):
+#             self.eval_data[prefix + "zs"].append(z_data[i, :])
+#         self.eval_data[prefix + "last_batch"] = (batch, reconstructions)
+
+#         return loss
+
+#     def dump_dynamics(self, batch, epoch):
+#         self.model.eval()
+#         state = batch['episode_obs']
+#         act = batch['episode_acts']
+#         size = self.model.imsize
+#         n = min(state.size(0), 8)
+
+#         all_imgs = [state[i].reshape(3, size, size).transpose(1, 2) for i in range(n)]
+#         latent_state = self.model.encode(state[0].reshape(1, -1), state[0].reshape(1, -1), distrib=False)
+#         pred_curr = self.model.decode(latent_state)[0]
+#         all_imgs.append(pred_curr.view(3, size, size).transpose(1, 2))
+
+#         for i in range(n - 1):
+#             latent_state = self.model.process_dynamics(latent_state.reshape(1, -1), act[i].reshape(1, -1))
+#             pred_curr = self.model.decode(latent_state)[0]
+#             all_imgs.append(pred_curr.view(3, size, size).transpose(1, 2))
+
+#         all_imgs = torch.stack(all_imgs)
+#         save_dir = osp.join(self.log_dir, 'dynamics%d.png' % epoch)
+#         save_image(
+#             all_imgs.data,
+#             save_dir,
+#             nrow=n,
+#         )
+
+#     def dump_reconstructions(self, epoch):
+#         batch, reconstructions = self.eval_data["test/last_batch"]
+#         self.dump_dynamics(batch, epoch)
+#         obs = batch["x_t"]
+#         env = batch["env"]
+#         n = min(obs.size(0), 8)
+#         comparison = torch.cat([
+#             env[:n].narrow(start=0, length=self.imlength, dim=1)
+#                 .contiguous().view(
+#                 -1,
+#                 3,
+#                 self.imsize,
+#                 self.imsize
+#             ).transpose(2, 3),
+#             obs[:n].narrow(start=0, length=self.imlength, dim=1)
+#                 .contiguous().view(
+#                 -1,
+#                 3,
+#                 self.imsize,
+#                 self.imsize
+#             ).transpose(2, 3),
+#             reconstructions.view(
+#                 self.batch_size,
+#                 3,
+#                 self.imsize,
+#                 self.imsize,
+#             )[:n].transpose(2, 3)
+#         ])
+#         save_dir = osp.join(self.log_dir, 'r%d.png' % epoch)
+#         save_image(comparison.data.cpu(), save_dir, nrow=n)
+
 class DeltaDynamicsCVAETrainer(CDVAETrainer):
 
     def compute_loss(self, epoch, batch, test=False):
diff --git a/railrl/torch/vae/experiment_grader.py b/railrl/torch/vae/experiment_grader.py
new file mode 100644
index 0000000..0d2fe7d
--- /dev/null
+++ b/railrl/torch/vae/experiment_grader.py
@@ -0,0 +1,81 @@
+import pickle
+from tkinter import *
+import numpy as np
+import tkinter as tk
+from PIL import Image, ImageTk
+import random
+
+def clean_photo(photo):
+    img = photo.reshape((3, 48, 48)).transpose()
+    img = (255 * img).astype(np.uint8)
+    im = Image.fromarray(img).resize((350, 350))
+    photo = ImageTk.PhotoImage(image=im)
+    return photo
+
+def load_dataset(filename):
+    combined_data = []
+    for epoch in range(1,5):
+        try:
+            temp_path = dataset_path + "video_{0}_env.p".format(epoch)
+            dataset = list(pickle.load(open(temp_path, "rb")))
+        except:
+            return combined_data
+
+        for episode in range(10):
+            data = dataset[episode]['full_observations'][-1]
+            last_img = clean_photo(data['image_achieved_goal'])
+            combined_data.append(last_img)
+            goal_img = clean_photo(data['image_desired_goal'])
+            combined_data.append(goal_img)
+    return combined_data
+
+index = 0
+if __name__ == "__main__":
+    index = 0
+    root = Tk()
+    dataset_path = "/home/ashvin/data/s3doodad/ashvin/corl2019/robot/test1/run50/id0/"
+    dataset = load_dataset(dataset_path)
+    keys = [i for i in range(len(dataset))]
+    random.shuffle(keys)
+    positions = [0 for i in range(len(dataset))]
+    
+    frame = Frame(root, bd=2, relief=SUNKEN)
+    frame.grid_rowconfigure(0, weight=1)
+    frame.grid_columnconfigure(0, weight=1)
+    xscroll = Scrollbar(frame, orient=HORIZONTAL)
+    xscroll.grid(row=1, column=0, sticky=E+W)
+    yscroll = Scrollbar(frame)
+    yscroll.grid(row=0, column=1, sticky=N+S)
+    canvas = Canvas(frame, bd=0, xscrollcommand=xscroll.set, yscrollcommand=yscroll.set)
+    canvas.grid(row=0, column=0, sticky=N+S+E+W)
+    xscroll.config(command=canvas.xview)
+    yscroll.config(command=canvas.yview)
+    frame.pack(fill=BOTH,expand=1)
+
+    canvas.create_image(0,0,image=dataset[keys[index]],anchor="nw")
+    canvas.config(scrollregion=canvas.bbox(ALL))
+
+    def calc_distances(positions):
+        dist = []
+        for i in range(len(positions) // 2):
+            pos1 = positions[2 * i]
+            pos2 = positions[2 * i + 1]
+            dist.append(np.linalg.norm(pos2 - pos1))
+        print(dist)
+
+    #function to be called when mouse is clicked
+    def printcoords(event):
+        global index
+        global positions
+        global keys
+        positions[keys[index]] = np.array([event.x, event.y])
+        index += 1
+        if index == len(dataset):
+            calc_distances(positions)
+        else:
+            canvas.create_image(0,0,image=dataset[keys[index]],anchor="nw")
+    #mouseclick event
+    canvas.bind("<Button 1>",printcoords)
+
+    root.mainloop()
+
diff --git a/railrl/torch/vae/online_vae_offpolicy_algorithm.py b/railrl/torch/vae/online_vae_offpolicy_algorithm.py
index 249d092..3ab196b 100644
--- a/railrl/torch/vae/online_vae_offpolicy_algorithm.py
+++ b/railrl/torch/vae/online_vae_offpolicy_algorithm.py
@@ -16,6 +16,9 @@ import numpy as np
 from railrl.core.logging import add_prefix
 from railrl.misc.asset_loader import load_local_or_remote_file
 import torch
+from collections import OrderedDict
+
+from railrl.torch.vae.conditional_conv_vae import DeltaCVAE
 
 class OnlineVaeOffpolicyAlgorithm(TorchBatchRLAlgorithm):
 
@@ -53,15 +56,27 @@ class OnlineVaeOffpolicyAlgorithm(TorchBatchRLAlgorithm):
 
         self.dataset_path = dataset_path
         if self.dataset_path:
-            self.load_dataset(dataset_path)
+            for d in dataset_path:
+                self.load_dataset(d)
 
         # train Q and policy rl_offpolicy_num_training_steps times
         self.rl_offpolicy_num_training_steps = rl_offpolicy_num_training_steps
 
     def pretrain(self):
+        logger.push_tabular_prefix("pretrain_q/")
+        # import ipdb; ipdb.set_trace()
         for _ in range(self.rl_offpolicy_num_training_steps):
             train_data = self.replay_buffer.random_batch(self.batch_size)
+
+            # hack to force logging
+            self.trainer._base_trainer._need_to_update_eval_statistics = True
+            self.trainer._base_trainer.eval_statistics = OrderedDict()
+            
             self.trainer.train(train_data)
+            logger.record_dict(self.trainer.get_diagnostics())
+            logger.dump_tabular(with_prefix=True, with_timestamp=False)
+        logger.pop_tabular_prefix()
+        # import ipdb; ipdb.set_trace()
 
     def load_dataset(self, dataset_path):
         dataset = load_local_or_remote_file(dataset_path)
@@ -77,23 +92,34 @@ class OnlineVaeOffpolicyAlgorithm(TorchBatchRLAlgorithm):
 
         self.vae.eval()
         for n in range(N):
+            #i,j = np.random.randint(0, N), np.random.randint(0, H)
             x0 = ptu.from_numpy(dataset['env'][n:n+1, :] / 255.0)
             x = ptu.from_numpy(observations[n, :, :] / 255.0)
-            latents = self.vae.encode(x, x0, distrib=False)
 
-            r1, r2 = self.vae.latent_sizes
-            conditioning = latents[0, r1:]
-            goal = torch.cat([ptu.randn(self.vae.latent_sizes[0]), conditioning])
+            if isinstance(self.vae, DeltaCVAE):
+                latents = self.vae.encode(x, x0, distrib=False)
+
+                r1, r2 = self.vae.latent_sizes
+                conditioning = latents[0, r1:]
+                #goal_cond = ptu.from_numpy(dataset['env'][i:i+1, :] / 255.0)
+                #goal_img = ptu.from_numpy(observations[i, j, :] / 255.0).reshape(goal_cond.shape[0], goal_cond.shape[1])
+                #goal = self.vae.encode(goal_img, goal_cond, distrib=False)
+
+                goal = torch.cat([ptu.randn(self.vae.latent_sizes[0]), conditioning])
+            else: # normal VAE
+                latents = self.vae.encode(x)[0]
+                goal = ptu.randn(self.vae.representation_size)
+
             goal = ptu.get_numpy(goal) # latents[-1, :]
 
             latents = ptu.get_numpy(latents)
             latent_delta = latents - goal
             distances = np.zeros((H - 1, 1))
             for i in range(H - 1):
-                distances[i, 0] = np.linalg.norm(latent_delta[i+1, :])
+                distances[i, 0] = np.linalg.norm(latent_delta[i + 1, :])
 
             terminals = np.zeros((H - 1, 1))
-            # terminals[-1, 0] = 1
+            #terminals[-1, 0] = 1
             path = dict(
                 observations=[],
                 actions=actions[n, :H-1, :],
diff --git a/railrl/torch/vae/vae_trainer.py b/railrl/torch/vae/vae_trainer.py
index aa09b2e..5b79435 100644
--- a/railrl/torch/vae/vae_trainer.py
+++ b/railrl/torch/vae/vae_trainer.py
@@ -984,43 +984,39 @@ class DeltaCVAETrainer(ConditionalConvVAETrainer):
 
 class CDVAETrainer(CVAETrainer):
 
-    def state_linearity_loss(self, x_t, x_next, env, actions):
+    def process_dynamics(self, x_t, x_next, env, actions):
         latent_obs = self.model.encode(x_t, env, distrib=False)
-        latent_next_obs = self.model.encode(x_next, env, distrib=False)
         predicted_latent = self.model.process_dynamics(latent_obs, actions)
+        self.model.decode(predicted_latent, env)
         return torch.norm(predicted_latent - latent_next_obs) ** 2 / self.batch_size
 
-    def state_distance_loss(self, x_t, x_next, env):
-        latent_obs = self.model.encode(x_t, env, distrib=False)
-        latent_next_obs = self.model.encode(x_next, env, distrib=False)
-        return torch.norm(latent_obs - latent_next_obs) ** 2 / self.batch_size
-
     def compute_loss(self, epoch, batch, test=False):
         prefix = "test/" if test else "train/"
         beta = float(self.beta_schedule.get_value(epoch))
-        reconstructions, obs_distribution_params, latent_distribution_params = self.model(batch["x_t"], batch["env"])
-        log_prob = self.model.logprob(batch["x_t"], obs_distribution_params)
-        kle = self.model.kl_divergence(latent_distribution_params)
-        state_distance_loss = self.state_distance_loss(batch["x_t"], batch["x_next"], batch["env"])
-        dynamics_loss = self.state_linearity_loss(
-            batch["x_t"], batch["x_next"], batch["env"], batch["actions"]
-        )
-
-        loss = -1 * log_prob + beta * kle + self.linearity_weight * dynamics_loss + self.distance_weight * state_distance_loss
+        obs_recon, obs_dist_params, latent_dist_params,\
+            pred_recon, pred_obs_dist_params, pred_dist_params = self.model(batch["x_t"], batch["env"], batch["actions"])
+        log_prob = self.model.logprob(batch["x_t"], obs_dist_params)
+        pred_log_prob = self.model.logprob(batch["x_next"], pred_obs_dist_params)
+        kle = self.model.kl_divergence(latent_dist_params)
+        pred_kle = self.model.kl_divergence(pred_dist_params)
+        loss = -1 * (log_prob + pred_log_prob) + beta * (kle + pred_kle)
+        # + self.linearity_weight * dynamics_loss + self.distance_weight * state_distance_loss
 
         self.eval_statistics['epoch'] = epoch
         self.eval_statistics['beta'] = beta
         self.eval_statistics[prefix + "losses"].append(loss.item())
         self.eval_statistics[prefix + "log_probs"].append(log_prob.item())
+        self.eval_statistics[prefix + "pred_log_probs"].append(pred_log_prob.item())
         self.eval_statistics[prefix + "kles"].append(kle.item())
-        self.eval_statistics[prefix + "dynamics_loss"].append(dynamics_loss.item())
-        self.eval_statistics[prefix + "distance_loss"].append(state_distance_loss.item())
+        self.eval_statistics[prefix + "pred_kles"].append(pred_kle.item())
+        #self.eval_statistics[prefix + "dynamics_loss"].append(dynamics_loss.item())
+        #self.eval_statistics[prefix + "distance_loss"].append(state_distance_loss.item())
 
-        encoder_mean = self.model.get_encoding_from_latent_distribution_params(latent_distribution_params)
+        encoder_mean = self.model.get_encoding_from_latent_distribution_params(latent_dist_params)
         z_data = ptu.get_numpy(encoder_mean.cpu())
         for i in range(len(z_data)):
             self.eval_data[prefix + "zs"].append(z_data[i, :])
-        self.eval_data[prefix + "last_batch"] = (batch, reconstructions)
+        self.eval_data[prefix + "last_batch"] = (batch, obs_recon)
 
         return loss
 
@@ -1037,7 +1033,7 @@ class CDVAETrainer(CVAETrainer):
         all_imgs.append(pred_curr.view(3, size, size).transpose(1, 2))
 
         for i in range(n - 1):
-            latent_state = self.model.process_dynamics(latent_state.reshape(1, -1), act[i].reshape(1, -1))
+            latent_state = self.model.process_dynamics(latent_state.reshape(1, -1), act[i].reshape(1, -1), distrib=False)
             pred_curr = self.model.decode(latent_state)[0]
             all_imgs.append(pred_curr.view(3, size, size).transpose(1, 2))
 
diff --git a/railrl/torch/vae/visualize_vae.py b/railrl/torch/vae/visualize_vae.py
index d1828fa..2d1cfa8 100644
--- a/railrl/torch/vae/visualize_vae.py
+++ b/railrl/torch/vae/visualize_vae.py
@@ -1,17 +1,18 @@
+import sys
+sys.path.remove('/opt/ros/kinetic/lib/python2.7/dist-packages')
 import tkinter as tk
-
 from railrl.misc.asset_loader import sync_down
 from railrl.misc.asset_loader import load_local_or_remote_file
+import torch
 import cv2
 import pickle
 import numpy as np
-
+import io
+import skvideo.io
 from PIL import Image, ImageTk
 from railrl.torch import pytorch_util as ptu
 from railrl.data_management.dataset  import \
         TrajectoryDataset, ImageObservationDataset, InitialObservationDataset
-import torch
-
 from railrl.data_management.images import normalize_image, unnormalize_image
 
 def load_vae(vae_file):
@@ -60,7 +61,7 @@ class VAEVisualizer(object):
 
         # import pdb; pdb.set_trace()
         self.new_train_image()
-
+        self.sweep()
         self.master.after(0, self.update)
 
     def load_dataset(filename, test_p=0.9):
@@ -93,20 +94,20 @@ class VAEVisualizer(object):
 
     def new_image(self, test=True):
         if test:
-            ind = np.random.randint(0, len(self.test_dataset), (1,))
-            self.sample = self.test_dataset[ind, :] / 255
+            self.batch = self.test_dataset.random_batch(1)
         else:
-            ind = np.random.randint(0, len(self.train_dataset), (1,))
-            self.sample = self.train_dataset[ind, :] / 255
-        img = self.sample.reshape((3, 48, 48)).transpose()
-        img = (255 * img).astype(np.uint8)
+            self.batch = self.train_dataset.random_batch(1)
+        self.sample = self.batch["x_t"]
+        #self.sample = self.train_dataset[ind, :] / 255
+        img = unnormalize_image(ptu.get_numpy(self.sample).reshape((3, 48, 48)).transpose())
+        #img = self.sample.reshape((3, 48, 48)).transpose()
+        #img = (255 * img).astype(np.uint8)
         # img = img.astype(np.uint8)
         self.im = Image.fromarray(img)
-        self.leftphoto = ImageTk.PhotoImage(image=self.im)
-        self.leftpanel.create_image(0,0,image=self.leftphoto,anchor=tk.NW)
+        #self.leftphoto = ImageTk.PhotoImage(image=self.im)
+        #self.leftpanel.create_image(0,0,image=self.leftphoto,anchor=tk.NW)
 
-        batch = ptu.from_numpy(self.sample)
-        self.mu, self.logvar = self.vae.encode(batch)
+        self.mu, self.logvar = self.vae.encode(self.sample)
         self.z = self.mu
         self.mean = ptu.get_numpy(self.z).flatten()
         self.recon_batch = self.vae.decode(self.z)[0]
@@ -137,53 +138,70 @@ class VAEVisualizer(object):
         img = (255 * img).astype(np.uint8)
         # img = img.astype(np.uint8)
         self.rightim = Image.fromarray(img)
-        self.rightphoto = ImageTk.PhotoImage(image=self.rightim)
-        self.rightpanel.create_image(0,0,image=self.rightphoto,anchor=tk.NW)
+        #self.rightphoto = ImageTk.PhotoImage(image=self.rightim)
+        #self.rightpanel.create_image(0,0,image=self.rightphoto,anchor=tk.NW)
 
     def get_batch(self, train=True):
         dataset = self.train_dataset if train else self.test_dataset
         ind = np.random.randint(0, len(dataset), self.batch_size)
         samples = dataset[ind, :]
-        # if self.normalize:
-        #     samples = ((samples - self.train_data_mean) + 1) / 2
         return ptu.from_numpy(samples)
 
+
     def sweep_element(self):
-        if self.sweep_i < self.vae.representation_size:
-            i = self.sweep_i
-            if self.sweep_k > 10:
-                self.mean[i] = self.original_mean[i]
-                self.sliders[i].set(self.original_mean[i])
-                self.check_change()
-                self.sweep_i += 1
-                self.sweep_k = 0
-                self.master.after(100, self.sweep_element)
-            else:
-                v = -2.5 + 0.5 * self.sweep_k
-                self.mean[i] = v
-                self.sliders[i].set(v)
-                self.check_change()
-                self.sweep_k += 1
-                self.master.after(100, self.sweep_element)
-        else: # done!
-            self.mean = self.original_mean
+        data = [np.copy(self.mean)]
+        self.rightim.save('/home/ashvin/ros_ws/src/railrl-private/visualizer/vae/img_0.jpg')
+        for i in range(40):
+            for k in self.sweep_i:
+                if np.random.uniform() < 0.5:
+                    sign = 1
+                else:
+                    sign = -1
+                if self.mean[k] >= 3:
+                    sign = -1
+                if self.mean[k] < -3:
+                    sign = 1
+                self.mean[k] += sign * 0.25
+                self.sliders[k].set(self.mean[k])
+            self.check_change()
+            self.rightim.save('/home/ashvin/ros_ws/src/railrl-private/visualizer/vae/img_{}.jpg'.format(i + 1))
+            data.append(np.copy(self.mean))
+            self.master.after(100, self.sweep_element)
+        np.save('/home/ashvin/ros_ws/src/railrl-private/visualizer/vae/latents.npy', np.array(data))
+        self.mean = self.original_mean
+
 
     def sweep(self):
         self.original_mean = self.mean.copy()
-        self.sweep_i = 0
+        self.sweep_i = [i for i in range(self.vae.representation_size)] #temp
         self.sweep_k = 0
         self.master.after(100, self.sweep_element)
-        # self.sweep_element()
-        # for i in range(self.vae.representation_size):
-        #     t = self.mean[i]
-        #     for j in np.linspace(-2.5, 2.5, 5):
-        #         self.sliders[i].set(j)
-        #         self.mean[i] = j
-        #         self.master.after(10, self.check_change)
-        #         # self.check_change()
-        #         # time.sleep(0.1)
-        #     self.mean[i] = t
-        #     self.sliders[i].set(self.mean[i])
+
+
+    # def sweep_element(self):
+    #     for i in self.sweep_i:
+    #         if self.sweep_k > 10:
+    #             self.mean[i] = self.original_mean[i]
+    #             self.sliders[i].set(self.original_mean[i])
+    #             self.check_change()
+    #             self.sweep_i += 1
+    #             self.sweep_k = 0
+    #             self.master.after(100, self.sweep_element)
+    #         else:
+    #             v = -2.5 + 0.5 * self.sweep_k
+    #             self.mean[i] = v
+    #             self.sliders[i].set(v)
+    #             self.check_change()
+    #             self.sweep_k += 1
+    #             self.master.after(100, self.sweep_element)
+    #     else: # done!
+    #         self.mean = self.original_mean
+
+    # def sweep(self):
+    #     self.original_mean = self.mean.copy()
+    #     self.sweep_i = [1,2,3] #Important latents
+    #     self.sweep_k = 0
+    #     self.master.after(100, self.sweep_element)
 
 class ConditionalVAEVisualizer(object):
     def __init__(self, path, train_dataset, test_dataset):
@@ -196,7 +214,7 @@ class ConditionalVAEVisualizer(object):
         self.master = tk.Tk()
 
         self.sliders = []
-        for i in range(self.vae.latent_size):
+        for i in range(self.vae.representation_size):
             w = tk.Scale(self.master, from_=-3, to=3, orient=tk.HORIZONTAL, resolution=0.01,)
             x, y = (i % 4), 13 + (i // 4)
             w.grid(row=x, column=y)
@@ -219,12 +237,14 @@ class ConditionalVAEVisualizer(object):
         self.leftpanel.grid(row=0, column=4, columnspan=4, rowspan=4)
         self.rightpanel.grid(row=0, column=8, columnspan=4, rowspan=4)
 
-        self.last_mean = np.zeros((self.vae.latent_size))
+        self.last_mean = np.zeros((self.vae.representation_size))
 
         # import pdb; pdb.set_trace()
+        self.vae.eval()
+        #self.new_train_image()
         self.new_train_image()
-
-        self.master.after(0, self.update)
+        self.master.after(100, self.update)
+        self.sweep()
 
     def load_dataset(filename, test_p=0.9):
         dataset = np.load(filename).item()
@@ -243,7 +263,7 @@ class ConditionalVAEVisualizer(object):
         return train_dataset, test_dataset
 
     def update(self):
-        for i in range(self.vae.latent_size):
+        for i in range(self.vae.representation_size):
             self.mean[i] = self.sliders[i].get()
         self.check_change()
 
@@ -303,7 +323,7 @@ class ConditionalVAEVisualizer(object):
             self.last_mean = self.mean.copy()
 
     def update_sliders(self):
-        for i in range(self.vae.latent_size):
+        for i in range(self.vae.representation_size):
             self.sliders[i].set(self.mean[i])
 
     def update_reconstruction(self):
@@ -322,49 +342,63 @@ class ConditionalVAEVisualizer(object):
         #     samples = ((samples - self.train_data_mean) + 1) / 2
         return ptu.from_numpy(samples)
 
+    # def sweep_element(self):
+    #     data = [np.copy(self.mean)]
+    #     self.rightim.save('/home/ashvin/ros_ws/src/railrl-private/visualizer/ccvae/img_0.jpg')
+    #     for i in range(40):
+    #         for k in self.sweep_i:
+    #             if np.random.uniform() < 0.5:
+    #                 sign = 1
+    #             else:
+    #                 sign = -1
+    #             if self.mean[k] >= 3:
+    #                 sign = -1
+    #             if self.mean[k] < -3:
+    #                 sign = 1
+    #             self.mean[k] += sign * 0.25
+    #             self.sliders[k].set(self.mean[k])
+    #         self.check_change()
+    #         self.rightim.save('/home/ashvin/ros_ws/src/railrl-private/visualizer/ccvae/img_{}.jpg'.format(i + 1))
+    #         data.append(np.copy(self.mean))
+    #         self.master.after(100, self.sweep_element)
+    #     np.save('/home/ashvin/ros_ws/src/railrl-private/visualizer/ccvae/latents.npy', np.array(data))
+    #     self.mean = self.original_mean
+
     def sweep_element(self):
-        if self.sweep_i < self.vae.representation_size[0]:
-            i = self.sweep_i
+        self.rightim.save('/home/ashvin/ros_ws/src/railrl-private/visualizer/ccvae/img_0.jpg')
+        while self.sweep_i < self.vae.latent_sizes[0]:
             if self.sweep_k > 10:
-                self.mean[i] = self.original_mean[i]
-                self.sliders[i].set(self.original_mean[i])
+                self.mean[self.sweep_i] = self.original_mean[self.sweep_i]
+                self.sliders[self.sweep_i].set(self.original_mean[self.sweep_i])
                 self.check_change()
                 self.sweep_i += 1
                 self.sweep_k = 0
                 self.master.after(100, self.sweep_element)
             else:
                 v = -2.5 + 0.5 * self.sweep_k
-                self.mean[i] = v
-                self.sliders[i].set(v)
+                self.mean[self.sweep_i] = v
+                self.sliders[self.sweep_i].set(v)
                 self.check_change()
                 self.sweep_k += 1
                 self.master.after(100, self.sweep_element)
+                self.rightim.save('/home/ashvin/ros_ws/src/railrl-private/visualizer/ccvae/img_{}.jpg'.format(self.sweep_i * self.sweep_k))
         else: # done!
             self.mean = self.original_mean
 
-    def sample_prior(self):
-        self.z = self.vae.sample_prior(1, self.batch["env"])
-        self.recon_batch = self.vae.decode(self.z)[0]
-        self.mean = ptu.get_numpy(self.z).flatten()
-        self.update_sliders()
-        self.check_change()
 
     def sweep(self):
         self.original_mean = self.mean.copy()
-        self.sweep_i = 0
+        self.sweep_i = 0 #[i for i in range(self.vae.latent_sizes[0])] #temp
         self.sweep_k = 0
         self.master.after(100, self.sweep_element)
-        # self.sweep_element()
-        # for i in range(self.vae.representation_size):
-        #     t = self.mean[i]
-        #     for j in np.linspace(-2.5, 2.5, 5):
-        #         self.sliders[i].set(j)
-        #         self.mean[i] = j
-        #         self.master.after(10, self.check_change)
-        #         # self.check_change()
-        #         # time.sleep(0.1)
-        #     self.mean[i] = t
-        #     self.sliders[i].set(self.mean[i])
+
+
+    def sample_prior(self):
+        self.z = self.vae.sample_prior(1, self.batch["env"]) #self.batch["context"]?
+        self.recon_batch = self.vae.decode(self.z)[0]
+        self.mean = ptu.get_numpy(self.z).flatten()
+        self.update_sliders()
+        self.check_change()
 
 def load_dataset(filename, test_p=0.9):
         dataset = load_local_or_remote_file(filename).item()
@@ -408,9 +442,11 @@ if __name__ == "__main__":
     # train_data, test_data, info = generate_vae_dataset(
     #     N=10000
     # )
-    data_path = "/tmp/SawyerMultiobjectEnv_N100000_sawyer_init_camera_zoomed_in_imsize48_random_oracle_split_0.npy"
+    data_path = "/home/ashvin/data/pusher_pucks/puck_red.npy"
     train_data, test_data = load_dataset(data_path)
-    model_path = "/home/khazatsky/rail/data/rail-khazatsky/sasha/PCVAE/dynamics-cvae/run106/id0/vae.pkl"
+    model_path = "/home/ashvin/data/s3doodad/ashvin/corl2019/robot/test1/run32/id0/vae.pkl"
     ConditionalVAEVisualizer(model_path, train_data, test_data)
+    # model_path = "/home/ashvin/data/s3doodad/ashvin/corl2019/robot/rig2/run0/id0/vae.pkl"
+    # VAEVisualizer(model_path, train_data, test_data)
 
     tk.mainloop()
diff --git a/reset.p b/reset.p
new file mode 100644
index 0000000..4fd1324
--- /dev/null
+++ b/reset.p
@@ -0,0 +1,80 @@
+(dp0
+S'joints'
+p1
+(dp2
+S'right_j6'
+p3
+F-3.7902724609375
+sS'right_j5'
+p4
+F0.8597841796875
+sS'right_j4'
+p5
+F-0.247814453125
+sS'right_j3'
+p6
+F1.4232744140625
+sS'right_j2'
+p7
+F-2.983263671875
+sS'right_j1'
+p8
+F-2.4111435546875
+sS'right_j0'
+p9
+F-1.065392578125
+ssS'end_eff_xyz'
+p10
+cnumpy.core.multiarray
+_reconstruct
+p11
+(cnumpy
+ndarray
+p12
+(I0
+tp13
+S'b'
+p14
+tp15
+Rp16
+(I1
+(I3
+tp17
+cnumpy
+dtype
+p18
+(S'f8'
+p19
+I0
+I1
+tp20
+Rp21
+(I3
+S'<'
+p22
+NNNI-1
+I-1
+I0
+tp23
+bI00
+S'\x88\x18F\xf4\xa9\x91\xb7\xbf\x7f}\xb6\x1f\xe3\x05\xe1?\x0e\xe2\x0f\xbb3x\xc5?'
+p24
+tp25
+bsS'end_eff_quat'
+p26
+g11
+(g12
+(I0
+tp27
+g14
+tp28
+Rp29
+(I1
+(I4
+tp30
+g21
+I00
+S'\x07b\xefx\x03\xd8\xe6?\x06\x9a[pI`\xe6\xbfh<\x1f\x06\x1c\x0e\x8a\xbf\tI\xd0\xc8\x9cE\xa2\xbf'
+p31
+tp32
+bs.
\ No newline at end of file
diff --git a/screenshot.xwd b/screenshot.xwd
new file mode 100644
index 0000000..324c65f
Binary files /dev/null and b/screenshot.xwd differ
diff --git a/scripts/compute_rewards_on_video.py b/scripts/compute_rewards_on_video.py
deleted file mode 100644
index e1ab9b5..0000000
--- a/scripts/compute_rewards_on_video.py
+++ /dev/null
@@ -1,344 +0,0 @@
-# from multiworld.envs.mujoco.sawyer_xyz.sawyer_push_multiobj_subset import SawyerMultiobjectEnv
-# from multiworld.envs.mujoco.sawyer_xyz.sawyer_reach import SawyerReachXYZEnv
-
-import sys
-from multiworld.core.image_env import ImageEnv
-from multiworld.envs.real_world.sawyer.sawyer_reaching import SawyerReachXYZEnv
-# from sawyer_control.envs.sawyer_reaching import SawyerReachXYZEnv
-
-from railrl.launchers.launcher_util import run_experiment
-# import railrl.util.hyperparameter as hyp
-from railrl.launchers.experiments.ashvin.rfeatures.encoder_wrapped_env import EncoderWrappedEnv
-from railrl.misc.asset_loader import load_local_or_remote_file
-
-import torch
-
-from railrl.launchers.experiments.ashvin.rfeatures.rfeatures_model import TimestepPredictionModel
-import numpy as np
-
-from railrl.torch.grill.video_gen import VideoSaveFunction
-
-from railrl.launchers.arglauncher import run_variants
-import railrl.misc.hyperparameter as hyp
-
-import railrl.torch.pytorch_util as ptu
-
-# from railrl.launchers.experiments.ashvin.rfeatures.rfeatures_trainer import TimePredictionTrainer
-
-import matplotlib
-matplotlib.use('TkAgg')
-import matplotlib.pyplot as plt
-
-from torchvision.utils import save_image
-import pickle
-
-demo_trajectory_rewards = []
-
-import torchvision
-from PIL import Image
-import torchvision.transforms.functional as TF
-import random
-
-import skvideo.io
-
-RANDOM_CROP_X = 16
-RANDOM_CROP_Y = 16
-WIDTH = 456
-HEIGHT = 256
-CROP_WIDTH = WIDTH - RANDOM_CROP_X
-CROP_HEIGHT = HEIGHT - RANDOM_CROP_Y
-
-eps = 1e-5
-
-t_to_pil = torchvision.transforms.ToPILImage()
-t_random_resize = torchvision.transforms.RandomResizedCrop(
-    size=(CROP_WIDTH, CROP_HEIGHT,),
-    scale=(0.9, 1.0),
-    ratio=(1.0, 1.0), # don't change aspect ratio
-)
-t_color_jitter = torchvision.transforms.ColorJitter(
-    brightness=0.2, # (0.8, 1.2),
-    contrast=0.2, # (0.8, 1.2),
-    saturation=0.2, # (0.8, 1.2),
-    hue=0.1, # (-0.2, 0.2),
-)
-t_to_tensor = torchvision.transforms.ToTensor()
-
-def get_random_crop_params(img, scale_x, scale_y):
-    """Get parameters for ``crop`` for a random sized crop.
-
-    Args:
-        img (PIL Image): Image to be cropped.
-        scale (tuple): range of size of the origin size cropped
-        ratio (tuple): range of aspect ratio of the origin aspect ratio cropped
-
-    Returns:
-        tuple: params (i, j, h, w) to be passed to ``crop`` for a random
-            sized crop.
-    """
-    w = int(random.uniform(*scale_x) * CROP_WIDTH)
-    h = int(random.uniform(*scale_y) * CROP_HEIGHT)
-
-    i = random.randint(0, img.size[1] - h)
-    j = random.randint(0, img.size[0] - w)
-    
-    return i, j, h, w
-
-def load_path(data, demo_path):
-    H = len(data)
-    print("loading video, size", data.shape)
-
-    rewards = []
-
-    # import ipdb; ipdb.set_trace()
-
-    t_color_jitter_instance = None
-
-    # obs_batch = data[:, 60:, :440, :] # right
-    obs_batch = data[:, 60:, 60:, :] # left
-    # np.zeros((num_obs, 240, 440, 3))
-    obs_batch = obs_batch.transpose(0, 3, 1, 2) / 255.0
-    zs = env._encode(obs_batch)
-
-    z0 = zs[0, :]
-    zT = zs[-1, :]
-
-    for reward_type in ["latent_distance", "regression_distance"]:
-
-        for i in range(len(zs)):
-            dt = zs[i, :] - z0
-            dT = zT - z0
-
-            if reward_type == "regression_distance":
-                regression_pred_yt = (dt * dT).sum() / ((dT ** 2).sum() + eps)
-                r = -np.abs(1-regression_pred_yt)
-            if reward_type == "latent_distance":
-                r = -np.linalg.norm(dt - dT)
-
-            rewards.append(r)
-
-        reward_filename = demo_path[:-4] + "_%s_imagenet.png" % reward_type
-        plt.figure(figsize=(8, 8))
-        plt.plot(rewards)
-        plt.savefig(reward_filename)
-
-    # # Order of these two lines matters
-    # env.zT = goal_image_transformed
-    # env.initialize(zs)
-    # # print("z0", env.z0, "zT", env.zT, "dT", env.dT)
-
-    # for i in range(H):
-    #     ob = path[i]
-    #     action = path["actions"][i]
-    #     reward = path["rewards"][i]
-    #     next_ob = path["next_observations"][i]
-    #     terminal = path["terminals"][i]
-    #     agent_info = path["agent_infos"][i]
-    #     env_info = path["env_infos"][i]
-
-    #     # goal = path["goal"]["state_desired_goal"][0, :]
-    #     # import ipdb; ipdb.set_trace()
-    #     # print(goal.shape, ob["state_observation"])
-    #     # state_observation = np.concatenate((ob["state_observation"], goal))
-    #     # action = action[:2]
-
-    #     # update_obs_with_latent(ob)
-    #     # update_obs_with_latent(next_ob)
-    #     env._update_obs_latent(ob, zs[i, :])
-    #     env._update_obs_latent(next_ob, zs[i+1, :])
-    #     reward = env.compute_reward(
-    #         action,
-    #         next_ob,
-    #     )
-    #     path["rewards"][i] = reward
-    #     # reward = np.array([reward])
-    #     # terminal = np.array([terminal])
-
-    #     # print(reward)
-    #     rewards.append(reward)
-    # demo_trajectory_rewards.append(rewards)
-
-def load_demos(demo_paths, processed_demo_path, reference_path, name):
-    datas = []
-    for demo_path in demo_paths:
-        data = skvideo.io.vread(demo_path)
-        load_path(data, demo_path)
-        print("Finished loading demo: " + demo_path)
-
-    # np.save(processed_demo_path, data)
-    print("Dumping data")
-    # pickle.dump(datas, open(processed_demo_path, "wb"), protocol=4)
-
-    plt.figure(figsize=(8, 8))
-    print("Demo trajectory rewards len: ", len(demo_trajectory_rewards), "Data len: ", len(datas))
-    pickle.dump(demo_trajectory_rewards, open("demos/rlbench/demo_rewards_%s.p" % name, "wb"), protocol=4)
-    for r in demo_trajectory_rewards:
-        plt.plot(r)
-    plt.savefig("demos/rlbench/demo_rewards_%s.png" %name)
-
-def update_obs_with_latent(obs):
-    latent_obs = env._encode_one(obs["image_observation"])
-    latent_goal = np.zeros([]) # env._encode_one(obs["image_desired_goal"])
-    obs['latent_observation'] = latent_obs
-    obs['latent_achieved_goal'] = latent_goal
-    obs['latent_desired_goal'] = latent_goal
-    obs['observation'] = latent_obs
-    obs['achieved_goal'] = latent_goal
-    obs['desired_goal'] = latent_goal
-    return obs
-
-if __name__ == "__main__":
-    use_imagenet = "imagenet" in sys.argv
-    variant = dict(
-        env_class=SawyerReachXYZEnv,
-        env_kwargs=dict(
-            action_mode="position", 
-            max_speed = 0.05, 
-            camera="sawyer_head"
-        ),
-        # algo_kwargs=dict(
-        #     num_epochs=3000,
-        #     max_path_length=20,
-        #     batch_size=128,
-        #     num_eval_steps_per_epoch=1000,
-        #     num_expl_steps_per_train_loop=1000,
-        #     num_trains_per_train_loop=1000,
-        #     min_num_steps_before_training=1000,
-        # ),
-        algo_kwargs=dict(
-            num_epochs=3000,
-            max_path_length=10,
-            batch_size=5,
-            num_eval_steps_per_epoch=10,
-            num_expl_steps_per_train_loop=10,
-            num_trains_per_train_loop=10,
-            min_num_steps_before_training=10,
-        ),
-        model_kwargs=dict(
-            decoder_distribution='gaussian_identity_variance',
-            input_channels=3,
-            imsize=224,
-            architecture=dict(
-                hidden_sizes=[200, 200],
-            ),
-            delta_features=True,
-            pretrained_features=False,
-        ),
-        trainer_kwargs=dict(
-            discount=0.99,
-            demo_path="/home/anair/ros_ws/src/railrl-private/demo_v2_2.npy",
-            add_demo_latents=True,
-            bc_num_pretrain_steps=100,
-        ),
-        replay_buffer_kwargs=dict(
-            max_size=100000,
-            fraction_goals_rollout_goals=1.0,
-            fraction_goals_env_goals=0.0,
-        ),
-        qf_kwargs=dict(
-            hidden_sizes=[400, 300],
-        ),
-        policy_kwargs=dict(
-            hidden_sizes=[400, 300],
-        ),
-
-        save_video=True,
-        dump_video_kwargs=dict(
-            save_period=1,
-            # imsize=(3, 500, 300),
-        )
-    )
-
-    ptu.set_gpu_mode("gpu")
-
-    representation_size = 128
-    output_classes = 20
-
-    model_class = variant.get('model_class', TimestepPredictionModel)
-    model = model_class(
-        representation_size,
-        # decoder_output_activation=decoder_activation,
-        output_classes=output_classes,
-        **variant['model_kwargs'],
-    )
-    # model = torch.nn.DataParallel(model)
-
-    imagenets = [True, False]
-    reg_types = ["regression_distance", "latent_distance"]
-    for use_imagenet in [True]:
-        for reg_type in ["latent_distance"]:
-            print("Processing with imagenet: %s, type: %s" %(str(use_imagenet), reg_type))
-            if use_imagenet:
-                model_path = "/home/anair/data/s3doodad/facebook/models/rfeatures/multitask1/run2/id0/itr_0.pt" # imagenet
-            else:
-                model_path = "/home/anair/data/s3doodad/facebook/models/rfeatures/multitask1/run2/id2/itr_4000.pt"
-
-            # model = load_local_or_remote_file(model_path)
-            state_dict = torch.load(model_path)
-            model.load_state_dict(state_dict)
-            model.to(ptu.device)
-            model.eval()
-
-            for color in ["grey"]:
-                reference_path = "demos/door_demos_v3/demo_v3_%s_0.pkl"%color
-                traj = np.load("demos/door_demos_v3/demo_v3_%s_0.pkl"%color, allow_pickle=True)[0]
-
-                goal_image_flat = traj["observations"][-1]["image_observation"]
-                goal_image = goal_image_flat.reshape(1, 3, 500, 300).transpose([0, 1, 3, 2]) / 255.0
-                # goal_image = goal_image[:, ::-1, :, :].copy() # flip bgr
-                goal_image = goal_image[:, :, 60:300, 30:470]
-                goal_image_pt = ptu.from_numpy(goal_image)
-                save_image(goal_image_pt.data.cpu(), 'goal.png', nrow=1)
-                goal_latent = model.encode(goal_image_pt).detach().cpu().numpy().flatten()
-
-                initial_image_flat = traj["observations"][0]["image_observation"]
-                initial_image = initial_image_flat.reshape(1, 3, 500, 300).transpose([0, 1, 3, 2]) / 255.0
-                # initial_image = initial_image[:, ::-1, :, :].copy() # flip bgr
-                initial_image = initial_image[:, :, 60:300, 30:470]
-                initial_image_pt = ptu.from_numpy(initial_image)
-                save_image(initial_image_pt.data.cpu(), 'initial.png', nrow=1)
-                initial_latent = model.encode(initial_image_pt).detach().cpu().numpy().flatten()
-                print("Finished initial_latent")
-                reward_params = dict(
-                    goal_latent=goal_latent,
-                    initial_latent=initial_latent,
-                    goal_image=goal_image_flat,
-                    initial_image=initial_image_flat,
-                    # type="latent_distance"
-                    # type="regression_distance"
-                    type=reg_type
-                )
-                config_params = dict(
-                    initial_type="use_initial_from_trajectory",
-                    # initial_type="use_initial_from_trajectory",
-                    # goal_type="use_goal_from_trajectory",
-                    goal_type="",
-                    use_initial=True
-                )
-
-                env = variant['env_class'](**variant['env_kwargs'])
-                env = ImageEnv(env,
-                    recompute_reward=False,
-                    transpose=True,
-                    image_length=450000,
-                    reward_type="image_distance",
-                    # init_camera=sawyer_pusher_camera_upright_v2,
-                )
-                env = EncoderWrappedEnv(env, model, reward_params, config_params)
-                print("Finished creating env")
-                demo_paths=["/home/anair/ros_ws/src/railrl-private/demos/rlbench/demo_left_%i.mp4" % i for i in range(10)]
-                # demo_paths+=["/home/anair/ros_ws/src/railrl-private/demos/rlbench/demo_left_%i.pkl" % i for i in range(10)]
-
-            # processed_demo_path = "/home/anair/ros_ws/src/railrl-private/demos/door_demos_v3/processed_demos_imagenet2.pkl" # use this for imagenet
-            # processed_demo_path = "/home/anair/ros_ws/src/railrl-private/demos/door_demos_v3/processed_demos_imagenet_jitter2.pkl"
-                if use_imagenet:
-                    processed_demo_path = "/home/anair/ros_ws/src/railrl-private/demos/rlbench/demo_right_%s_imagenet_jitter2.pkl" % color
-                else:
-                    processed_demo_path = "/home/anair/ros_ws/src/railrl-private/demos/door_demos_v3/processed_demos_%s_jitter2.pkl" % color
-                name = color
-                if use_imagenet:
-                    name = "_imagenet_%s"%color
-                name = "demos/rlbench/%s_%s"%(name,reward_params["type"])
-                print("Loading demos for: ", name)
-                load_demos(demo_paths, processed_demo_path, reference_path, name)
-                demo_trajectory_rewards = []
\ No newline at end of file
diff --git a/scripts/convert_pkl_to_torch.py b/scripts/convert_pkl_to_torch.py
deleted file mode 100644
index f81042e..0000000
--- a/scripts/convert_pkl_to_torch.py
+++ /dev/null
@@ -1,10 +0,0 @@
-import pickle
-import torch
-
-path = "/checkpoint/anair17/ashvin/rfeatures/multitask1/run2/id0/itr_0"
-model = pickle.load(open(path + ".pkl", "rb"))
-torch.save(model.state_dict(), path + ".pt")
-
-print(path + ".pt")
-
-
diff --git a/scripts/export_tensorboard.py b/scripts/export_tensorboard.py
deleted file mode 100644
index 11a5a3c..0000000
--- a/scripts/export_tensorboard.py
+++ /dev/null
@@ -1,202 +0,0 @@
-# modified from https://github.com/anderskm/exportTensorFlowLog/blob/master/exportTensorFlowLog.py
-
-# import tensorflow as tf
-import time
-import csv
-import sys
-import os
-import collections
-
-import glob
-
-# Import the event accumulator from Tensorboard. Location varies between Tensorflow versions. Try each known location until one works.
-eventAccumulatorImported = False;
-# TF version < 1.1.0
-if (not eventAccumulatorImported):
-    try:
-        from tensorflow.python.summary import event_accumulator
-        eventAccumulatorImported = True;
-    except ImportError:
-        eventAccumulatorImported = False;
-# TF version = 1.1.0
-if (not eventAccumulatorImported):
-    try:
-        from tensorflow.tensorboard.backend.event_processing import event_accumulator
-        eventAccumulatorImported = True;
-    except ImportError:
-        eventAccumulatorImported = False;
-# TF version >= 1.3.0
-if (not eventAccumulatorImported):
-    try:
-        from tensorboard.backend.event_processing import event_accumulator
-        eventAccumulatorImported = True;
-    except ImportError:
-        eventAccumulatorImported = False;
-# TF version = Unknown
-if (not eventAccumulatorImported):
-    raise ImportError('Could not locate and import Tensorflow event accumulator.')
-
-summariesDefault = ['scalars','histograms','images','audio','compressedHistograms'];
-
-class Timer(object):
-    # Source: https://stackoverflow.com/a/5849861
-    def __init__(self, name=None):
-        self.name = name
-
-    def __enter__(self):
-        self.tstart = time.time()
-
-    def __exit__(self, type, value, traceback):
-        if self.name:
-            print('[%s]' % self.name)
-            print('Elapsed: %s' % (time.time() - self.tstart))
-
-def exitWithUsage():
-    print(' ');
-    print('Usage:');
-    print('   python readLogs.py <output-folder> <output-path-to-csv> <summaries>');
-    print('Inputs:');
-    print('   <input-path-to-logfile>  - Path to TensorFlow logfile.');
-    print('   <output-folder>          - Path to output folder.');
-    print('   <summaries>              - (Optional) Comma separated list of summaries to save in output-folder. Default: ' + ', '.join(summariesDefault));
-    print(' ');
-    sys.exit();
-
-def convert(inputLogFile, outputFolder, summaries):
-    print(' ');
-    print('> Log file: ' + inputLogFile);
-    print('> Output folder: ' + outputFolder);
-    print('> Summaries: ' + ', '.join(summaries));
-
-    if any(x not in summariesDefault for x in summaries):
-        print('Unknown summary! See usage for acceptable summaries.');
-        exitWithUsage();
-
-
-    print(' ');
-    print('Setting up event accumulator...');
-    with Timer():
-        ea = event_accumulator.EventAccumulator(inputLogFile,
-        size_guidance={
-            event_accumulator.COMPRESSED_HISTOGRAMS: 0, # 0 = grab all
-            event_accumulator.IMAGES: 0,
-            event_accumulator.AUDIO: 0,
-            event_accumulator.SCALARS: 0,
-            event_accumulator.HISTOGRAMS: 0,
-        })
-
-    print(' ');
-    print('Loading events from file*...');
-    print('* This might take a while. Sit back, relax and enjoy a cup of coffee :-)');
-    with Timer():
-        ea.Reload() # loads events from file
-
-    print(' ');
-    print('Log summary:');
-    tags = ea.Tags();
-    for t in tags:
-        tagSum = []
-        if (isinstance(tags[t],collections.Sequence)):
-            tagSum = str(len(tags[t])) + ' summaries';
-        else:
-            tagSum = str(tags[t]);
-        print('   ' + t + ': ' + tagSum);
-
-    if not os.path.isdir(outputFolder):
-        os.makedirs(outputFolder);
-
-    if ('audio' in summaries):
-        print(' ');
-        print('Exporting audio...');
-        with Timer():
-            print('   Audio is not yet supported!');
-
-    if ('compressedHistograms' in summaries):
-        print(' ');
-        print('Exporting compressedHistograms...');
-        with Timer():
-            print('   Compressed histograms are not yet supported!');
-
-
-    if ('histograms' in summaries):
-        print(' ');
-        print('Exporting histograms...');
-        with Timer():
-            print('   Histograms are not yet supported!');
-
-    if ('images' in summaries):
-        print(' ');
-        print('Exporting images...');
-        imageDir = outputFolder + 'images'
-        print('Image dir: ' + imageDir);
-        with Timer():
-            imageTags = tags['images'];
-            for imageTag in imageTags:
-                images = ea.Images(imageTag);
-                imageTagDir = imageDir + '/' + imageTag;
-                if not os.path.isdir(imageTagDir):
-                    os.makedirs(imageTagDir);
-                for image in images:
-                    imageFilename = imageTagDir + '/' + str(image.step) + '.png';
-                    with open(imageFilename,'wb') as f:
-                        f.write(image.encoded_image_string);
-
-    if ('scalars' in summaries):
-        print(' ');
-        csvFileName =  os.path.join(outputFolder,'tensorboard_log.csv');
-        print('Exporting scalars to csv-file...');
-        print('   CSV-path: ' + csvFileName);
-        scalarTags = tags['scalars'];
-        with Timer():
-            with open(csvFileName,'w') as csvfile:
-                logWriter = csv.writer(csvfile, delimiter=',');
-
-                # Write headers to columns
-                headers = ['wall_time','step'];
-                for s in scalarTags:
-                    headers.append(s);
-                logWriter.writerow(headers);
-        
-                vals = ea.Scalars(scalarTags[0]);
-                for i in range(len(vals)):
-                    v = vals[i];
-                    data = [v.wall_time, v.step];
-                    for s in scalarTags:
-                        scalarTag = ea.Scalars(s);
-                        S = scalarTag[i];
-                        data.append(S.value);
-                    logWriter.writerow(data);
-
-    print(' ');
-    print('Bye bye...');
-
-if __name__ == "__main__":
-
-    # if (len(sys.argv) < 3):
-    #     exitWithUsage();
-
-    # inputLogFile = sys.argv[1];
-    # outputFolder = sys.argv[2];
-
-    # if (len(sys.argv) < 4):
-    #     summaries = summariesDefault;
-    # else:
-    #     if (sys.argv[3] == 'all'):
-    #         summaries = summariesDefault;
-    #     else:
-    #         summaries = sys.argv[3].split(',');
-
-    # convert(inputLogFile, outputFolder, summaries)
-
-    inputFolder = sys.argv[1];
-
-    folders = glob.glob(inputFolder + "/**/tensorboard/", recursive=True)
-
-    for f in folders:
-        inputLogFile = f
-        outputFolder = f + "../"
-        summaries = summariesDefault
-        try:
-            convert(inputLogFile, outputFolder, summaries)
-        except:
-            print("failed at", inputLogFile)
\ No newline at end of file
diff --git a/scripts/multiobj/compare_reward_curves.py b/scripts/multiobj/compare_reward_curves.py
index 92af3e6..7550b0a 100644
--- a/scripts/multiobj/compare_reward_curves.py
+++ b/scripts/multiobj/compare_reward_curves.py
@@ -8,84 +8,58 @@ from torchvision.utils import save_image
 from railrl.data_management.images import normalize_image
 import matplotlib.pyplot as plt
 
-
-dataset_path = "/tmp/SawyerMultiobjectEnv_N5000_sawyer_init_camera_zoomed_in_imsize48_random_oracle_split_0.npy"
-cvae_path = "/home/khazatsky/rail/data/rail-khazatsky/sasha/PCVAE/DCVAE/run103/id0/vae.pkl"
-vae_path = "/home/khazatsky/rail/data/rail-khazatsky/sasha/PCVAE/baseline/run1/id0/itr_300.pkl"
-prefix = "pusher1_"
+dataset_path = "/home/ashvin/data/s3doodad/ashvin/corl2019/robot/rig2/run2/id0/"
+#dataset_path = "/home/ashvin/data/s3doodad/ashvin/corl2019/robot/rig2/run2/id0/"
+cvae_path = "/home/ashvin/data/s3doodad/ashvin/corl2019/robot/rig2/run0/id0/vae.pkl"
+#vae_path = "/home/khazatsky/rail/data/rail-khazatsky/sasha/PCVAE/baseline/run1/id0/itr_300.pkl"
+prefix = "CRIG_pusher_novel_"
 
 # dataset_path = "/tmp/Multiobj2DEnv_N100000_sawyer_init_camera_zoomed_in_imsize48_random_oracle_split_0.npy"
 # cvae_path = "/home/khazatsky/rail/data/rail-khazatsky/sasha/PCVAE/dynamics-cvae/run1/id0/itr_500.pkl"
 # vae_path = "/home/khazatsky/rail/data/rail-khazatsky/sasha/PCVAE/baseline/run4/id0/itr_300.pkl"
 # prefix = "pointmass1_"
 
-N_ROWS = 3
-
-
-
-
-dataset = load_local_or_remote_file(dataset_path)
-dataset = dataset.item()
-
-imlength = 6912
-imsize = 48
-
-N = dataset['observations'].shape[0]
-test_p = 0.9
-t = 0 #int(test_p * N)
-n = 50
 cvae = load_local_or_remote_file(cvae_path)
 cvae.eval()
 model = cvae.cpu()
-
-
-
-cvae_distances = np.zeros((N - t, n))
-for j in range(t, N):
-    traj  = dataset['observations'][j, :, :] / 255.0
-    n = traj.shape[0]
-
-    x0 = traj[0, :] #dataset['env'][j, :]
-    x0 = ptu.from_numpy(x0.reshape(1, -1))
-    goal = traj[-1]
-    latent_goal = model.encode(ptu.from_numpy(goal.reshape(1,-1)), x0, distrib=False)
-    latent_goal = ptu.get_numpy(latent_goal)
-
-    latents = model.encode(ptu.from_numpy(traj.reshape(n, imlength)), x0, distrib=False)
-    latents = ptu.get_numpy(latents)
-    latent_delta = latents - latent_goal
-    for i in range(n):
-        cvae_distances[j - t, i] = np.linalg.norm(latent_delta[i, :])
-
-cvae_distances = cvae_distances.mean(axis=0) / np.amax(cvae_distances.mean(axis=0))
-
-
-vae = load_local_or_remote_file(vae_path).to("cpu")
-vae.eval()
-model = vae.cpu()
-
-
-vae_distances = np.zeros((N - t, n))
-for j in range(t, N):
-    traj  = dataset['observations'][j, :, :] / 255.0
-    n = traj.shape[0]
-    goal = traj[-1]
-    latent_goal = model.encode(ptu.from_numpy(goal.reshape(1,-1)))[0]
-    latent_goal = ptu.get_numpy(latent_goal)
-
-    latents = model.encode(ptu.from_numpy(traj.reshape(n, imlength)))[0]
-    latents = ptu.get_numpy(latents)
-    latent_delta = latents - latent_goal
-    for i in range(n):
-        vae_distances[j - t, i] = np.linalg.norm(latent_delta[i, :])
-
-vae_distances = vae_distances.mean(axis=0) / np.amax(vae_distances.mean(axis=0))
-
-
-
-
-
-plt.plot(np.arange(n), vae_distances)
-plt.plot(np.arange(n), cvae_distances)
-
-plt.savefig("/home/khazatsky/rail/data/%sreward_curve.pdf" % prefix)
+epochs = 5
+
+cvae_distances = np.zeros(5)
+all_distances = []
+
+# for epoch in range(epochs):
+#     temp_path = dataset_path + "video_{0}_env.p".format(epoch)
+#     dataset = list(pickle.load(open(temp_path, "rb")))
+#     total_dist = 0
+#     for episode in range(10):
+#         data = dataset[episode]['full_observations']
+#         context = ptu.from_numpy(data[0]['image_observation']).reshape(1,-1)
+#         last_img = ptu.from_numpy(data[-1]['image_achieved_goal']).reshape(1,-1)
+#         goal_img = ptu.from_numpy(data[-1]['image_desired_goal']).reshape(1,-1)
+#         z_n = ptu.get_numpy(model.encode(last_img, context, distrib=False))
+#         z_goal = ptu.get_numpy(model.encode(goal_img, context, distrib=False))
+#         total_dist -= np.linalg.norm(z_n - z_goal)
+#         all_distances.append(np.linalg.norm(z_n - z_goal))
+#     cvae_distances[epoch] = total_dist / 10
+for epoch in range(1,epochs):
+    temp_path = dataset_path + "video_{0}_env.p".format(epoch)
+    dataset = list(pickle.load(open(temp_path, "rb")))
+    total_dist = 0
+    for episode in range(10):
+        data = dataset[episode]['full_observations']
+        last_img = ptu.from_numpy(data[-1]['image_achieved_goal']).reshape(1,-1)
+        goal_img = ptu.from_numpy(data[-1]['image_desired_goal']).reshape(1,-1)
+
+        z_n = ptu.get_numpy(model.encode(last_img)[0])
+        z_goal = ptu.get_numpy(model.encode(goal_img)[0])
+        total_dist -= np.linalg.norm(z_n - z_goal)
+        all_distances.append(np.linalg.norm(z_n - z_goal))
+    cvae_distances[epoch] = total_dist / 10
+print(cvae_distances)
+
+dists = np.array(all_distances)
+print(dists)
+print(dists.mean(), dists.std())
+
+# plt.plot(np.arange(epochs), cvae_distances)
+# plt.savefig("/home/ashvin/data/%sreward_curve.pdf" % prefix)
diff --git a/scripts/multiobj/get_imagined_goals.py b/scripts/multiobj/get_imagined_goals.py
new file mode 100644
index 0000000..5e2675b
--- /dev/null
+++ b/scripts/multiobj/get_imagined_goals.py
@@ -0,0 +1,23 @@
+import torch
+import numpy as np
+import pickle
+from railrl.misc.asset_loader import load_local_or_remote_file
+import railrl.torch.pytorch_util as ptu
+import scipy.misc
+from PIL import Image
+from torchvision.utils import save_image
+from railrl.data_management.images import normalize_image
+import matplotlib.pyplot as plt
+
+dataset_path = "/home/ashvin/data/s3doodad/ashvin/corl2019/robot/test1/run50/id0/"
+
+for epoch in range(5):
+    temp_path = dataset_path + "video_{0}_vae.p".format(epoch)
+    dataset = list(pickle.load(open(temp_path, "rb")))
+    total_dist = 0
+    for episode in range(10):
+        data = dataset[episode]['full_observations']
+        goal_img = data[-1]['decoded_goal_image']
+        goal_img = goal_img.reshape(3, 48, 48).transpose() * 255.0
+        goal_img = Image.fromarray(goal_img.astype('uint8'), mode='RGB') #.convert('RGB')
+        goal_img.save('/home/ashvin/data/imagined_goals/goal{0}_{1}.jpg'.format(epoch, episode))
\ No newline at end of file
diff --git a/scripts/sim_multitask_policy.py b/scripts/sim_multitask_policy.py
index f76dd38..9d8149f 100644
--- a/scripts/sim_multitask_policy.py
+++ b/scripts/sim_multitask_policy.py
@@ -32,8 +32,8 @@ def simulate_policy(args):
     else:
         ptu.set_gpu_mode(False)
         policy.to(ptu.device)
-    if isinstance(env, VAEWrappedEnv):
-        env.mode(args.mode)
+    # if isinstance(env, VAEWrappedEnv):
+        # env.mode(args.mode)
     if args.enable_render or hasattr(env, 'enable_render'):
         # some environments need to be reconfigured for visualization
         env.enable_render()
diff --git a/scripts/update_demo_with_latents.py b/scripts/update_demo_with_latents.py
deleted file mode 100644
index ff39201..0000000
--- a/scripts/update_demo_with_latents.py
+++ /dev/null
@@ -1,355 +0,0 @@
-# from multiworld.envs.mujoco.sawyer_xyz.sawyer_push_multiobj_subset import SawyerMultiobjectEnv
-# from multiworld.envs.mujoco.sawyer_xyz.sawyer_reach import SawyerReachXYZEnv
-
-import sys
-from multiworld.core.image_env import ImageEnv
-from multiworld.envs.real_world.sawyer.sawyer_reaching import SawyerReachXYZEnv
-# from sawyer_control.envs.sawyer_reaching import SawyerReachXYZEnv
-
-from railrl.launchers.launcher_util import run_experiment
-# import railrl.util.hyperparameter as hyp
-from railrl.launchers.experiments.ashvin.rfeatures.encoder_wrapped_env import EncoderWrappedEnv
-from railrl.misc.asset_loader import load_local_or_remote_file
-
-import torch
-
-from railrl.launchers.experiments.ashvin.rfeatures.rfeatures_model import TimestepPredictionModel
-import numpy as np
-
-from railrl.torch.grill.video_gen import VideoSaveFunction
-
-from railrl.launchers.arglauncher import run_variants
-import railrl.misc.hyperparameter as hyp
-
-import railrl.torch.pytorch_util as ptu
-
-# from railrl.launchers.experiments.ashvin.rfeatures.rfeatures_trainer import TimePredictionTrainer
-
-import matplotlib
-matplotlib.use('TkAgg')
-import matplotlib.pyplot as plt
-
-from torchvision.utils import save_image
-import pickle
-
-demo_trajectory_rewards = []
-
-import torchvision
-from PIL import Image
-import torchvision.transforms.functional as TF
-import random
-
-RANDOM_CROP_X = 16
-RANDOM_CROP_Y = 16
-WIDTH = 456
-HEIGHT = 256
-CROP_WIDTH = WIDTH - RANDOM_CROP_X
-CROP_HEIGHT = HEIGHT - RANDOM_CROP_Y
-
-t_to_pil = torchvision.transforms.ToPILImage()
-t_random_resize = torchvision.transforms.RandomResizedCrop(
-    size=(CROP_WIDTH, CROP_HEIGHT,),
-    scale=(0.9, 1.0),
-    ratio=(1.0, 1.0), # don't change aspect ratio
-)
-t_color_jitter = torchvision.transforms.ColorJitter(
-    brightness=0.2, # (0.8, 1.2),
-    contrast=0.2, # (0.8, 1.2),
-    saturation=0.2, # (0.8, 1.2),
-    hue=0.1, # (-0.2, 0.2),
-)
-t_to_tensor = torchvision.transforms.ToTensor()
-
-def get_random_crop_params(img, scale_x, scale_y):
-    """Get parameters for ``crop`` for a random sized crop.
-
-    Args:
-        img (PIL Image): Image to be cropped.
-        scale (tuple): range of size of the origin size cropped
-        ratio (tuple): range of aspect ratio of the origin aspect ratio cropped
-
-    Returns:
-        tuple: params (i, j, h, w) to be passed to ``crop`` for a random
-            sized crop.
-    """
-    w = int(random.uniform(*scale_x) * CROP_WIDTH)
-    h = int(random.uniform(*scale_y) * CROP_HEIGHT)
-
-    i = random.randint(0, img.size[1] - h)
-    j = random.randint(0, img.size[0] - w)
-    
-    return i, j, h, w
-
-def load_path(path, reference_path):
-    goal_image_transformed = None
-    final_achieved_goal = path["observations"][-1]["state_achieved_goal"].copy()
-
-    print("loading path, length", len(path["observations"]), len(path["actions"]))
-    H = min(len(path["observations"]), len(path["actions"]))
-    rewards = []
-
-    # import ipdb; ipdb.set_trace()
-
-    t_color_jitter_instance = None
-
-    num_obs = len(path["observations"])
-    obs_batch = np.zeros((num_obs, 240, 440, 3))
-    for idx in range(num_obs):
-        ob = path["observations"][idx][env.vae_input_observation_key].reshape(3, 500, 300).transpose()
-        img = Image.fromarray(ob, 'RGB')
-
-        if t_color_jitter_instance is None:
-            i, j, h, w = get_random_crop_params(
-                img, 
-                t_random_resize.scale, 
-                t_random_resize.scale,
-            )
-
-            t_color_jitter_instance = t_color_jitter.get_params(
-                t_color_jitter.brightness,
-                t_color_jitter.contrast,
-                t_color_jitter.saturation,
-                t_color_jitter.hue,
-            )
-            traj = np.load("demos/door_demos_v3/demo_v3_%s_0.pkl"%color, allow_pickle=True)[0]
-            goal_image_transformed = traj["observations"][-1][env.vae_input_observation_key].reshape(3, 500, 300).transpose()
-            goal_image_transformed = Image.fromarray(goal_image_transformed, 'RGB')
-            goal_image_transformed = TF.resized_crop(goal_image_transformed, i, j, h, w, (CROP_HEIGHT, CROP_WIDTH,), t_random_resize.interpolation)
-            goal_image_transformed = t_color_jitter_instance(goal_image_transformed)
-            goal_image_transformed = np.array(goal_image_transformed)[None]
-            goal_image_transformed = goal_image_transformed.transpose(0, 3, 1, 2) / 255.0
-            goal_image_transformed = env._encode(goal_image_transformed)[0]
-
-        x0 = TF.resized_crop(img, i, j, h, w, (CROP_HEIGHT, CROP_WIDTH,), t_random_resize.interpolation)
-        x0 = t_color_jitter_instance(x0)
-
-        # x0 = t_to_tensor(x0)
-
-        obs_batch[idx, :] = np.array(x0) # [60:300, 30:470, :] # ob
-
-    obs_batch = obs_batch.transpose(0, 3, 1, 2) / 255.0
-    zs = env._encode(obs_batch)
-
-    # Order of these two lines matters
-    env.zT = goal_image_transformed
-    env.initialize(zs)
-    # print("z0", env.z0, "zT", env.zT, "dT", env.dT)
-
-    for i in range(H):
-        ob = path["observations"][i]
-        action = path["actions"][i]
-        reward = path["rewards"][i]
-        next_ob = path["next_observations"][i]
-        terminal = path["terminals"][i]
-        agent_info = path["agent_infos"][i]
-        env_info = path["env_infos"][i]
-
-        # goal = path["goal"]["state_desired_goal"][0, :]
-        # import ipdb; ipdb.set_trace()
-        # print(goal.shape, ob["state_observation"])
-        # state_observation = np.concatenate((ob["state_observation"], goal))
-        # action = action[:2]
-
-        # update_obs_with_latent(ob)
-        # update_obs_with_latent(next_ob)
-        env._update_obs_latent(ob, zs[i, :])
-        env._update_obs_latent(next_ob, zs[i+1, :])
-        reward = env.compute_reward(
-            action,
-            next_ob,
-        )
-        path["rewards"][i] = reward
-        # reward = np.array([reward])
-        # terminal = np.array([terminal])
-
-        # print(reward)
-        rewards.append(reward)
-    demo_trajectory_rewards.append(rewards)
-
-def load_demos(demo_paths, processed_demo_path, reference_path, name):
-    datas = []
-    for demo_path in demo_paths:
-        for i in range(10):
-            data = pickle.load(open(demo_path, "rb"))
-            for path in data:
-                load_path(path, reference_path)
-            datas.append(data)
-        print("Finished loading demo: " + demo_path)
-
-    # np.save(processed_demo_path, data)
-    print("Dumping data")
-    pickle.dump(datas, open(processed_demo_path, "wb"), protocol=4)
-
-    plt.figure(figsize=(8, 8))
-    print("Demo trajectory rewards len: ", len(demo_trajectory_rewards), "Data len: ", len(datas))
-    pickle.dump(demo_trajectory_rewards, open("demo_rewards_%s.p" % name, "wb"), protocol=4)
-    for r in demo_trajectory_rewards:
-        plt.plot(r)
-    plt.savefig("demo_rewards_%s.png" %name)
-
-def update_obs_with_latent(obs):
-    latent_obs = env._encode_one(obs["image_observation"])
-    latent_goal = np.zeros([]) # env._encode_one(obs["image_desired_goal"])
-    obs['latent_observation'] = latent_obs
-    obs['latent_achieved_goal'] = latent_goal
-    obs['latent_desired_goal'] = latent_goal
-    obs['observation'] = latent_obs
-    obs['achieved_goal'] = latent_goal
-    obs['desired_goal'] = latent_goal
-    return obs
-
-if __name__ == "__main__":
-    use_imagenet = "imagenet" in sys.argv
-    variant = dict(
-        env_class=SawyerReachXYZEnv,
-        env_kwargs=dict(
-            action_mode="position", 
-            max_speed = 0.05, 
-            camera="sawyer_head"
-        ),
-        # algo_kwargs=dict(
-        #     num_epochs=3000,
-        #     max_path_length=20,
-        #     batch_size=128,
-        #     num_eval_steps_per_epoch=1000,
-        #     num_expl_steps_per_train_loop=1000,
-        #     num_trains_per_train_loop=1000,
-        #     min_num_steps_before_training=1000,
-        # ),
-        algo_kwargs=dict(
-            num_epochs=3000,
-            max_path_length=10,
-            batch_size=5,
-            num_eval_steps_per_epoch=10,
-            num_expl_steps_per_train_loop=10,
-            num_trains_per_train_loop=10,
-            min_num_steps_before_training=10,
-        ),
-        model_kwargs=dict(
-            decoder_distribution='gaussian_identity_variance',
-            input_channels=3,
-            imsize=224,
-            architecture=dict(
-                hidden_sizes=[200, 200],
-            ),
-            delta_features=True,
-            pretrained_features=False,
-        ),
-        trainer_kwargs=dict(
-            discount=0.99,
-            demo_path="/home/anair/ros_ws/src/railrl-private/demo_v2_2.npy",
-            add_demo_latents=True,
-            bc_num_pretrain_steps=100,
-        ),
-        replay_buffer_kwargs=dict(
-            max_size=100000,
-            fraction_goals_rollout_goals=1.0,
-            fraction_goals_env_goals=0.0,
-        ),
-        qf_kwargs=dict(
-            hidden_sizes=[400, 300],
-        ),
-        policy_kwargs=dict(
-            hidden_sizes=[400, 300],
-        ),
-
-        save_video=True,
-        dump_video_kwargs=dict(
-            save_period=1,
-            # imsize=(3, 500, 300),
-        )
-    )
-
-    ptu.set_gpu_mode("gpu")
-
-    representation_size = 128
-    output_classes = 20
-
-    model_class = variant.get('model_class', TimestepPredictionModel)
-    model = model_class(
-        representation_size,
-        # decoder_output_activation=decoder_activation,
-        output_classes=output_classes,
-        **variant['model_kwargs'],
-    )
-    # model = torch.nn.DataParallel(model)
-
-    # imagenets = [True, False]
-    imagenets = [False]
-    reg_types = ["latent_distance"]
-    for use_imagenet in imagenets:
-        for reg_type in reg_types:
-            print("Processing with imagenet: %s, type: %s" %(str(use_imagenet), reg_type))
-            if use_imagenet:
-                model_path = "/home/anair/data/s3doodad/facebook/models/rfeatures/multitask1/run2/id0/itr_0.pt" # imagenet
-            else:
-                model_path = "/home/anair/data/s3doodad/facebook/models/rfeatures/multitask1/run2/id2/itr_4000.pt"
-
-            # model = load_local_or_remote_file(model_path)
-            state_dict = torch.load(model_path)
-            model.load_state_dict(state_dict)
-            model.to(ptu.device)
-            model.eval()
-
-            for color in ["grey", "beige", "green", "brownhatch"]:
-                reference_path = "demos/door_demos_v3/demo_v3_%s_0.pkl"%color
-                traj = np.load("demos/door_demos_v3/demo_v3_%s_0.pkl"%color, allow_pickle=True)[0]
-
-                goal_image_flat = traj["observations"][-1]["image_observation"]
-                goal_image = goal_image_flat.reshape(1, 3, 500, 300).transpose([0, 1, 3, 2]) / 255.0
-                # goal_image = goal_image[:, ::-1, :, :].copy() # flip bgr
-                goal_image = goal_image[:, :, 60:300, 30:470]
-                goal_image_pt = ptu.from_numpy(goal_image)
-                save_image(goal_image_pt.data.cpu(), 'goal.png', nrow=1)
-                goal_latent = model.encode(goal_image_pt).detach().cpu().numpy().flatten()
-
-                initial_image_flat = traj["observations"][0]["image_observation"]
-                initial_image = initial_image_flat.reshape(1, 3, 500, 300).transpose([0, 1, 3, 2]) / 255.0
-                # initial_image = initial_image[:, ::-1, :, :].copy() # flip bgr
-                initial_image = initial_image[:, :, 60:300, 30:470]
-                initial_image_pt = ptu.from_numpy(initial_image)
-                save_image(initial_image_pt.data.cpu(), 'initial.png', nrow=1)
-                initial_latent = model.encode(initial_image_pt).detach().cpu().numpy().flatten()
-                print("Finished initial_latent")
-                reward_params = dict(
-                    goal_latent=goal_latent,
-                    initial_latent=initial_latent,
-                    goal_image=goal_image_flat,
-                    initial_image=initial_image_flat,
-                    # type="latent_distance"
-                    # type="regression_distance"
-                    type=reg_type
-                )
-                config_params = dict(
-                    # initial_type="",
-                    initial_type="use_initial_from_trajectory",
-                    goal_type="use_goal_from_trajectory",
-                    # goal_type="",
-                    use_initial=True
-                )
-
-                env = variant['env_class'](**variant['env_kwargs'])
-                env = ImageEnv(env,
-                    recompute_reward=False,
-                    transpose=True,
-                    image_length=450000,
-                    reward_type="image_distance",
-                    # init_camera=sawyer_pusher_camera_upright_v2,
-                )
-                env = EncoderWrappedEnv(env, model, reward_params, config_params)
-                print("Finished creating env")
-                demo_paths=["/home/anair/ros_ws/src/railrl-private/demos/door_demos_v3/demo_v3_%s_%i.pkl" % (color, i) for i in range(10)]
-
-                name = color
-                if use_imagenet:
-                    name = "_imagenet_%s"%color
-                name = "%s_%s"%(name,reward_params["type"])
-                if config_params["use_initial"]:
-                    name = name + "_use_initial"
-                name = name + "_%s_%s" %(config_params["initial_type"], config_params["goal_type"])
-
-                processed_demo_path = "/home/anair/ros_ws/src/railrl-private/demos/door_demos_v3/processed_demos_%s_jitter2.pkl" % name
-
-                print("Loading demos for: ", name)
-                load_demos(demo_paths, processed_demo_path, reference_path, name)
-                demo_trajectory_rewards = []
